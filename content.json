{"meta":{"title":"Young's Blog","subtitle":"","description":"keep foolish, keep hungry","author":"Alexis Young","url":"http://yangtf983.github.io","root":"/"},"posts":[{"tags":[{"name":"机器学习基石听课笔记","slug":"机器学习基石听课笔记","permalink":"http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E5%90%AC%E8%AF%BE%E7%AC%94%E8%AE%B0/"}],"title":"机器学习基石笔记笔记3：学习类型","date":"2020/01/24","text":"【0】说明 之前在笔记1中的机器学习三要素*（ 1. 存在一个可以被学习的潜在模式； 2. 不知道如何定义规则并写入程序； 3. 可以获得大量数据。）*中已经讲过，机器学习可以执行的前提是存在可以学习的模式以及足够的数据。在实际情况中，学习的模式和输入的数据都可能有很多不同的类型，相应的也诞生了很多不同的学习算法和思想，根据这些不同，机器学习有很多分类，这一节我们介绍一些分类方法和机器学习类型。 【1】Learning wiwh Different Output Space Y\\mathcal{Y}Y 上一节我们介绍了PLA算法和SVM算法，它们只能将一个空间分成两部分，也就是两个不同的类，即输出空间只有+1和-1,称这样的分类问题为二元分类。 此外，根据输出空间的更多不同情况，还存在多种类别，总结如下： 二元分类 binary classification 输出空间：Y={−1,+1}\\mathcal{Y}=\\{-1,+1\\}Y={−1,+1} 举例：判断是否发放信用卡（输出空间：发放；不发放） 多元分类 multiclass classification 输出空间：Y={1,2，…,K}(abstractly)\\mathcal{Y}=\\{1,2，\\ldots,K\\}(abstractly)Y={1,2，…,K}(abstractly) 举例：手写数字识别（输出空间：0,1,…,9） 回归 regression 输出空间：Y=R or Y=[lower,upper]⊂R\\mathcal{Y}=\\mathbb{R}\\ or\\ \\mathcal{Y}=[lower,upper]\\subset\\mathbb{R}Y=R or Y=[lower,upper]⊂R 举例：根据公司数据预测股价 结构化学习 structured learning 输出空间：Y=structures\\mathcal{Y}=structuresY=structures 距离：蛋白质测序；语句结构识别（自动识别 一个句子中哪些是动词哪些是名词等） 除了以上四种，还有更多分类，这里不再进行介绍。但是值得一提的是，在这么多问题中，最核心的是二分类和回归，许多其他的问题的算法都是由这两种问题的算法进行改进得到的，甚至对于回归问题做一些处理也可以解决而分类问题（如逻辑斯谛回归）。 【2】Learning with Different Data Label yny_{n}yn​ 根据标签值的不同特点可以将机器学习分成以下几类： 监督学习 supervised learning 所有的数据都有标签，学习的目的是给出正确的标签值。 无监督学习 unsupervised learning 所有数据都没有标签，学习的目的是找出感兴趣的数据结构，比如概率密度等。 半监督学习 semi-supervised learning 一部分数据有标签，另一部分数据无标签，无标签数据可以辅助提高学习的精度，一般而言无标签数据远多于有标签数据。 强化学习 reinforcement learning 输出结果是一个行为而不是一个标签，建立奖励函数对这个行为的好坏进行判断。一般而言强化学习更加适合有明确规则的情况，如围棋。规则明确并且简单时容易建立奖励函数。强化学习需要知道三个信息，分别是：action(行为),observation(观测值),reward(奖励)。目前其最广为人知的应用可能是alphago. 【3】Learning with Different Protocol batch learning 特点：每次抽取一批数据进行训练，这要求所有的数据在一开始就是确定的，计算机通过一定的程序每次从中随机抽取一批进行训练。 online learning 特点：数据一个个进来，每次数据的更新都能对模型进行优化，因此称作在线学习。例如在线邮件过滤系统可以根据当前算法判断下一封邮件的内容，再根据用户反馈（如用户反馈判断错误）及时对模型进行优化。 avtive learning 特点：是一种新的机器学习类型，其特点是让机器主动问问题来提升模型的性能。如手写数字识别中，可以让机器对自己判断困难的数字进行提问，由人来对其打标签，这类判断困难的 数据往往对于提升模型性能更加有效。其优势之一是在获取样本标签困难的时候可以节约时间和成本，只对一些重要的数据打标签。 以上三种学习类型可以分别类比为：填鸭式、老师教学和主动问问题。 【4】Learning with Different Input Space X\\mathcal{X}X 根据输入的值类型不同，可以分为具体特征、原始特征和抽象特征，下面分别介绍： concrete feature 具体特征是指输入值具有清晰的实际意义，例如输入用户的收入和存款，判断是否应该发放信用卡。这样的例子中输入数据的实际意义是很明显的，与需要判断的结果联系很密切。具体特征的选择要求实验者有一定的相关知识，这样才能选择最合适的特征。也由于具体特征与判断目标的相关性较大，无关信息少，因此相应的算法往往更加简单，计算难度也较小。 raw feature 原始特征是指具有一些简单的实际意义的数据。例如输入图片的像素值组成的数组来判断图像中是什么。这样的数据中包含与判断结果相关的信息，例如在鉴别数字1和5时可以通过像素值得到图像的对称性和密度，这两个特征对于判别1和5显然是较为有效的，如果提取出来这两个特征并用其来作为判别的数据，那么就变成了具体特征，这个提取特征的过程有一个好听的名字叫做“特征提取”。在传统机器学习中，这一步往往需要认为来进行尝试，在深度学习中已经可以自动提取，不过需要更加大量的数据。 abstract feature 抽象特征是指数据没有或者很少有实际意义，例如输入数据是用户ID，ID数字的大小和接近程度并没有实际的意义，不同的ID只是代表不同的用户，显然其中包含的信息非常少。抽象特征也是三种输入空间中最难进行机器学习的 一种，也需要特征工程。","permalink":"http://yangtf983.github.io/2020/01/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B0%E7%AC%94%E8%AE%B03%EF%BC%9A%E5%AD%A6%E4%B9%A0%E7%B1%BB%E5%9E%8B/","photos":[]},{"tags":[{"name":"机器学习基石听课笔记","slug":"机器学习基石听课笔记","permalink":"http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E5%90%AC%E8%AF%BE%E7%AC%94%E8%AE%B0/"}],"title":"机器学习基石笔记二：从PLA到SVM","date":"2020/01/23","text":"【0】说明 PLA，全称perceptron learning algorithm，是一种二分类算法，也可以认为是单层神经网络，与SVM有着密不可分的关系。SVM，即support vector machine，中文名是支持向量机，也是一种常见的二分类算法，这种算法的 一种推导方式就是从PLA出发，找到一条“最好的”PLA分类器。在机器学习基石课程中林轩田老师只介绍了PLA，而SVM是他在后续的另一门课程——机器学习技法——中介绍的。由于二者具有紧密的联系，故在此一并介绍。 【1】PLA介绍 PLA是解决二分类问题的一种算法，其来源可以从几何上理解：自变量可以投影到一个超平面，通过构造这个超平面的一个线性分割，尽可能好地将不同的类别的点分开。 PLA(perceptron learning algorithm)，中文名：线性感知机算法，又称线性分类器。 【2】符号表示 已有数据指标X=(x1,…,xd)\\mathcal{X}=(x_{1},\\ldots,x_{d})X=(x1​,…,xd​)，数据标签Y=(+1,−1)\\mathcal{Y}=(+1,-1)Y=(+1,−1)，目标是通过PLA算法 计算出一系列权重W=(w1,…,wd)\\mathcal{W}=(w_{1},\\ldots,w_{d})W=(w1​,…,wd​)，使得当Σi=1dwixi&gt;threshold\\Sigma^{d}_{i=1}w_{i}x_{i}&gt;thresholdΣi=1d​wi​xi​&gt;threshold时，预测标签值y=1，当Σi=1dwixi&lt;threshold\\Sigma^{d}_{i=1}w_{i}x_{i}&lt;thresholdΣi=1d​wi​xi​&lt;threshold，预测标签值y=-1，即h(x)=sign((Σi=1dwixi)−threshold),h∈Hh(x)=sign((\\Sigma^{d}_{i=1}w_{i}x_{i})-threshold),h\\in\\mathcal{H}h(x)=sign((Σi=1d​wi​xi​)−threshold),h∈H h(x)h(x)h(x)的形式可以进行简化： h(x)=sign((Σi=1dwixi)+(−threshold)∗(+1))=sign(Σi=0dwixi)=sign(wTx),h∈H\\begin{array}{rl} h(x)&amp;=sign((\\Sigma^{d}_{i=1}w_{i}x_{i})+(-threshold)*(+1))\\\\ &amp;=sign(\\Sigma^{d}_{i=0}w_{i}x_{i})\\\\ &amp;=sign(w^{T}x),h\\in\\mathcal{H}\\end{array}h(x)​=sign((Σi=1d​wi​xi​)+(−threshold)∗(+1))=sign(Σi=0d​wi​xi​)=sign(wTx),h∈H​ 容易看出上述简化其实是将thresholdthresholdthreshold记为了w0w_{0}w0​，显然x0=1x_{0}=1x0​=1. 简化成这种形式有两个好处，一是可以把threshold放到权重w中一起求出，二是在几何表示上更加直观，付出了将指标维数+1的牺牲，但是可以将超平面转移到一定经过原点，这对于我们后面的图形表示以及理解PLA算法都很有帮助。 【3】PLA过程 从表达式h(x)=sign(wTx),h∈Hh(x)=sign(w^{T}x),h\\in\\mathcal{H}h(x)=sign(wTx),h∈H中可以看出，对于和向量www呈锐角的数据xxx，由于二者内积是正的，所以用该分类器给出的预测值是1；反之，对于呈钝角的数据，给出的预测值是-1，对于呈直角的数据，该式未给出合理的预测值，此时可以理解为从该分类器中无法辨别该数据究竟更加符合哪一类，可以随机制定一个类别。 根据上述分析，若将自变量xxx在空间中的位置标出，并在其中画出以www为法向量并经过原点的（超）平面，则平面一侧的所有点代表的向量都与www呈锐角，另一侧的所有点代表的向量都与www呈钝角，平面上的所有点代表的向量都与www呈直角，即该（超）平面就是由上式h(x)h(x)h(x)定义的分类器。 下面以二维情况为例对PLA算法进行解释。 显然，二维情况下H\\mathcal{H}H应当包含平面上所有的直线，最理想的算法应当是从平面上所有的直线中找到一条最好的直线作为ggg，不过我们无法将平面上所有的直线遍历，一个合理的办法是设定初值再进行迭代。这也是PLA算法的基本思路。下面看一下迭代过程： For t=0,1,… step1. find the next mistake of wtw_{t}wt​ called (xn(t),yn(t))(x_{n(t)}, y_{n(t)})(xn(t)​,yn(t)​) $$sign(w^{T}{t}x{n(t)}\\neq y_{n(t)})$$ step2. correct the mistake by $$w_{t+1}\\leftarrow w_{t}+y_{n(t)}x_{n(t)}$$ …until a full cycle of not encountering mistakes (“next” can follow navie cycle (1,…,N) or precomputed random cycle) 上述算法又被称为循环PLA，因为它必须经过一个确定的循环没有错误后才会终止。 从中可以看到每次迭代的规则是： wt+1←wt+yn(t)xn(t)w_{t+1}\\leftarrow w_{t}+y_{n(t)}x_{n(t)} wt+1​←wt​+yn(t)​xn(t)​ 由于只对sign(wtTxn(t)≠yn(t))sign(w^{T}_{t}x_{n(t)}\\neq y_{n(t)})sign(wtT​xn(t)​​=yn(t)​)的点进行迭代得到新的www，即找到yn(t)wtTxn(t)&lt;0y_{n(t)}w^{T}_{t}x_{n(t)}&lt;0yn(t)​wtT​xn(t)​&lt;0的点时进行一次迭代，因此最终得到的ggg是满足yn(t)wtTxn(t)&gt;0,∀ty_{n(t)}w^{T}_{t}x_{n(t)}&gt;0,\\forall tyn(t)​wtT​xn(t)​&gt;0,∀t的线性函数。 且每次迭代后，yn(t)wt+1Txn(t)≥yn(t)wtTxn(t)y_{n(t)}w^{T}_{t+1}x_{n(t)}\\geq y_{n(t)}w^{T}_{t}x_{n(t)}yn(t)​wt+1T​xn(t)​≥yn(t)​wtT​xn(t)​，即yn(t)wt+1Txn(t)y_{n(t)}w^{T}_{t+1}x_{n(t)}yn(t)​wt+1T​xn(t)​一定在变大，所以模型确实随着不断迭代在优化。 从图形上看，用向量运算的方法容易看出，每次迭代的结果都使得wt+1w_{t+1}wt+1​和xn(t)x_{n(t)}xn(t)​之间的夹角变得更优了，如下图： 不过值得一提的是，这种优化只是对于预测错误的那个点的优化，在总的数据集上的表现有没有优化是不确定的，有可能在某一步迭代过后得到分类器在总的数据集上的表现更差。下面是一个PLA迭代过程中每一步的表示： 从这个实例可以看到，第七到第八步迭代时，用来计算迭代式的点进行了优化并通过迭代取得了正确的结果，但是新的分类器在数据集上的错误却增加了一个，即表现变差了。此外，这个迭代 最终产生了一个在数据集上表现完美的分类器，说明了前述的循环PLA算法是有效的，我们的机器确实学到了东西。 【4】有穷性与Pocket算法 因为终止条件没有时间或步数限制，所以上述算法的有穷性有待考虑。 循环PLA算法能够终止的前提是对所有的数据点都得到正确的分类，因此显然其终止前提是至少存在一个分类器可以将不同类别的点 分开，因为PLA是线性分类器，所以这个前提的等价说法是不同类别的区域应当是线性可分的，也就是可以用线性分类器将其分开。这个要求可以用数学中一个概念来表述，那就是不同类别点组成的区域应当是凸区域。 上述表述说明线性可分（凸区域）是终止的前提条件，下面我们说明，当数据点线性可分时，我们可以找到算法步数的上界的表达式，从而证明线性可分是算法有界的充分条件；当数据点非线性可分时，我们应当对算法的终止条件进行修改避免死循环。 线性可分时 线性可分时，存在理想模式fff，记对应的分类器的权重为wfw_{f}wf​，满足yn=sign(wfTxn),∀ny_{n}=sign(w^{T}_{f}x_{n}),\\forall nyn​=sign(wfT​xn​),∀n，按照循环PLA算法流程，仍取w0=(0,…,0)w_{0}=(0,\\ldots,0)w0​=(0,…,0)，可以做如下推导： yn(t)wfTxn(t)≥minn{ynwfTxn}&gt;0wfTwt+1=wfT(wt+yn(t)xn(t))≥wfTwt+minn{ynwfTxn}≥…≥wfTw0+(t+1)∗minn{ynwfTxn}=(t+1)∗minnynwfTxn∣∣wt∣∣2=∣∣wt−1+yn(t−1)xn(t−1)∣∣2=∣∣wt−1∣∣2+2yn(t−1)wt−1Txn(t−1)+∣∣yn(t−1)xn(t−1)∣∣2≤∣∣wt−1∣∣2+0+∣∣yn(t−1)xn(t−1)∣∣2≤∣∣wt−1∣∣2+maxn{∣∣xn∣∣2}≤…≤∣∣w0∣∣2+t∗maxn{∣∣xn∣∣2}=t∗maxn{∣∣xn∣∣2}wfT∥wf∥wT∥wT∥≥T∗min⁡n{ynwfTxn}∥wfT∥∗∥wT∥≥T∗min⁡n{ynwfTxn}∥wfT∥∗T∗max⁡n∥xn∥≥T∗min⁡n{ynwfTxn}∥wfT∥∗max⁡n∥xn∥=T∗ constant \\begin{array}{rl} y_{n(t)}w^{T}_{f}x_{n(t)}&amp;\\geq min_{n}\\{ y_{n}w^{T}_{f}x_{n} \\}\\\\ &amp;&gt;0\\\\ w^{T}_{f} w_{t+1} &amp;= w^{T}_{f}(w_{t}+y_{n(t)}x_{n(t)}) \\\\ &amp;\\geq w^{T}_{f}w_{t} + min_{n}\\{ y_{n}w^{T}_{f}x_{n} \\} \\\\ &amp;\\geq \\ldots \\\\ &amp;\\geq w^{T}_{f}w_{0}+(t+1)*min_{n}\\{ y_{n}w^{T}_{f}x_{n} \\} \\\\ &amp;=(t+1)*min_{n}{y_{n}w^{T}_{f}x_{n}}\\\\ ||w_{t}||^{2} &amp;= || w_{t-1}+y_{n(t-1)x_{n(t-1)}} ||^{2}\\\\ &amp;= ||w_{t-1}||^{2}+2y_{n(t-1)}w^{T}_{t-1}x_{n(t-1)}+|| y_{n(t-1)}x_{n(t-1)} ||^{2}\\\\ &amp;\\leq ||w_{t-1}||^{2}+0+|| y_{n(t-1)}x_{n(t-1)} ||^{2} \\\\ &amp;\\leq ||w_{t-1}||^{2}+max_{n}\\{ ||x_{n}||^{2} \\}\\\\ &amp;\\leq \\ldots\\\\ &amp;\\leq ||w_{0}||^{2}+t*max_{n}\\{ ||x_{n}||^{2} \\}\\\\ &amp;= t*max_{n}\\{ ||x_{n}||^{2} \\}\\\\ \\frac{w_{f}^{T}}{\\left\\|w_{f}\\right\\|} \\frac{ w_{T} }{\\left\\|w_{T}\\right\\|} &amp;\\geq \\frac{T * \\min _{n}\\left\\{y_{n} \\mathrm{w}_{f}^{T} x_{n}\\right\\}}{\\left\\|w_{f}^{T}\\right\\|*\\left\\|w_{T}\\right\\|}\\\\ &amp;\\geq \\frac{T * \\min _{n}\\left\\{y_{n} w_{f}^{T} x_{n}\\right\\}}{\\left\\|w_{f}^{T}\\right\\| * \\sqrt{T} * \\max _{n}\\left\\|x_{n}\\right\\|} \\\\ &amp;\\geq \\frac{\\sqrt{T} * \\min _{n}\\left\\{y_{n} \\mathrm{w}_{f}^{T} x_{n}\\right\\}}{\\left\\|w_{f}^{T}\\right\\|*\\max _{n}\\left\\|x_{n}\\right\\|}\\\\ &amp;=\\sqrt{T} * \\text { constant } \\end{array}yn(t)​wfT​xn(t)​wfT​wt+1​∣∣wt​∣∣2∥wf​∥wfT​​∥wT​∥wT​​​≥minn​{yn​wfT​xn​}&gt;0=wfT​(wt​+yn(t)​xn(t)​)≥wfT​wt​+minn​{yn​wfT​xn​}≥…≥wfT​w0​+(t+1)∗minn​{yn​wfT​xn​}=(t+1)∗minn​yn​wfT​xn​=∣∣wt−1​+yn(t−1)xn(t−1)​​∣∣2=∣∣wt−1​∣∣2+2yn(t−1)​wt−1T​xn(t−1)​+∣∣yn(t−1)​xn(t−1)​∣∣2≤∣∣wt−1​∣∣2+0+∣∣yn(t−1)​xn(t−1)​∣∣2≤∣∣wt−1​∣∣2+maxn​{∣∣xn​∣∣2}≤…≤∣∣w0​∣∣2+t∗maxn​{∣∣xn​∣∣2}=t∗maxn​{∣∣xn​∣∣2}≥∥wfT​∥∗∥wT​∥T∗minn​{yn​wfT​xn​}​≥∥wfT​∥∗T​∗maxn​∥xn​∥T∗minn​{yn​wfT​xn​}​≥∥wfT​∥∗maxn​∥xn​∥T​∗minn​{yn​wfT​xn​}​=T​∗ constant ​ 由于$ \\frac{w_{f}^{T}}{\\left|w_{f}\\right|} \\frac{ w_{T} }{\\left|w_{T}\\right|} \\leq 1，故，故，故 \\frac{\\sqrt{T} * \\min {n}\\left{y{n} \\mathrm{w}{f}^{T} x{n}\\right}}{\\left|w_{f}^{T}\\right|\\max {n}\\left|x{n}\\right|}\\leq 1 ，进而得到，进而得到，进而得到 T\\leq (\\frac{\\sqrt{T} * \\min {n}\\left{y{n} \\mathrm{w}{f}^{T} x{n}\\right}}{\\left|w_{f}^{T}\\right|\\max {n}\\left|x{n}\\right|})^{2}=\\text{constant}’. 至此，我们证明了在存在 至此，我们证明了在存在至此，我们证明了在存在w_{f}的情况下T存在上界，但是由于机器学习问题中我们不可能提前知道的情况下T存在上界，但是由于机器学习问题中我们不可能提前知道的情况下T存在上界，但是由于机器学习问题中我们不可能提前知道w_{f}，因此无法算出这个上界的精确值，只能知道其存在上界。此外，，因此无法算出这个上界的精确值，只能知道其存在上界。此外，，因此无法算出这个上界的精确值，只能知道其存在上界。此外，w_{f}$也只有在数据点线性可分的情况下才可能存在（不考虑noise）。 实际问题中我们还常常面临着测量误差（noise的一种）的问题，一组线性可分的数据，可能因为测量误差而并非线性可分，但是在测量误差不大的情况下，使用PLA算法仍然可以找到一个合适的近似函数ggg，只是此时需要对算法的终止条件进行改动，否则就会陷入死循环。 一个自然的想法是：既然g≈fg\\approx fg≈f，不妨求出一个在数据集上表现最好（判断错误数最少）的ggg作为fff的近似。此时，PLA问题等价于优化： wg←argmin⁡w∑n=1N∥yn≠sign⁡(wTxn)∥\\mathbf{w}_{g} \\leftarrow \\underset{\\mathbf{w}}{\\operatorname{argmin}} \\sum_{n=1}^{N} \\| y_{n} \\neq \\operatorname{sign}\\left(\\mathbf{w}^{T} \\mathbf{x}_{n}\\right)\\| wg​←wargmin​n=1∑N​∥yn​​=sign(wTxn​)∥ 这个问题经过证明是一个N-P难问题，我们求解这样的问题一般采用求近似解的方法，不是想办法找到该优化问题的最优解，而是找近似最优解。求PLA问题的近似最优解的算法又称作Pocket算法，其想法是我们总之把当前最好的一个分类器放在口袋中，只有迭代后的新的分类器的表现比当前口袋中分类器的表现更好时才会将当前口袋中的分类器扔掉将新的分类器放入口袋，这里评价好坏的标准就是看谁的wg\\mathbf{w}_{g}wg​更小，此外，为了避免当数据点过多时遍历花费太多时间，一般的选择是仅仅选取一个随机子集来判断分类器的好坏,再通过设定最大迭代步数对算法进行终止。根据这种思想，可以写出Pocket算法流程如下： initialize pocket weight w^\\hat{\\mathbf{w}}w^ For t=0,1,\\ldots step1. find a (random) mistake of wt\\mathbf{w}_{t}wt​ called (xn(t),yn(t))(\\mathbf{x}_{n(t)},y_{n(t)})(xn(t)​,yn(t)​) step2. (try to) correct the mistake by wt+1←wt+yn(t)xn(t)\\mathbf{w}_{t+1}\\leftarrow \\mathbf{w}_{t}+y_{n(t)}x_{n(t)} wt+1​←wt​+yn(t)​xn(t)​ step3. if wt+1\\mathbf{w}_{t+1}wt+1​ makes fewer mistakes than w^\\hat{\\mathbf{w}}w^, replace w^\\hat{\\mathbf{w}}w^ by wt+1\\mathbf{w}_{t+1}wt+1​ …until enough iterations return w^\\hat{\\mathbf{w}}w^ (called wPOCKET\\mathbf{w}_{POCKET}wPOCKET​) as ggg 从PLA到SVM PLA算法的可能结果不唯一，如下图展示了同一组数据的多个最优PLA分类器（这里的最优是对所有数据点均判断正确，后续会定义新的“最优”）： 在这种情况下，我们希望定义 更加“严格”的最优，最好是使得上述情况只能有一种最优解。一种合理的方法是将数据测量的误差考虑在内，尽量使得数据测量有误差时不对类别的判断产生影响，也称之为更稳定（more robust）。 一般而言数据测量的误差不大，在真实数据周围较近的位置，因此可以以不同数据点为圆心画等半径圆并要求所有圆不能与分类器相交，此时找出一个分类器使得能够做出最大的半径，这个分类器就是我们要找的最优分类器，如下图第三个分类器： 等价地，我们可以用线宽来表现这种稳定性，不断增加分割线的宽度，当分割线恰好与数据点相交时，线宽最大的线最稳定，二者的等价性是显然的，线宽的表示如图： 从图中也可以看出，最大线宽有时不是由所有点决定的，上例中最大线宽仅仅由三个数据点即可决定，还有一个数据点没有起作用。类似于概率统计中将概率密度不为0的集合定义为支撑集（support set），这里将这些对线宽起决定作用的点定义为支持向量（support vector），将求这个具有最大线宽的线性分类器的方法称为支持向量机（support vector machine）. 用公式表示这个最优化问题： max⁡wmargin⁡(w) subject to every ynwTxn&gt;0margin⁡(w)=min⁡n=1,…,Ndistance⁡(xn,w)\\begin{array}{rl} {\\max _{\\mathbf{w}}} &amp; {\\operatorname{margin}(\\mathbf{w})} \\\\ {\\text { subject to }} &amp; {\\text { every } y_{n} \\mathbf{w}^{T} \\mathbf{x}_{n}&gt;0} \\\\ {} &amp; {\\operatorname{margin}(\\mathbf{w})=\\min _{n=1, \\ldots, N} \\operatorname{distance}\\left(\\mathbf{x}_{n}, \\mathbf{w}\\right)} \\end{array}maxw​ subject to ​margin(w) every yn​wTxn​&gt;0margin(w)=minn=1,…,N​distance(xn​,w)​ 下面处理这个最优化问题。 在本篇文章的【2】符号表示一节中我们将thresholdthresholdthreshold并入了www中： h(x)=sign((Σi=1dwixi)+(−threshold)∗(+1))=sign(Σi=0dwixi)=sign(wTx),h∈H\\begin{array}{rl} h(x)&amp;=sign((\\Sigma^{d}_{i=1}w_{i}x_{i})+(-threshold)*(+1))\\\\ &amp;=sign(\\Sigma^{d}_{i=0}w_{i}x_{i})\\\\ &amp;=sign(w^{T}x),h\\in\\mathcal{H}\\end{array}h(x)​=sign((Σi=1d​wi​xi​)+(−threshold)∗(+1))=sign(Σi=0d​wi​xi​)=sign(wTx),h∈H​ 这里，我们将xxx和www缩短，也就是去掉x0x_{0}x0​和w0w_{0}w0​，将thresholdthresholdthreshold再单独出来，此时h(x)=sign(wTx+b)h(x)=sign(w^{T}x+b)h(x)=sign(wTx+b) 数据点x\\mathbf{x}x到超平面的距离由(x,b,w)( \\mathbf{x},b,\\mathbf{w} )(x,b,w)决定，记为distance⁡(x,b,w)\\operatorname{distance}\\left(\\mathbf{x}, b, \\mathbf{w}\\right)distance(x,b,w) 超平面上的点由wTx+b=0\\mathbf{w}^{T}\\mathbf{x}+b=0wTx+b=0确定。 若记x′\\mathbf{x}&#x27;x′为超平面上的点，则distance⁡(x,b,w)\\operatorname{distance}\\left(\\mathbf{x}, b, \\mathbf{w}\\right)distance(x,b,w)为x′−x\\mathbf{x}&#x27;-\\mathbf{x}x′−x到超平面的法向量w\\mathbf{w}w的映射，即： distance⁡(x,b,w)=project(x′−x)to⊥hyprtplane=∣wT∥w∥(x−x′)∣=(1)1∥w∥∣wTx+b∣\\begin{array}{rl} \\operatorname{distance}\\left(\\mathbf{x}, b, \\mathbf{w}\\right) &amp;= project(\\mathbf{x}&#x27;-\\mathbf{x})to\\perp hyprtplane\\\\ &amp;=\\left|\\frac{\\mathbf{w}^{T}}{\\|\\mathbf{w}\\|}\\left(\\mathbf{x}-\\mathbf{x}^{\\prime}\\right)\\right| \\\\ &amp;\\stackrel{(1)}{=} \\frac{1}{\\|\\mathbf{w}\\|}\\left|\\mathbf{w}^{T} \\mathbf{x}+b\\right| \\end{array}distance(x,b,w)​=project(x′−x)to⊥hyprtplane=∣∣∣​∥w∥wT​(x−x′)∣∣∣​=(1)∥w∥1​∣∣​wTx+b∣∣​​ 这样，原问题就转化为： max⁡b,wmargin⁡(b,w) subject to every yn(wTxn+b)&gt;0margin⁡(b,w)=min⁡n=1,…,N1∥w∥yn(wTxn+b)\\begin{array}{rl} {\\max _{b,\\mathbf{w}}} &amp; {\\operatorname{margin}(b, \\mathbf{w})} \\\\ {\\text { subject to }} &amp; {\\text { every } y_{n} (\\mathbf{w}^{T} \\mathbf{x}_{n}+b)&gt;0} \\\\ &amp; {\\operatorname{margin}(b,\\mathbf{w})}=\\min _{n=1, \\ldots, N} \\frac{1}{\\|\\mathbf{w}\\|}y_{n}(\\mathbf{w}^{T} \\mathbf{x}_{n}+b) \\end{array}maxb,w​ subject to ​margin(b,w) every yn​(wTxn​+b)&gt;0margin(b,w)=minn=1,…,N​∥w∥1​yn​(wTxn​+b)​ 易知wTx+b=0\\mathbf{w}^{T} \\mathbf{x}+b=0wTx+b=0与3wTxn+3b=03\\mathbf{w}^{T} \\mathbf{x}_{n}+3b=03wTxn​+3b=0表示相同的超平面，因此wT\\mathbf{w}^{T}wT与bbb的大小不影响分类器，其比例才影响分类器，故可以通过改变其大小使得min⁡n=1,…,Nyn(wTxn+b)=1\\min _{n=1, \\ldots, N} y_{n}(\\mathbf{w}^{T} \\mathbf{x}_{n}+b)=1minn=1,…,N​yn​(wTxn​+b)=1，此时优化目标可以表示为margin⁡(b,w)=1∥w∥{\\operatorname{margin}(b,\\mathbf{w})}=\\frac{1}{\\|\\mathbf{w}\\|}margin(b,w)=∥w∥1​，优化问题可以转化如下： max⁡b,w1∥w∥ subject to every yn(wTxn+b)&gt;0min⁡n=1,…,Nyn(wTxn+b)=1\\begin{array}{cl} {\\max _{b, \\mathbf{w}}} &amp; {\\frac{1}{\\|\\mathbf{w}\\|}} \\\\ {\\text { subject to }} &amp; {\\text { every } y_{n}\\left(\\mathrm{w}^{T} \\mathrm{x}_{n}+b\\right)&gt;0} \\\\ {} &amp; {\\min _{n=1, \\ldots, N} y_{n}\\left(\\mathbf{w}^{T} \\mathrm{x}_{n}+b\\right)=1} \\end{array} maxb,w​ subject to ​∥w∥1​ every yn​(wTxn​+b)&gt;0minn=1,…,N​yn​(wTxn​+b)=1​ 进一步将问题转化为： min⁡b,w12wTw subject to yn(wTxn+b)≥1for all n\\begin{array}{cl} {\\min _{b, \\mathbf{w}}} &amp; {\\frac{1}{2}\\mathbf{w}^{T}\\mathbf{w}} \\\\ {\\text { subject to }} &amp; { y_{n}\\left(\\mathrm{w}^{T} \\mathrm{x}_{n}+b\\right)\\geq 1} for\\ all\\ n \\end{array} minb,w​ subject to ​21​wTwyn​(wTxn​+b)≥1for all n​ 上述表达为二次规划标准型，二次规划的求解已经是一个比较成熟的领域，在很多软件中都有求解该问题的函数，按照对应的格式将数据传入相应函数即可求出SVM的解。","permalink":"http://yangtf983.github.io/2020/01/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B02%EF%BC%9A%E4%BB%8EPLA%E5%88%B0SVM/","photos":[]},{"tags":[{"name":"机器学习基石听课笔记","slug":"机器学习基石听课笔记","permalink":"http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E5%90%AC%E8%AF%BE%E7%AC%94%E8%AE%B0/"}],"title":"机器学习基石笔记一：机器学习简介","date":"2020/01/23","text":"【0】 说明 机器学习基石是一门很好的机器学习入门课，能够让人在短时间内掌握大量的机器学习的基础理论和原理。从这篇文章开始，我会用大约十几篇文章的内容将机器学习基石我听机器学习基石这门课时候的所学和思考表达出来，我的笔记中会包含大部分基石课程中的内容，并且不会略去任何课程中的证明。对于课程中证明讲的不清楚的一些地方，我还会附上我的理解。关于这门课程的课件可以从课程主页上下载。 【1】机器学习概念 人类的学习行为可以看作是从观察和实践中学习获得技能，类似地，可以定义机器学习为机器从数据中学习获得技能。 【2】应用领域 人类无法手动编程的系统——探索火星的系统； 人类难以轻易写出程序规则的系统——语音/图像识别； 人类无法做到的快速决策系统——高速股票交易系统； 个性化服务系统——广告精准投放。 【3】机器学习三要素 一般认为，一个系统要想应用机器学习方法，需要具备三个条件： 存在一个可以被学习的潜在模式； 不知道如何定义规则并写入程序； 可以获得大量数据。 这三个要素缺一不可：首先，被学习的问题中必须有一个可以被学习的模式，否则机器学习算法永远不可能学到有用的内容，例如用机器学习算法预测一个公平骰子下一次掷出几点；其次，这个规则应当是不知道如何定义，或至少不知道如何准确定义的，否则直接用程序写出规则更加简单和节省算力；最后，要有机器学习的来源，也就是大量的数据，后面我们将会提到，机器学习的可信度与数据量密不可分，而且随着模型变得复杂，达到同样的可信度需要的数据量会剧增。 【4】例子 预测婴儿在某一时刻会不会哭（没有规则，不适合机器学习）； 判断一个 已给的图像中 是否包含一个圆（可以写出规则，不适合机器学习）； 决定是发否应该发给某人信用卡（满足三个条件，适合机器学习）； 预测接下来的十年地球是否会被核武器毁灭（没有充足的数据，不适合机器学习）； 【5】符号化表示 以上述例3为例，将机器学习中的要素进行符号化表示如下： 输入：x∈Xx\\in \\mathcal{X}x∈X（刻画消费者特征的相关指标） 输出：y∈Yy\\in \\mathcal{Y}y∈Y（适合/不适合发放信用卡） 未知的模式：f:X→Yf:\\mathcal{X}\\rightarrow \\mathcal{Y}f:X→Y（理想的信用卡发放公式） 数据（训练集）：D={(x1,y1),…,(xN,yN}\\mathcal{D}=\\{ (x_{1},y_{1}),\\ldots,(x_{N},y_{N} \\}D={(x1​,y1​),…,(xN​,yN​}（银行的历史数据） 假设：g:X→Yg:\\mathcal{X}\\rightarrow \\mathcal{Y}g:X→Y（g可以通过对数据进行学习逼近fff） 模型的假设：g∈Hg\\in \\mathcal{H}g∈H，ggg是假设H\\mathcal{H}H中表现最好的那个； 学习流程：fff（未知）→ {(xn,yn)}\\rightarrow \\ \\{(x_{n},y_{n})\\}→ {(xn​,yn​)}（来自fff的训练集）$\\rightarrow\\ $算法 → g≈f\\rightarrow\\ g\\approx f→ g≈f 【6】机器学习 vs 数据挖掘 vs 人工智能 vs 统计学 机器学习 数据挖掘 使用数据得到一个逼近目标f的近似模式g 从大量数据中寻找感兴趣的特性 当感兴趣的性质就是寻找估计的目标函数时，二者一致 当感兴趣的性质与寻找估计的目标函数时有关时，两者的方法可以相互帮助 机器学习 人工智能 使用数据得到一个逼近目标f的近似模式g 使机器能够做一些智能行为 得到近似于f的g的过程就是一个显示智能的过程，因此，机器学习是实现人工智能的一种方法 机器学习 统计学 使用数据得到一个逼近目标f的近似模式g 使用数据推断未知的参数或目标 更重视如何通过计算机得到结果 在一定的假设下通过数学证明得到一些可信的结果，很少关注计算的实现 当g是需要推断的结果，f是未知的目标时，统计学与机器学习的目标和方法是一致的，因此统计学的很多方法和工具可以用于机器学习。","permalink":"http://yangtf983.github.io/2020/01/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B01%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/","photos":[]},{"tags":[{"name":"Semi-supervised Learning","slug":"Semi-supervised-Learning","permalink":"http://yangtf983.github.io/tags/Semi-supervised-Learning/"}],"title":"论文翻译：半监督学习介绍","date":"2020/01/09","text":"0 说明 本文是一篇关于半监督学习介绍性论文集的第一篇的翻译，大部分是人工翻译，少部分是机器翻译加人工修改。不过限于知识有限，其中很多内容并不了解，所以一些地方翻译不到位，仅供参考。 论文集电子版链接onedrive 1 监督学习、无监督学习与半监督学习 为了理解半监督学习的本质，有必要首先了解一下监督学习和无监督学习。 1.1 监督学习与无监督学习 一般来说，传统的机器学习有两种不同类型，即无监督学习（Unsupervised Learning ）和监督学习（Supervised Learning）。 首先看一下无监督学习。假设存在一个包含n个数据点的集合X=(x1,…,xn)X=(x_{1},\\ldots,x_{n})X=(x1​,…,xn​)，其中xi∈X,i∈[n]:={1,…,n}x_{i}\\in\\mathcal{X},i\\in[n]:=\\{1,\\ldots,n\\}xi​∈X,i∈[n]:={1,…,n}.通常这些数据点被认为是独立同分布于分布X\\mathcal{X}X，常将其方便地定义为n∗dn*dn∗d维矩阵X=(xiT)i∈[n]TX=(x_{i}^{T})^{T}_{i\\in[n]}X=(xiT​)i∈[n]T​，该矩阵的每一行也就是一个数据点，无监督学习的目标就是从这样的数据XXX中找到一个有趣的结构。通常可以认为无监督学习的基本的问题是找到一个产生数据XXX的概率密度。不过无监督学习也有一些相对更弱的形式，例如分位数估算（quantile estimation），聚类（clustering），离群值检测（outlier detection）和降维（dimensionality reduction）。 接下来看一下监督学习。它的目标是从一个由成对数据(xi,yi)(x_{i},y_{i})(xi​,yi​)组成的数据集中学习一个从xxx到yyy的映射。其中yi∈Yy_{i}\\in\\mathcal{Y}yi​∈Y被称作数据xxx的标签或目标值。如果标签也是数字，就可以用y=(yi)i∈[n]Ty=(y_{i})^{T}_{i\\in[n]}y=(yi​)i∈[n]T​表示由标签组成的列向量。与无监督学习类似，一个基本的要求是数据(xi,yi(x_{i},y_{i}(xi​,yi​独立同分布取样于X×Y\\mathcal{X}\\times\\mathcal{Y}X×Y.由于得到的映射很容易通过其在测试集上的表现进行评估，所以监督学习的任务可以被很好的定义。当Y=R\\mathcal{Y}=\\mathbb{R}Y=R或Y=Rd\\mathcal{Y}=\\mathbb{R}^{d}Y=Rd时（也可以直接说当标签值的取值范围是连续的数字时），这个任务又被叫做回归。本书的大部分内容都是关于分类的，即Y\\mathcal{Y}Y的值来源于一个有限集，或者说标签是分离的。书中也有一些关于回归问题的探讨，在第23章。监督学习有两类算法：生成式算法（Generative algorithms）和判别式算法（Discriminative algorithm）。生成式算法试图用一些无监督学习过程构造条件密度模型p(x∣y)p(x|y)p(x∣y)（这里我们假定所有的分布都是有概率密度函数的，这样可以简化问题，我们将自己限定在处理概率密度的问题上），通过这个概率密度，再结合贝叶斯公式p(y∣x)=p(x∣y)p(y)∫Yp(x∣y)p(y)dyp(y|x)=\\frac{p(x|y)p(y)}{\\int_{\\mathcal{Y}}p(x|y)p(y)dy}p(y∣x)=∫Y​p(x∣y)p(y)dyp(x∣y)p(y)​就可以预测新的数据的标签值。事实上生成式算法中的p(x∣y)p(y)=p(x,y)p(x|y)p(y)=p(x,y)p(x∣y)p(y)=p(x,y)就是生成成对数据(xi,yi)(x_{i},y_{i})(xi​,yi​)的联合概率密度。而判别式算法则并不试图估计产生数据xix_{i}xi​的概率密度，它仅仅关注如何估计p(y∣x)p(y|x)p(y∣x)。一些判别式算法的目的甚至限制在仅仅关注p(y∣x)p(y|x)p(y∣x)是否大于0.5，例如支持向量机算法（SVM）。通常认为由于判别式算法与监督学习的目标之间的关联更加直接，在实际问题中判别式算法往往更加有效。这两种框架的更多细节会在2.2.1和2.2.2部分进一步讨论。 1.2 半监督学习 半监督学习（Semi-Supervised Learning），简记为SSL，是一种介于监督学习与无监督学习之间的机器学习类型。其特点是数据以无标签为主，再加上一部分的有标签信息的数据，通常所说的标签信息就是这部分数据的标签。此时，数据集x=(xi)i∈[n]x=(x_{i})_{i\\in[n]}x=(xi​)i∈[n]​可以被分成两部分，一部分是标签Yl:=(y1,…,yn)Y_{l}:=(y_{1},\\ldots,y_{n})Yl​:=(y1​,…,yn​)已知的点Xl:=(x1,…,xl)X_{l}:=(x_{1},\\ldots,x_{l})Xl​:=(x1​,…,xl​)，另一部分是标签未知的点Xu:=(xl+1,…,xl+u)X_{u}:=(x_{l+1},\\ldots,x_{l+u})Xu​:=(xl+1​,…,xl+u​).这种情况就是这本书所研究的“标准的”半监督学习的形式，书中大部分章节研究的问题都是这种形式。 其他形式的部分监督也是可能的。例如，我们已知的标签信息是某些点有（或者没有）相同的标签（参照Abu-Mostafa, 1995）.这种更广的设定会在第五章中被讨论。不同的设定将会导致对半监督学习产生不同的观点：在第五章中，SSL被看作在限制条件下的无监督学习。与之相反的是，大部分方法都把SSL看作是知道了额外的关于xxx的信息的有监督学习。后一种解释与SSL的大多数应用相一致，因为大部分时候SSL的目标与监督学习的目标相一致，都是预测所给数据xix_{i}xi​的目标值。然而，当类别的数量和本质不能提前知道，并且需要从数据中推断出这些的话，后一种观点就不适用了。此时，将SSL看作在限制条件下的无监督学习的观点则是适用的。 几十年前，Vapnik就提出了一个与SSL有关的问题，这就是所谓的转导学习（transductive learning）。在转导学习的设定下，实验者拥有一个有标签的训练集和一个无标签的测试集，其特点是预测的结果仅仅作用在测试集上。相对应的是归纳学习（inductive learning），其目的是得到一个定义在整个空间X\\mathcal{X}X上的预测函数，可以用这个函数预测整个空间上所有的点的标签值。这本书中的许多方法都是转导式的，尤其是基于图形类数据进行推断时，转导式方法更加自然，转导学习将在1.2.4部分被进一步解释。 1.3 半监督学习的历史简介 最早将无标签数据用于分类问题的方法很可能使自学习（self-learning），这种方法又被称为自训练（self-training）、自标签（self-labeling）或决策导向或学习（decision-directed learning）。这种方法从在有标签数据上训练开始，每一步都会用当前的决策函数（decision function）预测一部分无标签点的标签，然后把这部分被标注的点纳入训练集，重新应用有监督学习方法训练模型。这种方法很早就出现在一些文献中了（如：Scudder (1965); Fralick (1967); Agrawala (1970)）。 这种算法令人不满意的一个方面是包装（wrapper）效果依赖于所使用的监督学习方法。如果用实际风险最小化与1-0损失来做自学习，那原来的无标签数据加上后对新训练的模型没有任何影响。而如果用余量最大法（margin maximizing method）来做自学习，其结果是决策边界将被推离无标签点（参见第6章）。在其他情况下似乎还不清楚自学习方法真正的作用情况以及对应于哪种假设。 与 半监督学习关系密切的是转导推理（transductive inference或transduction），这种方法的先行者是Vapnik(Vapnik and Chervonenkis, 1974;Vapnik and Sterin, 1977).与归纳推理（inductive inference）相反，转导推理不追求推断出一般的决策规则，而是仅专注于预测无标签点（测试集），不做外推。一个早期的转导推理的例子在1968年就已经由Hartley和Rao提出来了，虽然当时还没有明确定义这个概念。他们建议对测试集的标签进行组合优化以最大化其模型的可能性。 半监督学习真正意义上的起飞似乎是从1970年代人们开始用无标签数据估计费舍尔线性判别式（Fisher linear discriminant）规则开始（Hosmer, 1973; McLachlan, 1977; O’Neill, 1978; McLachlan and Ganesalingam, 1982）.更确切地说，这种情况是指每一类的条件密度都是满足有着相同协方差矩阵的高斯条件的，这种模型可以通过诸如EM算法等迭代算法进行优化，同时使用有标签数据和无标签数据，求出数据标签的最大可能性。与混合高斯情况不同，Cooper和Freeman已经研究过了同时使用有标签数据和无标签数据估计多项分布的情况。 后来，每类一种成分的设定已经被推广到了每类几种成分（Shahshahani and Landgrebe, 1994），而后又被Miller和Uyar进一步推广（1997）。 在PAC（probably approximately correct）框架下，二元高斯混合情况下的学习率已经在1995由Ratsaby和Venkatesh得到。在混合情况可识别的情况下，Castelli和Cover证明，在无标签点的数量是有限个的情况下，错误率呈指数级收敛到贝叶斯风险。可识别的意思是说，在给定P(x)P(x)P(x)的情况下，其分解式ΣyP(y)P(x∣y)\\Sigma_{y}P(y)P(x|y)Σy​P(y)P(x∣y)是唯一的。这似乎是一个相对较强的假设，但在高斯混合条件下是满足的。Castelli和Cover在1996年发表的一篇论文中对此进行了分析，该论文中类别的条件密度是已知的而类别的先验密度是未知的。 之后，研究半监督学习的兴趣在1990年代增加了，其中大部分原因要归因于自然语言处理与文本分类问题的需要 (Yarowsky, 1995; Nigam et al., 1998; Blum and Mitchell, 1998; Collins and Singer, 1999; Joachims, 1999). 注意，就我们所知，Mert等人于1992年第一次使用术语“半监督学习”来描述同时使用有标签数据和无标签数据进行分类的问题。不过，事实上这种问题在之前就已经有研究了，但是研究的情况和本书中所要讲的有所不同，一个例子是(Board and Pitt, 1989). 2 半监督学习何时发挥作用 一个很自然的问题产生了：半监督学习有意义吗？更确切地说，与仅仅使用有标签数据进行学习的情况相比，考虑到无标签数据真的能够产生更加精确的预测吗？你可能已经从你手头这本书的厚度猜到了，答案是yes!不过有一个重要的先决条件：无标签数据与分类问题相关，它包含能够阐明数据分布的信息。 用更数学化的表达可以这么说：从无标签数据中获得的关于p(x)p(x)p(x)的信息中 必须包含有用于推断p(y∣x)p(y|x)p(y∣x)的信息。如果不满足这个先决条件，半监督学习无法产生比监督学习更好地结果，强行加入无标签数据进行推断甚至可能产生误导，降低预测的精确程度。这个效应的细节将在第4章深入研究。 只有在假设成立的情况下半监督学习才会有效，因此不必对半监督学习的作用过于惊讶。而且纯监督学习也必须满足某些假设才行。之后将会给出几种假设，事实上第22章会讨论一种在PAC风格的框架下形式化这些假设的方法，其中一种受欢迎的假设可以被形式化如下： 监督学习的平滑性假设（Smoothnss assumption of supervise learningSmoothnss\\ assumption\\ of\\ supervise\\ learningSmoothnss assumption of supervise learning）:如果两个点x1,x2x_{1},x_{2}x1​,x2​距离很近，那么其对应的输出y1,y2y_{1},y_{2}y1​,y2​也应当相应很近。 严格说来，这个假设仅仅指的是连续性而不是光滑性，不过smoothesssmoothesssmoothess这个术语却常常被使用在这里，或许是由于实际问题的回归中yyy常常被当作xxx的光滑函数建立模型。 说的更清楚一点，一旦不满足这个假设，就永远不可能讲一个在有限训练集上训练出来的模型一般化到一个测试集可能是无限情况下。 2.1 半监督学习平滑性假设 我们现在要提出一个对半监督学习有用的平滑性假设的一般化形式，我们称其为“半监督学习平滑性假设（semi-supervised smoothness assumption）”。在监督情况下，在我们先前的假设下，输出会随着距离而平滑变化，现在我们考虑加入输入的密度。这个假设是说预测函数在高密度区域将会比在低密度区域更加平滑： 半监督学习平滑性假设（semi−supervised smoothness assumptionsemi-supervised \\ smoothness\\ assumptionsemi−supervised smoothness assumption）：如果一个高密度区域中的两个点x1,x2x_{1},x_{2}x1​,x2​距离很近，那么其对应的输出y1,y2y_{1},y_{2}y1​,y2​也应当相应很近。 注意，根据传递性，这个假设隐含了一个信息：如果两个点可以通过一个高密度区域中的路径被连接，那么它们的输出值也可能很近。另一方面，如果它们被一个 低维区域隔开，那么它们的输出也就不必较近。 半监督学习平滑性假设同时适用于回归问题与分类问题。在下一部分，我们将展示在分类问题中将它简化为半监督问题常用的假设。目前尚不清楚该假设对于回归问题有多有用。作为替代方案，第23章提出了一种使用无标签数据进行模型选择的方法，该方法同时适用于回归和分类。 2.2 聚类假设 假设我们一直每一类别的点都倾向于聚成一团，那么无标签数据将会有助于精确化分类边界，做法是：可以运行一个聚类算法并用有标签数据给每个聚类指定类别（见第二章）。这个问题的基本的，现在也是经典的假设可以被描述如下： 聚类假设（Cluster assumptionCluster\\ assumptionCluster assumption）：在相同聚类中的点可能属于同一类。 这个假设在类别之间是陡峭的情况下可能是合理的，也就是说：如果分类点一个对象密集的连续体，那么就无法从中辨别出不同的类别。 聚类假设并不是说每一类数据形成一个单一的密集的区域，它仅仅意味着：在通常情况下，我们不会看到两中不同类别的点聚在同一类。 聚类假设可以被看作是上面提出的半监督学习平滑性假设的一个特例，因为一个类通常被定义为一个包含一堆互相之间可以由一条穿过高密度区域的短线连接起来的点组成的点集。 聚类假设的等价的规范化定义如下： 低密度分割（Low density separationLow\\ density\\ separationLow density separation）：决策边界应当落在低密度区域中。 很容易看出来二者的等价性：高密度区域中的决策边界将会把一个聚类切分成两类；同一聚类中的许多不同类别的点将需要决策边界来切割聚类，即穿过高密度区域。 尽管两种形式在概念上是等价的，但是却能产生出不同的算法，我们将在1.3节讨论相关问题。低密度版本还提供了直觉上的理解，为什么这种假设在许多实际问题中都是明智的。 例如，考虑数字识别，并假设人们想学习如何区分手写数字“ 0”与数字“ 1”。 准确地从决策边界获取的采样点将在0到1之间，最有可能是一个看起来像很长的零的数字。 但是有人写下这个“怪异”数字的可能性很小。 2.3 流形假设 流形假设是一个与前述假设不同但是相关，而且形成了几种半监督学习方法的基础的假设，定义如下： 流形假设（Manifold assumptionManifold\\ assumptionManifold assumption）：高维数据（大致）存在于低维流形上。 这个假设怎么发挥作用？一个广为认知的统计方法和学习算法的问题是所谓的维度诅咒（curse of dimensions）（参见11.6.2部分），维度诅咒基于一个事实：物体的体积随着其维度增加呈指数级增长。与之相应的是统计任务需要指数级增长的数据量来保证估计的可靠性。这个问题直接影响到基于输入空间的密度故居的生成式算法。对于判别式算法可能更严重的一个相关的问题是：随着维度的上升，成对的数据看起来更加相似，传递出更少的信息。 不过，如果数据恰好位于低维流形上，学习算法基本上就可以在相应的低维度空间上空间上操作，从而避免维度诅咒。 如上所述，可以认为使用流形的算法可能被视为近似实现了半监督平滑度假设：此类算法使用流形的度量来计算测地距离。 如果我们将流形视为高密度区域的近似值，那么很明显，在这种情况下，半监督平滑度假设降低为应用于流形的监督学习的标准平滑度假设。 请注意，如果流形以弯曲的方式嵌入到高维输入空间中（即，它不仅是子空间），则测地线距离将与输入空间中的测地线距离不同。 通过确保更准确的密度估计和更合适的距离，流形假设对于分类以及回归分析都可能有用。 2.4 转导 如前所述，某些算法本质上是在转导设定（transductive setting）下运行的。 根据Vapnik提出的哲学，高维估计问题应尝试遵循以下原则： VapnikVapnikVapnik 原则（Vapnik′s principleVapnik&#x27;s\\ principleVapnik′s principle）：当解决某个问题的时候，应避免将解决一个更加困难的问题作为中间的一个步骤。 以监督学习为例，其中需要对与某些点x相对应的标签y进行预测。生成式模型估计x的密度作为中间步骤，而判别式模型则直接估计标签。 同样地，如果仅对给定的测试集需要标签预测，则可以认为转导比归纳更直接：归纳方法在整个空间X\\mathcal{X}X上推导函数f:X→Yf:\\mathcal{X}\\rightarrow \\mathcal{Y}f:X→Y，然后在测试点上估计f(xi)f(x_{i})f(xi​)并返回，转导方法则直接估计有限的测试标签集，即仅在测试集上定义的函数f:Xu→Yf:\\mathcal{X}_{u}\\rightarrow \\mathcal{Y}f:Xu​→Y。 请注意，转导（如本书中所定义）并不等同于SSL：一些半监督算法是转导性的，而另一些是归纳性的。 现在假定给定了一种转导算法，该算法产生的解决方案优于对相同标记数据（丢弃未标记数据）训练的归纳算法，则此时的性能差异可能是由于以下两点之一（或其组合）引起的： 转导式方法比归纳式方法更加契合Vapnik原则； 转导式算法用一种类似于半监督学习算法的方式利用了无标签数据。 目前有充分的证据表明第二点对模型提升具有影响，不过尚不清楚有选择地支持第一点的经验结果。特别是，对本书相关基准的评估（第21章）似乎并未暗示转导方法的系统优势。 转导的性质仍然是争论的话题，第25章试图提出不同的意见。 3 算法类别与本书的架构 尽管许多方法不能明确地说来源于以上哪种假设，但大多数算法可以被看作对应或产生于上述假设之一或之几。我们尝试将本书中介绍的半监督学习方法组织为四个大致与基本假设相对应的类。 尽管分类并不总是唯一的，但我们希望这样的组织能够提供一个指导体系，使读者可以更轻松地阅读本书内容。出于同样的原因，本书按“部分”进行组织。每一类SSL算法都有一部分，此外还有一部分侧重于生成式方法。 另外两个部分专门介绍SSL的应用和前景。 在下文中，我们简要介绍每本书各部分所涵盖的内容。 3.1 生成式模型 第一部分展示了半监督学习中生成式模型的历史和前沿，第二章就从这个领域的概览开始谈起。用生成式模型估计条件密度p(x∣y)p(x|y)p(x∣y)， 使用生成模型进行推断涉及条件密度p(x∣y)p(x|y)p(x∣y)的估算。在这样的设定下，任何关于p(x)p(x)p(x)的额外信息都是有用的。一个简单的例子是假定p(x∣y)p(x|y)p(x∣y)是高斯分布的，这样就可以用EM找到每一种类别对应的高斯分布的参数的值。与聚类中使用的标准的标准的高斯分布的唯一不同之处在于与任何带标签的示例关联的“隐藏变量”实际上并未被隐藏，而是已知并且等于其类别标签。它实现了聚类假设（参见第2.2.1节），因为给定的聚类仅属于一个类。 这个小例子强调了了生成模型对半监督学习的不同解释： 在具有边缘密度的基础上添加了一些额外信息的分类问题； 具有额外信息的聚类问题。在标准设定下，这些额外信息就是一个数据子集的标签，但是它也可以采用更一般的约束形式。这是第5章的主题。 生成式方法的一个优点在于：关于问题或数据结构的知识可以通过建模自然地整合进模型中。在第3章中，将EM算法应用于文本数据的应用将展示这一优势。可以观察到，当建模假设不正确时，未标记的数据会降低预测准确性。在第4章中将对这种效果进行深入研究。在统计学习中，在进行推理之前，人们会选择一类函数或先验函数。必须根据事先对该问题的了解来选择它。在半监督学习环境中，如果能从数据的结构中推测到一些目标函数的信息，则在看到未标记的数据后可以更精确地选择先验函数 ：这样通常可以提高函数满足聚类假设的先验概率。从理论上讲，这是获取第22章所述的半监督学习边界的自然方法。 3.2 低维分割 第二部分聚焦于试图通过将决策边界推离无标签点而直接应用低维分割假设的算法。 最常用的达到这个目标的方法是用一个实现此目标的最常见方法是使用最大余量算法，例如支持向量机。最大化未标记点和标记点的边距的方法称为转导SVM（TSVM）。不过，相应的问题是非凸的，因此难以优化。 第6章介绍了一种TSVM的优化算法。从仅对标记数据进行训练的SVM解决方案开始，对未标记的点进行SVM预测标记，并对SVM进行所有点的重新训练。在未标记点的权重缓慢增加的同时进行迭代。另一种可能性是在第7章中建议的半定义SDP松弛编程。 然后，提出了TSVM的两种替代方法，分别用概率论和信息理论框架提出。在第8章中，通过引入空类来增加二进制高斯过程分类，该空类占用了两个常规类之间的空间。作为与TSVM相比的优势，这允许概率输出。 第9章介绍的熵最小化同样具有这一优势。它鼓励类条件概率P(x∣y)P(x|y)P(x∣y)在标记和未标记的点接近1或0。作为平滑度假设的结果，在任何高密度区域中，概率将趋于接近0或1，而类别边界对应于中间概率。 使用熵或信息的另一种方法是在第10章中开发的与数据相关的正则化（data-dependent regularization）。与TSVM相比，这似乎更直接地实现了低密度分离：标准平方范数正则化因子乘以一项反映接近决策边界的密度。 3.3 基于图的方法 在过去的两年中，半监督学习最活跃的领域是基于图的方法（graph-based methods），这是本书第三部分的主题。 这些方法的共同点是，数据由图的节点表示，图的边缘用入射节点的成对距离标记（而缺失的边缘则与无限距离相对应）。如果通过最小化连接两个点的所有路径上的总路径距离来计算两个点的距离，则这可以看作是两个点的测地线距离相对于数据点的流形的近似值。 因此，图方法可以看作是基于流形假设的。 大多数图方法通过利用图拉普拉斯算子（Laplacian）来引用图。令g=(V,E)g=(V,E)g=(V,E)是由w:E→Rw:E\\rightarrow \\mathbb{R}w:E→R给出实际边缘权重的图。这里，边缘e的权重w(e)w(e)w(e)表示入射节点的相似度（丢失的边缘对应于零）。现在，图g=(V,E)g=(V,E)g=(V,E)的加权邻接矩阵（weighted adjacency matrix）WWW定义如下： Wij:={w(e) if e=(i,j)∈E0 if e=(i,j)∈E\\mathbf{W}_{i j}:=\\left\\{\\begin{array}{ll} {w(e)} &amp; {\\text { if } e=(i, j) \\in E} \\\\ {0} &amp; {\\text { if } e=(i, j) \\in E} \\end{array}\\right. Wij​:={w(e)0​ if e=(i,j)∈E if e=(i,j)∈E​ 定义由Dii:=ΣjWijD_{ii}:=\\Sigma_{j}W_{ij}Dii​:=Σj​Wij​组成的诊断矩阵DDD被称作g的度矩阵（degree matrix），接下来就可以用不同的方法定义图拉普拉斯算子。两个最著名的图拉普拉斯算子分别是正规化拉普拉斯算子L\\mathcal{L}L和非正规化拉普拉斯算子LLL： L:=I−D−1/2WD−1/2L:=D−W\\begin{array}{l} {\\mathcal{L}:=\\mathbf{I}-\\mathbf{D}^{-1 / 2} \\mathbf{W D}^{-1 / 2}} \\\\ {L:=\\mathbf{D}-\\mathbf{W}} \\end{array} L:=I−D−1/2WD−1/2L:=D−W​ 许多惩罚加权图边缘的非光滑度的图方法可以看作是一个较为通用的算法系列的不同实例，这将在第11章进行阐述。第13章采用了更理论的观点，并将从连续情况的图形的光滑度转移到离散情况。由此，基于数据的图形表示，提出了不同的正则化器。通常，预测由未标记节点的标记组成。因此，这种算法本质上是转导的，即，它仅返回未标记点上决策函数的值，而不返回决策函数本身。但是，最近进行了一些工作，以扩展基于图的方法以产生归纳解，如第12章所述。 在图上的信息传播还可以考虑到未标记的数据，从而改善给定（可能受到严格监督）的分类。第14章介绍了以这种方式使用有向图的概率方法。 通常，图g是通过以某种其他表示形式计算对象的相似性来构造的，例如，使用欧几里得数据点上的核函数。但是有时原始数据已经具有图形形式。示例包括网页的链接模式和蛋白质的相互作用（请参见第20章）。在这种情况下，边缘的方向性可能很重要。 3.4 表示形式的改变 第四部分的主题是本质上不是半监督的算法，这类算法表现为两步学习： 在所有有标签和无标签数据上应用监督学习算法，但是不考虑已有的标签。例如，这可以是表示形式的更改，也可以是新metric或新kernel的构造。 忽略未标记的数据，并使用新的距离、表示形式或kernel执行纯监督学习。 这可以看作是半监督平滑度假设的直接实现，因为以一种方式更改了表示形式，从而可以节省高密度区域中的小距离。 请注意，基于图的方法（第3部分）与本部分中介绍的方法密切相关：根据数据构建图的过程可以看作是表示形式的无监督更改。因此，第四部分的第15章讨论了这种图形的频谱变换（spectral transforms），以构建kernel.频谱方法（spectral methods）也可以用于非线性降维，如第16章所述。此外，在第17章中，研究了从图形派生的度量，例如，从最短路径派生的度量。 3.5 半监督学习的实际应用 每当未标记数据比标记数据多得多时，半监督学习将是最有用的。如果获取数据点很便宜，但是获取标签会花费大量时间，精力或金钱，则很可能会发生这种情况。在机器学习的许多应用领域中就是这种情况，例如： 在语音识别中，录制大量语音几乎不需要花什么钱，但是标记它需要一些人工来听并键入笔录。 数十亿个网页可直接用于自动化处理，但是要可靠地对其进行分类，人们必须逐一浏览它们。 如今，蛋白质序列是以工业速度获得的（通过基因组测序，计算基因发现和自动翻译），但是要解析三维（3D）结构或确定单个蛋白质的功能可能需要多年的科学工作。 第3章在生成模型的背景下介绍了网页分类。 由于未标记的数据携带的信息少于标记的数据，因此需要大量使用它们才能显着提高预测准确性。这意味着需要快速有效的SSL算法。 第18章和第19章介绍了处理具有大量数据的问题的两种方法。在第18章中，开发了用于加快在第11章中介绍的标签传播方法的方法。在第19章中，显示了cluster kernel是一种有效的SSL方法。 第19章还介绍了半监督学习在重要的生物信息学应用中的两种方法中的第一种：蛋白质序列的分类。尽管这里的预测是基于蛋白质序列本身的，但第20章将进行一些更为复杂的设置：这里的信息假定以表征蛋白质相互作用的图表形式出现。存在几个这样的图，必须以适当的方式进行组合。 本书的结尾部分有一个非常实用的章节：与本书相关的基准的介绍和评估（第21章）。旨在给从业者一些提示，说明如何根据问题的性质选择合适的方法。 3.6 概览 本书的最后一部分，第六部分，致力于解释SSL中一些目前正在被研究的最有趣的方向。 到现在为止，这本书基本上只限于分类。 第23章介绍了另一种适用于分类和回归的SSL方法，并从中推导了算法。有趣的是，这似乎不需要第1章中提出的假设。 此外，本书主要介绍了SSL算法。 尽管上面讨论的假设提供了一些有关SSL何时以及为什么起作用的直觉，并且第4章研究了何时以及为什么SSL可能失败，但是对SSL有了透彻的理论理解显然会更令人满意。为此第22章提供了一个PAC框架，该框架产生SSL问题的错误边界。 在第24章中，依据VC bound以及其他理论和哲学概念对归纳式半监督学习和转导式学习进行了比较。 本书以三位机器学习研究人员之间关于半监督学习与转导学习之间的关系（以及两者之间的差异）的假设讨论（第25章）作为结尾。","permalink":"http://yangtf983.github.io/2020/01/09/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%EF%BC%9A%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D/","photos":[]},{"tags":[{"name":"Reinforcement Learning","slug":"Reinforcement-Learning","permalink":"http://yangtf983.github.io/tags/Reinforcement-Learning/"}],"title":"Imitation Learning & Dagger Algorithm","date":"2020/01/02","text":"[0] 说明 进入Reinforcement Learning的世界，一般而言应当是从tabular method学起，但是从imitation learning学起推广到Reinforcrmrnt Learning可以帮助我们更好的理解RL的解决问题的思路和发展目的，因此我们本次继续跟随cs294的脚步，学习Imitation learning的相关知识以及相对简单的Dagger算法。 值得一提的是，IL应当归类于监督学习(Supervised Learning)，但是其可以应用于IRL(Inverse Reinforcement Learning)领域，这也是IL与IRL的重要联系之一。 [1] Terminology &amp; Notation 首先我们应当明白我们探讨的是什么类型的问题，我们称之为时序决策问题(sequrntial decision)，这类问题的主要特点是具有马尔科夫性(markov property)，也即是说，在当前条件已知的情况下，未来和过去是独立的。换句话说，如果我掌握了现在的情况，那么从纯理性的角度讲，是否掌握过去的情况对我做出下一步决策毫无影响。其数学定义参照下式： P(X(tn)≤xn∣X(tn−1)=xn−1,…,X(t1)=x1)=P(X(tn)≤xn∣X(tn−1)=xn−1)P( X(t_{n} )\\leq x_{n} | X(t_{n-1})=x_{n-1}, \\ldots,X( t_{1} ) = x_{1} ) = P( X(t_{n} )\\leq x_{n} | X(t_{n-1})=x_{n-1}) P(X(tn​)≤xn​∣X(tn−1​)=xn−1​,…,X(t1​)=x1​)=P(X(tn​)≤xn​∣X(tn−1​)=xn−1​) 其中，t1&lt;t2&lt;…&lt;tnt_{1}&lt;t_{2}&lt;\\ldots &lt;t_{n}t1​&lt;t2​&lt;…&lt;tn​. 对于这样一个决策问题，系统首先需要一个输入，也即是系统的观测值，记为oto_{t}ot​。众所周知，神经网络会对oto_{t}ot​进行一系列处理，得到一个可能采取的行为的分布函数（对于有限行为决策，即为分布列），我们称这个结果称为我们的决策策略。我们将行为记作ata_{t}at​，策略记作πθ(at∣ot)\\pi_{\\theta}(a_{t}|o_{t})πθ​(at​∣ot​). 上面都很好理解，但是我们这里还要定义另外一个符号，这个符号叫做状态，记为sts_{t}st​. 定义sts_{t}st​是因为oto_{t}ot​并不总是无损的。举例来说，假如我有一张图片，图片上是一辆行驶的汽车，我们将这张图片输入给一个神经网络系统，oto_{t}ot​就是这张照片的像素，是一个或几个二维数组，我们可以通过神经网络对这张图片进行分析，得到这张图片上各种信息，但是我们永远不能判断车后面有没有一只动物，有可能这辆车刚刚好挡住了一只动物，但是这只动物不在照片里，或者说不在oto_{t}ot​里，但是我们说它在sts_{t}st​里，sts_{t}st​包含此刻决策相关的所有信息，但是oto_{t}ot​不一定。 我们再通过下图来理解这个决策系统： 我们将ot=sto_{t}=s_{t}ot​=st​的系统称为完全观察(fully observed)系统，将ot≠sto_{t} \\neq s{t}ot​​=st的系统称为部分观察(partial observed)系统。显然，对于一个partial observed问题，若我们只有oto_{t}ot​而没有sts_{t}st​，ot−1o_{t-1}ot−1​对我们做出下一步行动是有用的，因为它可能帮助我们发现车后究竟有没有动物。此时，我们需要一个决策系统可以不仅仅考虑到oto_{t}ot​，还可以考虑到ot−1o_{t-1}ot−1​甚至于更早的观测值。 [2] Introduce to Behavior Cloning 我们将模仿专家行为进行学习的一类算法又称为behavior cloning，这类算法以专家行为作为数据标签，通过对数据集进行监督学习得到模型πθ(at∣ot)\\pi_{\\theta}( a_{t}|o_{t} )πθ​(at​∣ot​)，以此进行决策。 我们以自动驾驶为例，behavior cloning的一个可能做法是找一些驾驶熟练的人类司机，让他们驾车行驶，并且通过摄像头拍摄沿路情况，并记录下司机的行为(左转或者右转)作为数据标签，接着用这些有标签的数据进行监督学习，因此本质上IL/behavior cloning是监督学习的分支，下图是我们所举例子的一个直观表示： [3] DAgger Algorithm DAgger算法的诞生是为了解决behavior cloning中一个很严重的问题，因此我们先来看看这个问题。 前面已经讲过，我们在使用behavior cloning时采用了监督学习算法。我们都知道，无论数据量有多大，采用的监督学习算法有多准确，都可能存在一种情况，就是在行驶过程中某一步和原来的专家行为产生偏差，因为我们不是完全的模仿专家行为，而是采用了一个监督学习模型(如果完全模仿专家行为，则模型的利用场景过于有限，一旦碰到任何没有碰到的情况，汽车就会手足无措，而在行驶过程中，可能发生的各种情况是难以模拟遍的)。一旦这个偏差产生，由于它不是在数据集中的，做出的进一步决策很可能使其朝着进一步偏差的方向进行，而随着汽车碰到的状况与数据集的偏差越来越大，其做出的决策则越来越不具有合理性，偏离原数据集的速度会越来越快，放在自动驾驶上，就是偏离车道的速度加快。这一现象在很多实验中得到了验证，下图是一张验证这一现象的模拟图： 针对之前提到的自动驾驶，NVIDIV提出了一种解决方案，就是同时拍摄左中右三个方向的情况，给左侧的数据打上标签“右”，右侧的数据打上标签“左”,这样当检测到车头方向偏离时，及时调整回去。经过实际检验，这种方案的效果很不错。 这个问题看似已经解决了，但是我们还是要思考一下，有没有更加通用的方法，使得我们不用重新获取这些数据，并且可以用于一些其他领域的behavior cloning问题？ 还是用自动驾驶举例，想像一下，如果我们能够得到整个轨迹的分布，也就是所有的汽车可能碰到的情况，然后让专家给出所有的label，这样无论汽车遇到什么情况，都可以精确按照专家给出的建议行驶。但是这种假设的问题在于，我们无法得到一个有无数种可能的情况的分布，即便是我们找出了所有情况，也无法承担让专家给所有情况打分的高额花费。 那我们能不能找到一种方法，来只对个数不多的有限种情况进行打分，并尽量让汽车在这些情况能够给出有效指导的范围内行动呢？ 我们将原始数据集记为data，我们知道，我们的模型能够应对的情况是Pdata(ot)P_{data}(o_{t})Pdata​(ot​)，而汽车在行驶过程中真正面对的情况是Pπθ(ot)P_{\\pi_{\\theta}}(o_{t})Pπθ​​(ot​) ，当data ≠πθ\\neq \\pi_{\\theta}​=πθ​ 时，自然有 Pdata(ot)≠Pπθ(ot)P_{data}(o_{t})\\neq P_{\\pi_{\\theta}}(o_{t})Pdata​(ot​)​=Pπθ​​(ot​)，因此产生了偏离。如果我们能找到一种方法使得Pdata(ot)≠Pπθ(ot)P_{data}(o_{t})\\neq P_{\\pi_{\\theta}}(o_{t})Pdata​(ot​)​=Pπθ​​(ot​)，便能解决这个问题。因此我们的一个思路是尽可能让data=πθdata=\\pi_{\\theta}data=πθ​. 依照这种思路，我们找到了DAgger(Dataset Aggregation)算法。DAgger是一种很简单的方法，并且已经被证明在在线学习的情况下这种算法是收敛的。下面介绍一下DAgger算法的流程： train πθ(at∣ot)\\pi_{\\theta}(a_{t}|o_{t})πθ​(at​∣ot​) from human data D= {o1,a1,…,oN,aN}\\{o_{1},a_{1},\\ldots,o_{N},a_{N}\\}{o1​,a1​,…,oN​,aN​}. run πθ(at∣ot)\\pi_{\\theta}(a_{t}|o_{t})πθ​(at​∣ot​) to get dataset Dπ={o1,…,oM}D_{\\pi}=\\{o_{1},\\ldots,o_{M}\\}Dπ​={o1​,…,oM​}. Ask human to label DπD_{\\pi}Dπ​ with actions ata_{t}at​. Aggregate: D→D⋃Dπ.D\\rightarrow D\\bigcup D_{\\pi}.D→D⋃Dπ​. repeat step1 ~ step4. 这里有一个问题，是cs294中的一个学生提出来的：为什么step4要将D和DπD_{\\pi}Dπ​聚合，而不是用DπD_{\\pi}Dπ​代替D？ 视频中老师给出了两个原因：(1)这样做效果不好;(2)DAgger算法收敛的基础是在线学习。 但是视频中没有给出更详细的解释，我理解了一下，可以大概给出一种解释方法： 首先，behavior cloning使用的是监督学习方法，DAggger算法的目的是尽可能让车的行驶路径在我们的模型的无偏差的计算范围内，即便是偏差了也要尽可能是我们考虑过的偏差情况。那么如果我们不聚合D和DπD_{\\pi}Dπ​，我们也能仅仅用DπD_{\\pi}Dπ​去训练一个新模型，因为这样同样是部分的，与用D训练一个新模型并没有本质差别，因此如果我们每一步仅仅用新的DπD_{\\pi}Dπ​训练模型，最后必然要将所有循环中得到的数据进行一次聚合再进行训练得到使用的模型，这样训练的模型收敛的可能显然不如每一次循环将所有已有的数据聚合起来进行训练的收敛的可能大（直观理解），而后者已经被证明是收敛的。 其次，我们也可以考虑在不聚合数据的情况下每次不去重新训练模型，而是在已有模型的基础上训练模型，相当于迁移学习。但是这种方法效果无法超过每次都聚合的方法，这是很多实验的结果。当然这种方法比较节省时间，所以当时间不充足或者计算资源较少的情况下可以使用这种方法，但是得到的效果不会太好。 理解完了DAgger算法，下面自然就要理解一下这个算法的缺陷。 事实上，除去一些所有监督学习方法的共同缺陷外，这个算法的缺陷并不多，其中值得我们认证考虑的缺陷只有一个，就是我们如何划算的给所有数据打标签？ 将DAgger算法迭代越多，则得到的数据量越大，得到的模型效果越好，但是面对增加的数据量，打标签的花费也在上升，有些时候打标签花费比较廉价，但有时可以很昂贵，尤其是面对巨大的数据量，一般最后都不会太廉价。我们还没有考虑其他方面带来的花费，比如采集数据。此外，打标签有时也不是一件简单的事情。仍然以自动驾驶为例，让司机通过看录像打标签，很可能比直接开车做出正确选择的概率会小一点，虽然我们无法直接得到这个概率差别是多少。 谈到这里，我们自然会诞生一个疑问，就是我们怎么克服数据量这个缺陷？显然数据量的需求来源于模型的要求，要克服这个缺陷，我们就要问一个问题，就是能不能找到一种不需要大量数据的模型来完成IL这件事？ 针对这个问题，我们可以提出一种思路，但是真正的解决还是要用到RL模型。下面是这种思路的想法： DAgger addresses the problem of distributional “drift” What if our model is so good that it doesn’t drift? Need to mimic expert behavior very accurately. But don’t overfit! [4] Why might we fail to fit the expert? 下面我们抛开具体算法来讨论一个问题：为什么我们拟合专家行为可能会失败？ 其中一个原因是专家行为可能是非马尔科夫行为(Non-Markovian behavior)。我们之前已经提到过，sequential decision的基本假设是markov property，这一特性在我们的模型中体现为我们使用的监督学习模型是无记忆性的，事实上，大多数监督学习方法都是无记忆性的。但是现实生活中，习惯、心情等事物都可能使一个人在某时刻更加偏爱某种决策，甚至于这种影响有时候其本人都无法察觉。如当我选择一条上班的路线时，我一般会选择最常走的那条，但是某天心情好，我就很想走一条之前未曾走过的路线。这两种选择都不是markovian behavior， 因为正确判断我要选择的路线不仅仅需要知道我当时的状态（心情），还需要知道我之前哪些路走得多，哪些路走得少。 IL面对的另一个重要问题是多方式行为(Multimodal behavior)。意思是说，有些情况下，我有多种方法达到同种效果，但是不能综合这些方法去达到这种效果，只能选择其一。例如，当我想绕过面前的一棵树时，我可以从左边绕过，也可以从右边绕过，但是我不能综合两种方法从中间绕过。 下面我们针对两种问题分别给出一些对应的解决思路，由于笔者能力有限，暂时对这些问题不能给出更深入的理解，其中大部分是对cs294课程的重述，有感兴趣的读者可以进一步研究，同时，笔者会在对此部分有进一步理解时在博文中更新本部分。 [a] Non-Markovian behavior 前面说过，这个问题导致的结果就是：我们在做出下一步决策时，不仅仅要考虑到现在的状况，还要考虑到之前的状况。也就是说，我们要从计算 πθ(at∣ot)\\pi_{\\theta}(a_{t}|o_{t})πθ​(at​∣ot​) 转变为计算 πθ(at∣o1,…,ot).\\pi_{\\theta}(a_{t}|o_{1},\\ldots,o_{t}).πθ​(at​∣o1​,…,ot​). 要达到这种效果也很简单，就是采用递归神经网络，这类神经网络已经有了很大发展，相信大家并不陌生，比如iphone的语音助手siri，它可以联系用户的前几句话来理解用户的意图，其根本原因就是采用了递归神经网络。 [b] Muitimodal behavior [I] 离散模型 对于离散模型的multimodal behavior行为，我们要做的是从一些有限的方案中选出一种方案，只需要在神经网络最后加上一层softmax层。 [II]连续结构 对于能够采取连续行为的问题，模型最后的输出应当能够对应连续行为的决策，这类问题一般采用高斯分布实现（有时也用均方误差，等价于使用高斯分布，因为均方误差即高斯分布的对数概率），实现方式一般有三种，各有优劣： [i] Output mixture of Gaussians π(a∣o)=ΣiωiN(μi,Σi)\\pi(a|o)=\\Sigma_{i}\\omega_{i}N(\\mu_{i},\\Sigma_{i})π(a∣o)=Σi​ωi​N(μi​,Σi​) 特点：对低维决策效果较好。 [ii] Latent varible models step1. 不改变输出结构，仍以单高斯分布模型的简单形式存在； step2. 在神经网络底部输入额外的随机数（分布不唯一）。 难点：如何让神经网络有效利用噪声。 [iii] Autoregressive discretization step1. 从一个决策维度开始，一个神经网络增加一个决策维度； step2. 每个网络都结合前一个网络的输出和新的条件得到新的输出，并且决策的维度增加一。 特点：简单，但对网络结构改变较大，需要重新设计。 [5] Other topics in imitation learning Structured prediction 这一领域对输出的结构往往有一定要求，应用比较广泛的如机器翻译领域等。 Inverse reinforcement learning 通过模仿，反向理解行为的目的，之后寻找更好的方法来达到该目的。 [6] Imitation learning: What’s the problem? Data is typical finite; Humans are not good at providing some kinds of actions; Humans can learn autonomously; can our mechines do the same? 参考资料 cs294-112, lec-2","permalink":"http://yangtf983.github.io/2020/01/02/Imitation_Learing&Dagger_Algorithm/","photos":[]},{"tags":[{"name":"Reinforcement Learning","slug":"Reinforcement-Learning","permalink":"http://yangtf983.github.io/tags/Reinforcement-Learning/"}],"title":"A simple introduce of reinforcement learning","date":"2019/01/30","text":"[0] 说明 这是本博客RL系列的第一篇文章，旨在对RL进行一个简单的介绍，不涉及高深的理论。作为一个RL初学者，本博客的一系列RL文章都将是我在学习过相应的课程后写的，我会在学习已有资料的基础上加上自己的理解，但是一般不会注明哪些是自己的理解，哪些是材料中的观点。当然我的理解难免会有差错，因此我会在每一篇博文后注明我的学习材料，有需要的读者可以自行寻找阅读，若发现我的差错，欢迎批评指正。 [1] How do we build intelligent machines? 首先我们尝试理解一个大问题，就是：How do we build intelligent machines?理解这个问题是为了解决本节一个更为核心的问题：What is reinforcement learning, and why should we care? 机器是能够根据设定的指令行事的一种无生命的形式，比如手表等物品，但是我们不说手表是智能机器。我们在智能手机名称前加了个智能，但是我们仍然不认为它是智能机器。甚至更复杂一点的，如飞机、火箭、宇宙飞船，集结了人类无数智慧结晶的东西，我们仍然不说它们是智能机器。但是，我们会说microsoft的情感机器人是智能机器、击败人类围棋冠军的alphago是智能机器，哪怕它们目前技术含量并不一定比得上火箭之类的“非智能机器”，但是它们能够对人类的行为做出反应，并且这些反应并不是人类提前用判断语句判断好写入的，而是它们自己学习并适应的，只要给他们足够的时间，他们可以适应越来越多的情况。所以我们可以说：Intelligent machines must be able to adapt. 要想建立这种智能机器，我们就必须要有一种能够让它们学习适应各种环境的方法，这种方法几十年前就有人提出了，但是受限于算力和数据的问题，没有得到很好的发展，直到近年这些问题被解决后才又一次受到大众的观众，这种方法就是**“deep learning”(简称DL)** DL是一种可以处理非结构化环境(unstructures environment)的方法，你也可以说它是一种黑箱算法，它改变了我们传统的自己定义特征的方法，转而由数据进行训练。在看不出过拟合现象（训练集效果很好，预测集效果很差）的前提下，你可以尽可能地增加层数，即便你也不知道每一层分别得到了什么特征。这种方法简化了解决问题的难度，同时具有很高的通用性（不是说迁移能力强），前提是你要有足够的数据。 但是在reinforcement learning(简称RL)诞生并与DL结合前，DL的应用仅仅是在处理感知信息上，比如对数据进行分类，不管数据是有标签的还是无标签的都可以做到，我们分别称之为supervised learning和unsupervised learning，直到RL诞生，才提供了一种让机器学习行为的方式，后来将RL于DL结合，等于是将“是什么”(DL)和“怎么做”(RL)结合到了一起，诞生了一种说法叫做deep reinforcement learning(简称DRL)，人类得以开始建造intelligient mechines.RL领域现在备受关注的主要原因也是由于其与DL方法的结合产生了很多新的成果，甚至于我们可以说RL取得的最大成功就是与神经网络和深度网络的结合。在后续系列博客中，我们将不再区分RL与DRL，一般均用RL代替。 Two ways to build intelligent learning 标准方法：解析→分块生成→组合 学习方法：建立学习算法→自动学习功能 [2] What is deep RL, and why should we care? 在上一部分中我们已经介绍了什么是DL以及什么是DRL，并且向大家简单说明了在使用DL之前的方法（我们称之为“标准方法”）是怎样的。简而言之，标准方法中，特征是人为定义和提取的，机器的行为策略也是人为定义的。下面我们将总结一下引入RL后的改变，帮助大家理解Why should we care about RL? 计算机视觉领域 标准方法：人为提取每一层特征，很复杂，甚至可以作为一个人整个博士期间的研究工作； DL：end-to-end training (端到端) 优点： 减少人工量；2. 找到的方法往往比用标准方法更好。 游戏领域 标准方法：人为建立许多特征与策略； DL：end-to-end training (端到端) 优点： 减少人工量；2. 找到的方法往往比用标准方法更好。 [3] What does end-to-end learning mean for sequential decision making? 我们先来解释一下什么是end-to-end learning. 这个问题的解释其实和你如何定义一个完整的过程有关，假如我们现在定义一个人的一个完整的反射过程是从他接收到环境信息到他对环境做出反应这一整个过程，那么一个end-to-end learning的意思就是我的学习系统只需要这两个端口的信息（也就是环境信息和人做出的反应）就可以进行学习，而不必对中间的过程再进行拆解。 举个例子，假如你现在在野外，你接受到的环境信息是看到了一只凶猛的野生老虎，一个end-to-end learning系统会直接告诉你快点逃跑（当然如果你有武器你可以选择一搏，我们这里只用一般情况做例子），如果你想问这个系统为什么发出这样的指令，它会告诉你不跑你很可能会死掉，但不会告诉你任何判断的中间分析过程。而如果是其他方法，可能会将这个反射过程分解，分解为信息处理系统与决策系统，在信息处理系统中，它专注于得到信息的精准描述，如判断是不是老虎，如果是，再判断是不是动物园的老虎，判断发现这是一只野生老虎，再判断你是否有武器，发现你没有，这个系统的工作就算结束了，接着把这些信息发送给决策系统，决策系统受到信息：你正面对一只野生老虎，没有武器。接着，给你下达了逃跑的指令。想一想如果你问这个系统你为什么需要逃跑它会怎么回答你？它很可能把信息处理系统的结果告诉你，然后说根据这个信息，判断结果你需要逃跑。这就是二者的区别。 再考虑一下它们的实现机理，发现什么不同了吗？在后面的决策过程中，这个系统根本不需要知道如果你不逃跑会有什么后果，我们只需要把指令写进去，它进行简单的逻辑判断就可以发出决策，但是在前者中，你必须知道哪一种行为会得到什么后果，并且知道你需要什么样的后果，当然这里的后果可以通过不断的尝试得到，因为我们可以通过计算机模拟，不像前面举的例子，计算机模拟中尝试后出现不满意后果是完全可以接受的。 实际上我们完全可以这样总结，end-to-end learning在sequential decision中最大的特点就是需要知道不同的结果是好是坏。 Conclusion1: Deep mdoels are what allow reinforcement learning algorithms to solve complex problems end-to-end. [4] 从机器学习到RL 前面的部分中我们已经讲了很多对RL的理解，下面我们引用RL领域一本经典书籍中对RL的一个说明作为其定义，为后面的叙述做铺垫。这本书是:Reinforcement learning: An introduce，关于该定义的详细解释可以自己从书中去找。 Beyond the agent and the environment, one can identify four main subelements of a reinforcement learning system: a policy, a reward signal, a value function, and, optionally, a model of the environment. 结合该定义与之前的例子，我们很容易知道，实现一个RL方法，我们至少需要三个信息：action(行为),observation(观测值),reward(奖励)。下面我们通过重构分类问题以及NPL问题来说明一种观点：RL问题是大多数其他机器学习的一种更一般的表现形式。 分类问题→RL action:输出的标签 observation:图像的像素 reward:分类正确率 NPL(以翻译为例)→RL action:翻译 observation:原语言 reward:翻译质量 [5] Why should we study this now? 这里，我们不加解释的给出三条理由，相信大家能够理解： Advances in deep learning Advances in reinforcement learning Advances in computational capability 一些成功的例子可能更能够让你感受到这个领域目前的发展前景： 用Q-learning学习玩游戏 用policy training控制机器人 alphago, alphastar [6] What other problems do we need to solve to enable real-world sequential decision making? Beyond learning from reward 基础RL方法处理最大化奖励问题，但是sequential decision还涉及其他方面，这对RL方法提出了新的要求。 Where do rewards come from 现实世界中的奖励函数很难得到，甚至在某些事情上，感知做完了和完成这件事一样困难。 Are there other forms of supervision Learning from demonstrations（模仿、推断） Learning from observing the world Prediction [7] Why deep RL? deep = can process complex sensory input and also compute really complex functions RL = can choose complex actions [8] What can deep learning &amp; RL do well now? 有明确的已知简单规则的事物，如围棋； 有原始感觉的输入以及足够经验的事物，如机器人； 通过专家行为的模仿学习. [9] What has proven challenging so far? 学习速度：DRL学习速度比人类慢很多； 迁移能力弱； 难以找到合适的奖励函数，奖励函数对于学习行为与学习速度都至关重要； 应该大力发展基于模型的or无模型的RL. 参考资料 cs294-112, lec-1 Reinforcement Learning: An Introduction","permalink":"http://yangtf983.github.io/2019/01/30/A_simple_introduce_of_RL/","photos":[]},{"tags":[{"name":"Reinforcement Learning","slug":"Reinforcement-Learning","permalink":"http://yangtf983.github.io/tags/Reinforcement-Learning/"}],"title":"Reinforcement Learning学习计划","date":"2019/01/28","text":"A simple introduce of reinforcement learning 建立对RL的认识和理解，帮助进一步学习。 Imitation Learning &amp; Dagger Algorithm 从imitation learning开始，学习相对简单的Dagger算法，并理解为什么要发展RL. 3.","permalink":"http://yangtf983.github.io/2019/01/28/Reinforcement-Learning%E5%AD%A6%E4%B9%A0%E8%AE%A1%E5%88%92/","photos":[]}]}