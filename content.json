{"meta":{"title":"Hexo","subtitle":"","description":"","author":"John Doe","url":"http://yoursite.com","root":"/"},"posts":[{"tags":[{"name":"Reinforcement Learning","slug":"Reinforcement-Learning","permalink":"http://yoursite.com/tags/Reinforcement-Learning/"}],"title":"Imitation Learning & Dagger Algorithm","date":"2020/01/02","text":"[0] 说明 进入Reinforcement Learning的世界，一般而言应当是从tabular method学起，但是从imitation learning学起推广到Reinforcrmrnt Learning可以帮助我们更好的理解RL的解决问题的思路和发展目的，因此我们本次继续跟随cs294的脚步，学习Imitation learning的相关知识以及相对简单的Dagger算法。 值得一提的是，IL应当归类于监督学习(Supervised Learning)，但是其可以应用于IRL(Inverse Reinforcement Learning)领域，这也是IL与IRL的重要联系之一。 [1] Terminology &amp; Notation 首先我们应当明白我们探讨的是什么类型的问题，我们称之为时序决策问题(sequrntial decision)，这类问题的主要特点是具有马尔科夫性(markov property)，也即是说，在当前条件已知的情况下，未来和过去是独立的。换句话说，如果我掌握了现在的情况，那么从纯理性的角度讲，是否掌握过去的情况对我做出下一步决策毫无影响。其数学定义参照下式： P(X(tn)≤xn∣X(tn−1)=xn−1,…,X(t1)=x1)=P(X(tn)≤xn∣X(tn−1)=xn−1)P( X(t_{n} )\\leq x_{n} | X(t_{n-1})=x_{n-1}, \\ldots,X( t_{1} ) = x_{1} ) = P( X(t_{n} )\\leq x_{n} | X(t_{n-1})=x_{n-1}) P(X(tn​)≤xn​∣X(tn−1​)=xn−1​,…,X(t1​)=x1​)=P(X(tn​)≤xn​∣X(tn−1​)=xn−1​) 其中，t1&lt;t2&lt;…&lt;tnt_{1}&lt;t_{2}&lt;\\ldots &lt;t_{n}t1​&lt;t2​&lt;…&lt;tn​. 对于这样一个决策问题，系统首先需要一个输入，也即是系统的观测值，记为oto_{t}ot​。众所周知，神经网络会对oto_{t}ot​进行一系列处理，得到一个可能采取的行为的分布函数（对于有限行为决策，即为分布列），我们称这个结果称为我们的决策策略。我们将行为记作ata_{t}at​，策略记作$ \\pi_{\\theta}(a_{t}|o_{t}) $. 上面都很好理解，但是我们这里还要定义另外一个符号，这个符号叫做状态，记为sts_{t}st​. 定义sts_{t}st​是因为oto_{t}ot​并不总是无损的。举例来说，假如我有一张图片，图片上是一辆行驶的汽车，我们将这张图片输入给一个神经网络系统，oto_{t}ot​就是这张照片的像素，是一个或几个二维数组，我们可以通过神经网络对这张图片进行分析，得到这张图片上各种信息，但是我们永远不能判断车后面有没有一只动物，有可能这辆车刚刚好挡住了一只动物，但是这只动物不在照片里，或者说不在oto_{t}ot​里，但是我们说它在sts_{t}st​里，sts_{t}st​包含此刻决策相关的所有信息，但是oto_{t}ot​不一定。 我们再通过下图来理解这个决策系统： ![sequrntial decision diagram](Imitation_Learing&Dagger_Algorithm/markov~1.png) 我们将ot=sto_{t}=s{t}ot​=st的系统称为完全观察(fully observed)系统，将ot≠sto_{t}\\neq s{t}ot​​=st的系统称为部分观察(partial observed)系统。显然，对于一个partial observed问题，若我们只有oto_{t}ot​而没有sts_{t}st​，ot−1o_{t-1}ot−1​对我们做出下一步行动是有用的，因为它可能帮助我们发现车后究竟有没有动物。此时，我们需要一个决策系统可以不仅仅考虑到oto_{t}ot​，还可以考虑到ot−1o_{t-1}ot−1​甚至于更早的观测值。 [2] Introduce to Behavior Cloning 我们将模仿专家行为进行学习的一类算法又称为behavior cloning，这类算法以专家行为作为数据标签，通过对数据集进行监督学习得到模型πθ(at∣ot)\\pi_{\\theta}( a_{t}|o_{t} )πθ​(at​∣ot​)，以此进行决策。 我们以自动驾驶为例，behavior cloning的一个可能做法是找一些驾驶熟练的人类司机，让他们驾车行驶，并且通过摄像头拍摄沿路情况，并记录下司机的行为(左转或者右转)作为数据标签，接着用这些有标签的数据进行监督学习，因此本质上IL/behavior cloning是监督学习的分支，下图是我们所举例子的一个直观表示： ![behavior cloning diagram](Imitation_Learing&Dagger_Algorithm/behavior.png) [3] DAgger Algorithm DAgger算法的诞生是为了解决behavior cloning中一个很严重的问题，因此我们先来看看这个问题。 前面已经讲过，我们在使用behavior cloning时采用了监督学习算法。我们都知道，无论数据量有多大，采用的监督学习算法有多准确，都可能存在一种情况，就是在行驶过程中某一步和原来的专家行为产生偏差，因为我们不是完全的模仿专家行为，而是采用了一个监督学习模型(如果完全模仿专家行为，则模型的利用场景过于有限，一旦碰到任何没有碰到的情况，汽车就会手足无措，而在行驶过程中，可能发生的各种情况是难以模拟遍的)。一旦这个偏差产生，由于它不是在数据集中的，做出的进一步决策很可能使其朝着进一步偏差的方向进行，而随着汽车碰到的状况与数据集的偏差越来越大，其做出的决策则越来越不具有合理性，偏离原数据集的速度会越来越快，放在自动驾驶上，就是偏离车道的速度加快。这一现象在很多实验中得到了验证，下图是一张验证这一现象的模拟图： ![behavior cloning diagram](Imitation_Learing&Dagger_Algorithm/simulation~1.png) 针对之前提到的自动驾驶，NVIDIV提出了一种解决方案，就是同时拍摄左中右三个方向的情况，给左侧的数据打上标签“右”，右侧的数据打上标签“左”,这样当检测到车头方向偏离时，及时调整回去。经过实际检验，这种方案的效果很不错。 这个问题看似已经解决了，但是我们还是要思考一下，有没有更加通用的方法，使得我们不用重新获取这些数据，并且可以用于一些其他领域的behavior cloning问题？ 还是用自动驾驶举例，想像一下，如果我们能够得到整个轨迹的分布，也就是所有的汽车可能碰到的情况，然后让专家给出所有的label，这样无论汽车遇到什么情况，都可以精确按照专家给出的建议行驶。但是这种假设的问题在于，我们无法得到一个有无数种可能的情况的分布，即便是我们找出了所有情况，也无法承担让专家给所有情况打分的高额花费。 那我们能不能找到一种方法，来只对个数不多的有限种情况进行打分，并尽量让汽车在这些情况能够给出有效指导的范围内行动呢？ 我们将原始数据集记为data，我们知道，我们的模型能够应对的情况是Pdata(ot)P_{data}(o_{t})Pdata​(ot​)，而汽车在行驶过程中真正面对的情况是Pπθ(ot)P_{\\pi_{\\theta}}(o_{t})Pπθ​​(ot​) ，当data \\neq \\pi_{\\thrta}时，自然有 Pdata(ot)≠Pπθ(ot)P_{data}(o_{t})\\neq P_{\\pi_{\\theta}}(o_{t})Pdata​(ot​)​=Pπθ​​(ot​)，因此产生了偏离。如果我们能找到一种方法使得Pdata(ot)≠Pπθ(ot)P_{data}(o_{t})\\neq P_{\\pi_{\\theta}}(o_{t})Pdata​(ot​)​=Pπθ​​(ot​)，便能解决这个问题。因此我们的一个思路是尽可能让data=πθdata=\\pi_{\\theta}data=πθ​. 依照这种思路，我们找到了DAgger(Dataset Aggregation)算法。DAgger是一种很简单的方法，并且已经被证明在在线学习的情况下这种算法是收敛的。下面介绍一下DAgger算法的流程： train πθ(at∣ot)\\pi_{\\theta}(a_{t}|o_{t})πθ​(at​∣ot​) from human data $ D={o_{1},a_{1},\\ldots,o_{N},a_{N}}. $ run πθ(at∣ot)\\pi_{\\theta}(a_{t}|o_{t})πθ​(at​∣ot​) to get dataset $ D_{\\pi}={o_{1},\\ldots,o_{M}}. $ Ask human to label $ D_{\\pi}$ with actions $ a_{t}. $ Aggregate: D→D⋃Dπ.D\\rightarrow D\\bigcup D_{\\pi}.D→D⋃Dπ​. repeat step1 ~ step4. 这里有一个问题，是cs294中的一个学生提出来的：为什么step4要将D和DπD_{\\pi}Dπ​聚合，而不是用DπD_{\\pi}Dπ​代替D？ 视频中老师给出了两个原因：(1)这样做效果不好;(2)DAgger算法收敛的基础是在线学习。 但是视频中没有给出更详细的解释，我理解了一下，可以大概给出一种解释方法： 首先，behavior cloning使用的是监督学习方法，DAggger算法的目的是尽可能让车的行驶路径在我们的模型的无偏差的计算范围内，即便是偏差了也要尽可能是我们考虑过的偏差情况。那么如果我们不聚合D和DπD_{\\pi}Dπ​，我们也能仅仅用DπD_{\\pi}Dπ​去训练一个新模型，因为这样同样是部分的，与用D训练一个新模型并没有本质差别，因此如果我们每一步仅仅用新的DπD_{\\pi}Dπ​训练模型，最后必然要将所有循环中得到的数据进行一次聚合再进行训练得到使用的模型，这样训练的模型收敛的可能显然不如每一次循环将所有已有的数据聚合起来进行训练的收敛的可能大（直观理解），而后者已经被证明是收敛的。 其次，我们也可以考虑在不聚合数据的情况下每次不去重新训练模型，而是在已有模型的基础上训练模型，相当于迁移学习。但是这种方法效果无法超过每次都聚合的方法，这是很多实验的结果。当然这种方法比较节省时间，所以当时间不充足或者计算资源较少的情况下可以使用这种方法，但是得到的效果不会太好。 理解完了DAgger算法，下面自然就要理解一下这个算法的缺陷。 事实上，除去一些所有监督学习方法的共同缺陷外，这个算法的缺陷并不多，其中值得我们认证考虑的缺陷只有一个，就是我们如何划算的给所有数据打标签？ 将DAgger算法迭代越多，则得到的数据量越大，得到的模型效果越好，但是面对增加的数据量，打标签的花费也在上升，有些时候打标签花费比较廉价，但有时可以很昂贵，尤其是面对巨大的数据量，一般最后都不会太廉价。我们还没有考虑其他方面带来的花费，比如采集数据。此外，打标签有时也不是一件简单的事情。仍然以自动驾驶为例，让司机通过看录像打标签，很可能比直接开车做出正确选择的概率会小一点，虽然我们无法直接得到这个概率差别是多少。 谈到这里，我们自然会诞生一个疑问，就是我们怎么克服数据量这个缺陷？显然数据量的需求来源于模型的要求，要克服这个缺陷，我们就要问一个问题，就是能不能找到一种不需要大量数据的模型来完成IL这件事？ 针对这个问题，我们可以提出一种思路，但是真正的解决还是要用到RL模型。下面是这种思路的想法： DAgger addresses the problem of distributional “drift” What if our model is so good that it doesn’t drift? Need to mimic expert behavior very accurately. But don’t overfit! [4] Why might we fail to fit the expert? 下面我们抛开具体算法来讨论一个问题：为什么我们拟合专家行为可能会失败？ 其中一个原因是专家行为可能是非马尔科夫行为(Non-Markovian behavior)。我们之前已经提到过，sequential decision的基本假设是markov property，这一特性在我们的模型中体现为我们使用的监督学习模型是无记忆性的，事实上，大多数监督学习方法都是无记忆性的。但是现实生活中，习惯、心情等事物都可能使一个人在某时刻更加偏爱某种决策，甚至于这种影响有时候其本人都无法察觉。如当我选择一条上班的路线时，我一般会选择最常走的那条，但是某天心情好，我就很想走一条之前未曾走过的路线。这两种选择都不是markovian behavior， 因为正确判断我要选择的路线不仅仅需要知道我当时的状态（心情），还需要知道我之前哪些路走得多，哪些路走得少。 IL面对的另一个重要问题是多方式行为(Multimodal behavior)。意思是说，有些情况下，我有多种方法达到同种效果，但是不能综合这些方法去达到这种效果，只能选择其一。例如，当我想绕过面前的一棵树时，我可以从左边绕过，也可以从右边绕过，但是我不能综合两种方法从中间绕过。 下面我们针对两种问题分别给出一些对应的解决思路，由于笔者能力有限，暂时对这些问题不能给出更深入的理解，其中大部分是对cs294课程的重述，有感兴趣的读者可以进一步研究，同时，笔者会在对此部分有进一步理解时在博文中更新本部分。 [a] Non-Markovian behavior 前面说过，这个问题导致的结果就是：我们在做出下一步决策时，不仅仅要考虑到现在的状况，还要考虑到之前的状况。也就是说，我们要从计算 πθ(at∣ot)\\pi_{\\theta}(a_{t}|o_{t})πθ​(at​∣ot​) 转变为计算 πθ(at∣o1,…,ot).\\pi_{\\theta}(a_{t}|o_{1},\\ldots,o_{t}).πθ​(at​∣o1​,…,ot​). 要达到这种效果也很简单，就是采用递归神经网络，这类神经网络已经有了很大发展，相信大家并不陌生，比如iphone的语音助手siri，它可以联系用户的前几句话来理解用户的意图，其根本原因就是采用了递归神经网络。 ![RNN](Imitation_Learing&Dagger_Algorithm/RNN~1.png) [b] Muitimodal behavior [I] 离散模型 对于离散模型的multimodal behavior行为，我们要做的是从一些有限的方案中选出一种方案，只需要在神经网络最后加上一层softmax层。 [II]连续结构 对于能够采取连续行为的问题，模型最后的输出应当能够对应连续行为的决策，这类问题一般采用高斯分布实现（有时也用均方误差，等价于使用高斯分布，因为均方误差即高斯分布的对数概率），实现方式一般有三种，各有优劣： [i] Output mixture of Gaussians $ \\pi(a|o)=\\Sigma_{i}\\omega_{i}N(\\mu_{i},\\Sigma_{i}) $ 特点：对低维决策效果较好。 [ii] Latent varible models step1. 不改变输出结构，仍以单高斯分布模型的简单形式存在； step2. 在神经网络底部输入额外的随机数（分布不唯一）。 难点：如何让神经网络有效利用噪声。 [iii] Autoregressive discretization step1. 从一个决策维度开始，一个神经网络增加一个决策维度； step2. 每个网络都结合前一个网络的输出和新的条件得到新的输出，并且决策的维度增加一。 特点：简单，但对网络结构改变较大，需要重新设计。 [5] Other topics in imitation learning Structured prediction 这一领域对输出的结构往往有一定要求，应用比较广泛的如机器翻译领域等。 Inverse reinforcement learning 通过模仿，反向理解行为的目的，之后寻找更好的方法来达到该目的。 [6] Imitation learning: What’s the problem? Data is typical finite; Humans are not good at providing some kinds of actions; Humans can learn autonomously; can our mechines do the same? 参考资料 cs294-112, lec-2","permalink":"http://yoursite.com/2020/01/02/Imitation_Learing&Dagger_Algorithm/","photos":[]},{"tags":[],"title":"Hello World","date":"2020/01/02","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","permalink":"http://yoursite.com/2020/01/02/hello-world/","photos":[]},{"tags":[{"name":"Reinforcement Learning","slug":"Reinforcement-Learning","permalink":"http://yoursite.com/tags/Reinforcement-Learning/"}],"title":"A simple introduce of reinforcement learning","date":"2019/01/30","text":"[0] 说明 这是本博客RL系列的第一篇文章，旨在对RL进行一个简单的介绍，不涉及高深的理论。作为一个RL初学者，本博客的一系列RL文章都将是我在学习过相应的课程后写的，我会在学习已有资料的基础上加上自己的理解，但是一般不会注明哪些是自己的理解，哪些是材料中的观点。当然我的理解难免会有差错，因此我会在每一篇博文后注明我的学习材料，有需要的读者可以自行寻找阅读，若发现我的差错，欢迎批评指正，我的邮箱是yangtf983@163.com，我会认真回复所有探讨相关问题的邮件。 [1] How do we build intelligent machines? 首先我们尝试理解一个大问题，就是：How do we build intelligent machines?理解这个问题是为了解决本节一个更为核心的问题：What is reinforcement learning, and why should we care? 机器是能够根据设定的指令行事的一种无生命的形式，比如手表等物品，但是我们不说手表是智能机器。我们在智能手机名称前加了个智能，但是我们仍然不认为它是智能机器。甚至更复杂一点的，如飞机、火箭、宇宙飞船，集结了人类无数智慧结晶的东西，我们仍然不说它们是智能机器。但是，我们会说microsoft的情感机器人是智能机器、击败人类围棋冠军的alphago是智能机器，哪怕它们目前技术含量并不一定比得上火箭之类的“非智能机器”，但是它们能够对人类的行为做出反应，并且这些反应并不是人类提前用判断语句判断好写入的，而是它们自己学习并适应的，只要给他们足够的时间，他们可以适应越来越多的情况。所以我们可以说：Intelligent machines must be able to adapt. 要想建立这种智能机器，我们就必须要有一种能够让它们学习适应各种环境的方法，这种方法几十年前就有人提出了，但是受限于算力和数据的问题，没有得到很好的发展，直到近年这些问题被解决后才又一次受到大众的观众，这种方法就是**“deep learning”(简称DL)** DL是一种可以处理非结构化环境(unstructures environment)的方法，你也可以说它是一种黑箱算法，它改变了我们传统的自己定义特征的方法，转而由数据进行训练。在看不出过拟合现象（训练集效果很好，预测集效果很差）的前提下，你可以尽可能地增加层数，即便你也不知道每一层分别得到了什么特征。这种方法简化了解决问题的难度，同时具有很高的通用性（不是说迁移能力强），前提是你要有足够的数据。 但是在reinforcement learning(简称RL)诞生并与DL结合前，DL的应用仅仅是在处理感知信息上，比如对数据进行分类，不管数据是有标签的还是无标签的都可以做到，我们分别称之为supervised learning和unsupervised learning，直到RL诞生，才提供了一种让机器学习行为的方式，后来将RL于DL结合，等于是将“是什么”(DL)和“怎么做”(RL)结合到了一起，诞生了一种说法叫做deep reinforcement learning(简称DRL)，人类得以开始建造intelligient mechines.RL领域现在备受关注的主要原因也是由于其与DL方法的结合产生了很多新的成果，甚至于我们可以说RL取得的最大成功就是与神经网络和深度网络的结合。在后续系列博客中，我们将不再区分RL与DRL，一般均用RL代替。 Two ways to build intelligent learning 标准方法：解析→分块生成→组合 学习方法：建立学习算法→自动学习功能 [2] What is deep RL, and why should we care? 在上一部分中我们已经介绍了什么是DL以及什么是DRL，并且向大家简单说明了在使用DL之前的方法（我们称之为“标准方法”）是怎样的。简而言之，标准方法中，特征是人为定义和提取的，机器的行为策略也是人为定义的。下面我们将总结一下引入RL后的改变，帮助大家理解Why should we care about RL? 计算机视觉领域 标准方法：人为提取每一层特征，很复杂，甚至可以作为一个人整个博士期间的研究工作； DL：end-to-end training (端到端) 优点： 减少人工量；2. 找到的方法往往比用标准方法更好。 游戏领域 标准方法：人为建立许多特征与策略； DL：end-to-end training (端到端) 优点： 减少人工量；2. 找到的方法往往比用标准方法更好。 [3] What does end-to-end learning mean for sequential decision making? 我们先来解释一下什么是end-to-end learning. 这个问题的解释其实和你如何定义一个完整的过程有关，假如我们现在定义一个人的一个完整的反射过程是从他接收到环境信息到他对环境做出反应这一整个过程，那么一个end-to-end learning的意思就是我的学习系统只需要这两个端口的信息（也就是环境信息和人做出的反应）就可以进行学习，而不必对中间的过程再进行拆解。 举个例子，假如你现在在野外，你接受到的环境信息是看到了一只凶猛的野生老虎，一个end-to-end learning系统会直接告诉你快点逃跑（当然如果你有武器你可以选择一搏，我们这里只用一般情况做例子），如果你想问这个系统为什么发出这样的指令，它会告诉你不跑你很可能会死掉，但不会告诉你任何判断的中间分析过程。而如果是其他方法，可能会将这个反射过程分解，分解为信息处理系统与决策系统，在信息处理系统中，它专注于得到信息的精准描述，如判断是不是老虎，如果是，再判断是不是动物园的老虎，判断发现这是一只野生老虎，再判断你是否有武器，发现你没有，这个系统的工作就算结束了，接着把这些信息发送给决策系统，决策系统受到信息：你正面对一只野生老虎，没有武器。接着，给你下达了逃跑的指令。想一想如果你问这个系统你为什么需要逃跑它会怎么回答你？它很可能把信息处理系统的结果告诉你，然后说根据这个信息，判断结果你需要逃跑。这就是二者的区别。 再考虑一下它们的实现机理，发现什么不同了吗？在后面的决策过程中，这个系统根本不需要知道如果你不逃跑会有什么后果，我们只需要把指令写进去，它进行简单的逻辑判断就可以发出决策，但是在前者中，你必须知道哪一种行为会得到什么后果，并且知道你需要什么样的后果，当然这里的后果可以通过不断的尝试得到，因为我们可以通过计算机模拟，不像前面举的例子，计算机模拟中尝试后出现不满意后果是完全可以接受的。 实际上我们完全可以这样总结，end-to-end learning在sequential decision中最大的特点就是需要知道不同的结果是好是坏。 Conclusion1: Deep mdoels are what allow reinforcement learning algorithms to solve complex problems end-to-end. [4] 从机器学习到RL 前面的部分中我们已经讲了很多对RL的理解，下面我们引用RL领域一本经典书籍中对RL的一个说明作为其定义，为后面的叙述做铺垫。这本书是:Reinforcement learning: An introduce，关于该定义的详细解释可以自己从书中去找。 Beyond the agent and the environment, one can identify four main subelements of a reinforcement learning system: a policy, a reward signal, a value function, and, optionally, a model of the environment. 结合该定义与之前的例子，我们很容易知道，实现一个RL方法，我们至少需要三个信息：action(行为),observation(观测值),reward(奖励)。下面我们通过重构分类问题以及NPL问题来说明一种观点：RL问题是大多数其他机器学习的一种更一般的表现形式。 分类问题→RL action:输出的标签 observation:图像的像素 reward:分类正确率 NPL(以翻译为例)→RL action:翻译 observation:原语言 reward:翻译质量 [5] Why should we study this now? 这里，我们不加解释的给出三条理由，相信大家能够理解： Advances in deep learning Advances in reinforcement learning Advances in computational capability 一些成功的例子可能更能够让你感受到这个领域目前的发展前景： 用Q-learning学习玩游戏 用policy training控制机器人 alphago, alphastar [6] What other problems do we need to solve to enable real-world sequential decision making? Beyond learning from reward 基础RL方法处理最大化奖励问题，但是sequential decision还涉及其他方面，这对RL方法提出了新的要求。 Where do rewards come from 现实世界中的奖励函数很难得到，甚至在某些事情上，感知做完了和完成这件事一样困难。 Are there other forms of supervision Learning from demonstrations（模仿、推断） Learning from observing the world Prediction [7] Why deep RL? deep = can process complex sensory input and also compute really complex functions RL = can choose complex actions [8] What can deep learning &amp; RL do well now? 有明确的已知简单规则的事物，如围棋； 有原始感觉的输入以及足够经验的事物，如机器人； 通过专家行为的模仿学习. [9] What has proven challenging so far? 学习速度：DRL学习速度比人类慢很多； 迁移能力弱； 难以找到合适的奖励函数，奖励函数对于学习行为与学习速度都至关重要； 应该大力发展基于模型的or无模型的RL. 参考资料 cs294-112, lec-1 Reinforcement Learning: An Introduction","permalink":"http://yoursite.com/2019/01/30/A_simple_introduce_of_RL/","photos":[]},{"tags":[{"name":"Reinforcement Learning","slug":"Reinforcement-Learning","permalink":"http://yoursite.com/tags/Reinforcement-Learning/"}],"title":"Reinforcement Learning学习计划","date":"2019/01/28","text":"A simple introduce of reinforcement learning 建立对RL的认识和理解，帮助进一步学习。 Imitation Learning &amp; Dagger Algorithm 从imitation learning开始，学习相对简单的Dagger算法，并理解为什么要发展RL. 3.","permalink":"http://yoursite.com/2019/01/28/Reinforcement-Learning%E5%AD%A6%E4%B9%A0%E8%AE%A1%E5%88%92/","photos":[]}]}