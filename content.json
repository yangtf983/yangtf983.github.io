{"meta":{"title":"Young's Blog","subtitle":"","description":"keep foolish, keep hungry","author":"Alexis Young","url":"http://yangtf983.github.io","root":"/"},"posts":[{"tags":[{"name":"机器学习基石听课笔记","slug":"机器学习基石听课笔记","permalink":"http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E5%90%AC%E8%AF%BE%E7%AC%94%E8%AE%B0/"}],"title":"机器学习基石笔记14：Regularization","date":"2020/03/19","text":"0 说明 regularization就是给假设加上限制条件后进行最优化的做法。 这一堂课将引入regularization，介绍weight decay regularization、regularization与VC理论的关系，并讲解正则化的一般方法。 1 Regularized Hypothesis Set 上一节课介绍了过拟合，说明了H2H_2H2​比H10H_{10}H10​有时更适合（虽然后者的假设集包含前者），最后介绍了几种避免过拟合的方法。除了数据清洗、删除和扩充，正则化也是一种重要的方法，这里就从上个例子来说明如何在不变动数据的情况下选择出H2H_2H2​而不是H10H_{10}H10​ H2H_2H2​可以看做是H10H_{10}H10​的后8个参数均为0，不过为了将模型更推广一点（因为如果我们知道后八个参数为0那就不需要再求解带约束的H10H_{10}H10​而是应该直接求解H2H_2H2​，而且一般也很难提前知道几次模型最合适），再将H2H_2H2​放松到H(C)H(C)H(C)，即限制所有参数的总和。 H2H_2H2​与H(C)H(C)H(C)是重叠而不是包含的关系，二者有重叠的部分但不存在从属关系。随着C的变大假设H(C)H(C)H(C)集合是越来越大的。像这种对模型的参数加以限制的假设集被称为正则化假设集(regularized hypothesis set) 2 Weight Decay Regularization 这里插入一点个人理解： 由于我们知道更复杂的模型一定可以使EinE_{in}Ein​更小，所以H(C)H(C)H(C)中最好的模型一定在限制区域的边缘。这个很容易证明，因为若假设C′&lt;CC&#x27;&lt;CC′&lt;C，则一定有H(C′)⊂H(C)H(C&#x27;)\\subset H(C)H(C′)⊂H(C)，因此H(C)H(C)H(C)的最优模型一定比H(C′)H(C&#x27;)H(C′)的好，因此H(C)H(C)H(C)的最优模型一定在其与H(C′)H(C&#x27;)H(C′)之间的部分，逐渐扩大C’使其与C无限逼近，就可以得到H(C)H(C)H(C)的最佳模型在限制区域的边缘上。 因此要求的只是边缘上的条件极值，这可以用拉格朗日乘子法，中间的过程见下面几张图，这里不再对其仔细解释，而是聚焦于regularization这个方法。 总之，正常的拉格朗日乘子法是确定了C进而可以求出来λ\\lambdaλ进而求得权重的唯一解的，不过在regularization中C不是一个一开始就确定的值，那么就相当于是未知数比方程数多了1个，那么λ\\lambdaλ和C可以互相确定，因此与其先给出C再求解出λ\\lambdaλ再求解 权重，不如直接尝试不同的λ\\lambdaλ，这样也就相当于尝试不同的C寻找最合的那个。 所以问题变成了求解∇Ein(wREG)+2λNwREG=0\\nabla E_{in}(w_{REG})+\\frac{2\\lambda}{N}w_{REG}=0∇Ein​(wREG​)+N2λ​wREG​=0，进而变成了最小化Ein(w)+λNwTwE_{in}(w)+\\frac{\\lambda}{N}w^TwEin​(w)+Nλ​wTw，其中的λ\\lambdaλ是可以人为选择的参数。值得注意的是，上面几张图中的变换都是以上一个例子为基础的，也就是说限制条件是参数的模长，实际上这个限制条件是可以变的，变化后上面图中的regularizer项wTww^TwwTw也要变成新的限制项。 针对上一个例子而言，可以得到最终结果的解析表达： 由于ZTZZ^TZZTZ是半正定矩阵，所以当λ\\lambdaλ是是正数时，括号内部分的逆矩阵一定存在，此时只接求逆是简单的。 而当regularizer不再是对参数的平方和模长的限制时，无法得到上式，应当采用其他优化方法，例如当Eaug(w)E_{aug}(w)Eaug​(w)可以求导时可以采取之前学过的梯度下降法求最优解。 上图是对这个例子的求解，通过尝试不同的λ\\lambdaλ发现当λ=0.0001\\lambda=0.0001λ=0.0001时就可以做到很好。现实问题不知道目标函数f，则应当通过validation进行判断哪一个结果较好，这是下一节课的内容。 提一个注意事项，当x的取值范围在-1到+1时，其高次项的绝对值非常小，这样就导致高次项的参数需要非常大才能体现出作用，这对于regularization的选参和求解不利，这时一种有效的解决方法是采用Legendre多项式，这种多项式的优点是能够求出来一组正交的多项式，这组多项式中都有次数较低的项，这就限制了Legendre多项式中不同项的绝对值差别不会太大。 3 Regularization and VC Theory 下面看一下用VC理论如何解释regularization方法的合理性： 从图中可以看出，regularization的做法实际上是用最小哈EaugE_{aug}Eaug​代替了最小化EinE_{in}Ein​，这种做法的优点是考虑到了模型复杂度（regularization实际上是对模型复杂的惩罚，本章的例子中C越大模型越复杂），这样当EaugE_{aug}Eaug​较小时不仅可以使得EinE_{in}Ein​也较小，而且当对模型复杂度的惩罚程度与Ω(H)\\Omega(H)Ω(H)较接近时，也可以保证EoutE_{out}Eout​较好。相反，若没有regularizer，选到的模型比较复杂时由于Ω(H)\\Omega(H)Ω(H)较大而无法确保EoutE_{out}Eout​较小。之前的线性回归和PLA之所以可以直接最优化EinE_{in}Ein​是因为模型复杂度已经被次数限制在了一个很小的范围内。 4 General Regularization 与err项的选择类似，regularizer的选择也有三种考量，分别是：target-dependent, plausible, friendly. L1和L2是常用的regularizer，对他们做一个比较： 由于L1L1L1作为regularizer产生的最优解常常在假设区域的顶点（顶点的某些参数为0），因此当已知最终解比较稀疏（sparse）时采用L1是比较合适的。 通过上图可以看出，当stochastic/deterministic噪声变大时，最优的λ\\lambdaλ都会随之变大。 不过现实情况是噪声往往是未知的，选择合适的λ\\lambdaλ就变得有技巧性，如何选择合适的λ\\lambdaλ留待下一次课程。 上图中是一种与本节的例子中不同的regularizer，其特征是对高次项的参数惩罚加大，因此倾向于选择低维的多项式，这种regularizer显然属于target-dependent类型的。","permalink":"http://yangtf983.github.io/2020/03/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B014%EF%BC%9ARegularization/","photos":[]},{"tags":[{"name":"机器学习基石听课笔记","slug":"机器学习基石听课笔记","permalink":"http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E5%90%AC%E8%AF%BE%E7%AC%94%E8%AE%B0/"}],"title":"机器学习基石笔记13：Hazard of Overfitting","date":"2020/03/19","text":"0 说明 这一节介绍overfitting的危害与应对方法 1 What is overfitting 假设dVC⋆d_{VC}^{\\star}dVC⋆​是某个问题的最佳VC Dimension，结合上一节课最后的曲线图看一下dVCd_{VC}dVC​： 图中紫色虚线对应最合适的模型，该处Eout−EinE_{out}-E_{in}Eout​−Ein​最小，EinE_{in}Ein​也足够小。当dVCd_{VC}dVC​变大时，EinE_{in}Ein​下降而EoutE_{out}Eout​增加，这部分属于overfitting；反之，Ein,EoutE_{in},E_{out}Ein​,Eout​同时上升，这一部分属于underfitting underfitting的模型一般不会被选择，因为EinE_{in}Ein​往往达不到要求，而overfitting的模型由于EinE_{in}Ein​很小容易被选中，但是generalization的效果太差，应当避免选择到overfitting的模型。 上图是一个典型的overfitting的例子，从一个二次函数上选择五个点并用四次函数拟合，拟合结果在这五个点上表现完美，但在其余出差别很大。 为了尽量避免overfitting，首先应该知道造成overfitting的原因都有哪些。 概略地说，造成过拟合的原因主要有三点：模型太过复杂；数据存在噪声；数据点较少。 后面将更加相信分析这些原因。 2 The Role of Noise and Data Size 假设现在有两组数据，分别来源于10次模型（且有噪声）和50次模型（无噪声）： 现在分别用2次模型和10次模型拟合这两组数据，发现在两组数据上都是2次模型的结果较好： 从图中可以看到，两组数据在从H2H_2H2​到H10H_{10}H10​的过程中都发生了EinE_{in}Ein​的下降和EoutE_{out}Eout​的上升，根据之前的定义，这是过拟合现象。 这两个过拟合似乎很出乎意料。第一组数据中用与目标函数相同类型的假设过拟合，第二组数据中用与目标函数更接近的模型过拟合。这说明假设与目标函数的接近与否不能作为判断是否过拟合的标志。 造成这一现象的原因可以直接从下面的曲线中得到： 从曲线中可以看出，当数据量较小的时候，H2H_2H2​总是更好。 对于第二个例子，没有噪声，但是H2H_2H2​仍然较好，其实我们可以把模型复杂度当作一种噪声来理解，只是这个噪声是确定的，这个噪声被称为deterministic noise，其定义是假设集HHH中 最好的h∗h^*h∗与目标函数之间的差距。 3 Deterministic Noise 通过实验来说明过拟合程度与噪声水平(σ2\\sigma^2σ2)、模型复杂度(QfQ_fQf​)、数据点数量NNN 之间的关系。 通过设计一个有噪声模型，从中抽取数据，分别采用2次和10次曲线进行拟合，用Eout(g10)−Eout(g2)E_{out}(g_{10})-E_{out}(g_2)Eout​(g10​)−Eout​(g2​)衡量10次模型的过拟合程度，得到下面两张图： 从红色到蓝色表示过拟合程度在减轻甚至g10g_{10}g10​不再过拟合。 称σ2\\sigma^2σ2为stochastic noise，QfQ_fQf​为deterministic noise，从图中可以得到下述结论： 当数据量N减小时，过拟合程度上升； 当stochastic noise上升时，过拟合程度上升； 当deterministic noise上升时，过拟合程度上升。 此外，从上图中的第二张图可以看到，左下角部分有一块红色，与上面部分的红色的交接点应当是Qf=10Q_f=10Qf​=10，从这里可以看出，当模型次数高于目标函数过多时，过拟合程度增加（当Qf=10Q_f=10Qf​=10时，g10g_{10}g10​相对于g2g_2g2​的过拟合程度是最小的）。不过这个结论相对于前三个没有那么重要。 从对过拟合的影响来说，deterministic noise与stochastic noise的影响一样，简单理解来说，假如目标函数没有被包含在假设集HHH中，则无论如何总是有一些部分无法被模型正确描述，这就是由deterministic noise造成的噪声。电脑中的伪随机数就是用一个很复杂的函数生成的，从中就体现了deterministic noise与stochastic noise相同的思想。二者的区别主要在两点：一点是deterministic noise依赖于假设HHH；另一点是当固定x多次取样（由于有随机噪声故多次取样y不同）时deterministic noise不变。 4 Dealing with Overfitting 针对不同的过拟合原因分别提出合适的避免过拟合的方法： data cleaning是修正数据集中的错误标签，data pruning是删除数据集中标签错误的数据，data hinting是通过一些方法利用现有数据集增加更多数据，regularization和validation留待后两节课讲解。","permalink":"http://yangtf983.github.io/2020/03/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B013%EF%BC%9AHazard%20of%20Overfitting/","photos":[]},{"tags":[{"name":"机器学习基石听课笔记","slug":"机器学习基石听课笔记","permalink":"http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E5%90%AC%E8%AF%BE%E7%AC%94%E8%AE%B0/"}],"title":"机器学习基石笔记12：Nonlinear Transformation","date":"2020/03/18","text":"0 说明 本节课从线性模型拓展到了非线性模型，总结了这种变换的注意事项和代价。 1 Quadratic Hypothesis 我们已经讲解了一些线性模型并说明了其可行性，但是有时会碰到用线性模型无论如何无法将EinE_{in}Ein​变得足够小的情况，此时用线性模型就不能保证学到东西了。为此，需要突破线性假设的限制。 上图是一个例子，数据点非线性可分且线性模型也难以将EinE_{in}Ein​做的足够小。但是通过观察加尝试，可以用一个通过原点的圆将数据完美的分开。 那么此时是不是应该建立一个寻找圆形分类器的PLA，再证明其可行性等等，将之前的部分再做一遍呢？ 其实有一种更好更通用的方法，那就是将数据变换到另一个空间使其成为线性可分的，套用PLA的过程和理论即可。 上图是对这个“圆形可分”的问题做的变换，变换后的数据(zn,yn)(z_n,y_n)(zn​,yn​)是线性可分的。我们称这样的变换为特征变换，z1,z2z_1,z_2z1​,z2​为特征。 如上图，改变变化后的模型的权重，对应于不同的变换前的曲线类型，如果再加上其他一次项和二次项，就可以得到一个能够对应所有二次曲线的分类器，z空间内的线性感知机对应的是x空间内的二次曲线（包括次数更低的曲线）。 2 Nonlinear Transform 容易知道z空间中被感知机分割的部分对应在x空间内也是被二次曲线分割开的，因此两个空间有下述关系： 也就是说这样的特征变换把从x空间内找好的分类器的问题转化为从z空间内找好的分类器。 用特征转换求解分类问题的大致步骤： 这个步骤中需要做两次选择： 特征变换Φ\\PhiΦ 线性模型A\\mathcal{A}A 通过类似的方法可以不断提高多项式次数，做到任意n次PLA，任意n次回归。 下面看一个之前提过的特征转换的例子： 这个例子中就是把难以直接用来分类的像素（raw feature）通过我们的知识变成了密度和对称性（concrete feature），是一个典型的特征变换。 特征变换的效果很强，但是也会付出一些代价，这就是下一小节的主题。 3 Price of Nonlinear Transform 虽然非线性变换可以提高假设的适用情况，但并不一定就是合适的，因为变换常常也需要付出代价。下面将介绍两种代价： 上面两张图介绍了两种代价。第一种是计算/存储代价，d个变量组成Q次多项式，次数为Q的项就相当于是把Q个相同的小球同时随机放到d个盒子中（一个盒子可以放多个），可以用隔板法计算，给Q+d-1个位置放d-1个隔板将空间隔离成d块，相邻两个隔板之间的空位置数就是对应项的次数，可以知道这样的项最多有(Q+d−1d−1)\\left(\\frac{Q+d-1}{d-1}\\right)(d−1Q+d−1​)个。再通过将所有次数不大于Q的项的最大数量相加，得到Q次项最多有： (Q+d−1d−1)+(Q+d−2d−1)+…+(dd−1)+1=(Q+dd)\\left(\\frac{Q+d-1}{d-1}\\right)+\\left(\\frac{Q+d-2}{d-1}\\right)+\\ldots+\\left(\\frac{d}{d-1}\\right)+1=\\left(\\frac{Q+d}{d}\\right) (d−1Q+d−1​)+(d−1Q+d−2​)+…+(d−1d​)+1=(dQ+d​) 当Q增大时，参数的数量增大很快，会导致计算/存储困难。 第二种是模型复杂度快速增加，导致dVCd_{VC}dVC​快速增加，VC bound过于宽松。 第二种代价会导致模型结果难以外推（generalization）。下图的例子就反映了这样的问题： 第二个模型虽然EinE_{in}Ein​更小，但是模型复杂度太高，无论是从理论还是从直观视觉上看与EoutE_{out}Eout​的差距都超过第一张图，generalization的效果较差。 如何决定选择多么复杂的模型？ 从上一个例子来看视觉直观似乎是一个不错的选择，但是当维数较高时无法采用。并且在采取视觉直观方法的时候我们通过看数据加上人脑的理解决定的模型，这样有一个很严重的问题就是其实模型加上了“大脑中的”复杂度（这个地方没听太理解，有点哲学的味道，总之就是不能通过人脑决策代替机器学习决策）。 4 Structured Hypothesis Sets 不同多项式次数假设的关系： 从关系图中可以看出，一味增加多项式模型的复杂度虽然可以降低EinE_{in}Ein​，但是Eout−EinE_{out}-E_{in}Eout​−Ein​难以保证，而实际问题中EoutE_{out}Eout​是未知的，不能每个模型计算出来Eout−EinE_{out}-E_{in}Eout​−Ein​进行判断，所以安全的方法是从一次模型开始，逐渐增大次数，当EinE_{in}Ein​能够满足要求时就终止。 事实上，线性模型已经能够满足很多模型的情况了。","permalink":"http://yangtf983.github.io/2020/03/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B012%EF%BC%9ANonlinear%20Transformation/","photos":[]},{"tags":[{"name":"机器学习基石听课笔记","slug":"机器学习基石听课笔记","permalink":"http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E5%90%AC%E8%AF%BE%E7%AC%94%E8%AE%B0/"}],"title":"机器学习基石笔记11：Linear Models for Classification","date":"2020/03/18","text":"0 说明 本堂课在整合之前讲的分类问题后将其延伸到更多更复杂的问题上。 1 Binary Classification 这一节比较一下线性分类、线性回归、logistic回归三种方法在二分类问题上的应用。 三者的简单总结： 之前已经证明过PLA的合理性，那么线性回归和logistic回归的合理性怎么说明？ 其实这个问题之前已经涉及到，就是用损失函数的大小来判断，如果后两者损失函数很小的时候可以保证PLA的损失函数也很小，那我们就可以说后两者用来做二分类是合理的。 对三者的损失函数做一点变换，目的是将h和x换成s，再将y和s换成ys，而ys是一个有实际意义的变量，当其为正时，说明估计值与真实值相同（且绝对值越大说明做出这样估计的把握越大）；当其为负时，说明估计值与真实值相反（且绝对值越大说明做出这样估计所犯的错误越大）。因此ys被称为分类正确分数（classification correctness score）。变换方法如下： 这样我们可以画出三者的函数图像： err0/1err_{0/1}err0/1​是否为0与ys是否大于0完全一致，也就是说err0/1err_{0/1}err0/1​恰好在该惩罚的情况下惩罚。errsqrerr_{sqr}errsqr​在s不太大的情况下做的不错，但是当s太大时过度惩罚。errCEerr_{CE}errCE​的单调性与err0/1err_{0/1}err0/1​相同，但是二者没有明确的大小关系，这一点可以通过对errCEerr_{CE}errCE​改变比例得到errsce=errCEln⁡2err_{sce}=\\frac{err_{CE}}{\\ln2}errsce​=ln2errCE​​完全在err0/1err_{0/1}err0/1​上方。 因此，我们可以对几者得关系作出如下判断： 进而得到三者的优缺点和应用方法： PLA算法在数据线性可分时非常有效，但是缺点是线性可分的数据很难保证。线性回归是最容易优化的，但是损失函数和真实情况在ys较大时不契合。logistic回归也比较容易优化，缺点是当ys很小时惩罚可能太大。 总之，PLA/pocket/logistic回归的结果的合理性都是可以保证的，线性回归有时可以保证，因此常用的方式是把线性回归模型的结果 作为其他三者迭代的初始值。并且前三者在实际应用中logistic回归最受欢迎，由于其容易优化的性质和结果的可靠性。 2 Stochastic Grad. Descent（随机梯度下降法） 比较PLA和logistic回归/pocket算法，我们发现前者每次迭代只需要找到一个点，而后者却需要算所有点的梯度（用梯度下降法解logistic回归）/标签，计算量大大增加，因此这一部分将提出一个新的梯度下降方法来降低计算复杂度，使得logistic回归的求解每次也只需要计算一个点的梯度。 先展示一下之前的logistic回归的梯度下降法的迭代公式： wt+1←wt+η1N∑n=1Nθ(−ynwtTxn)(ynxn)⏟−∇Ein(wt)\\mathbf{w}_{t+1} \\leftarrow \\mathbf{w}_{t}+\\eta \\underbrace{\\frac{1}{N} \\sum_{n=1}^{N} \\theta\\left(-y_{n} \\mathbf{w}_{t}^{T} \\mathbf{x}_{n}\\right)\\left(y_{n} \\mathbf{x}_{n}\\right)}_{-\\nabla E_{\\mathrm{in}}\\left(\\mathbf{w}_{t}\\right)} wt+1​←wt​+η−∇Ein​(wt​)N1​n=1∑N​θ(−yn​wtT​xn​)(yn​xn​)​​ Ein=∑errE_{in}=\\sum errEin​=∑err，上式中的1N\\frac1NN1​是我们之前刻意加上的，为的就是此时看起来EinE_{in}Ein​的梯度像errerrerr的梯度的平均值。这样我们只计算任意一个errerrerr的梯度，它就是EinE_{in}Ein​的梯度的一个点估计。随机梯度下降法（SGD）就是用这个点估计代替真实值，从而降低计算复杂度，每次只需要对一个点求∇err\\nabla err∇err 因此，logistic回归的SGD的迭代公式为： wt+1←wt+ηθ(−ynwtTxn)(ynxn)⏟−∇err(wt,xn,yn)\\mathbf{w}_{t+1} \\leftarrow \\mathbf{w}_{t}+\\eta \\underbrace{\\theta\\left(-y_{n} \\mathbf{w}_{t}^{T} \\mathbf{x}_{n}\\right)\\left(y_{n} \\mathbf{x}_{n}\\right)}_{-\\nabla err\\left(\\mathbf{w}_{t},x_n,y_n\\right)} wt+1​←wt​+η−∇err(wt​,xn​,yn​)θ(−yn​wtT​xn​)(yn​xn​)​​ SGD有两个重要的经验法则： 终止条件 SGD的目的就在于减少计算量，因此终止条件不能是计算∇Ein\\nabla E_{in}∇Ein​使其足够小（这样会导致计算量与梯度下降法相同），只能是迭代足够的步数（有效性由经验表明）。 η\\etaη 经验表明，0.1左右是一个合适的值，可以采用你自己的在0.1左右的“幸运数字”（老师的幸运数字是0.1126） 看一个例子： 由于线性回归的err=(yn−wtTxn)2err=(y_n-w_t^Tx_n)^2err=(yn​−wtT​xn​)2，因此∇err=2(wtTxn−yn)xn\\nabla err=2(w_t^Tx_n-y_n)x_n∇err=2(wtT​xn​−yn​)xn​，因此负随机梯度方向是第四项，这也是SGD解线性回归的迭代方向。 3 Multiclass via Logistic Regression 多分类问题是一种常见的问题，也可以称之为识别。 假设现在有一个有四种类别的数据的图像，下面讨论如何合理的分类。 首先容易想到的一种方法是每次将三种类别合为一类，与第四种类别做一个二分类，一共做四次。 上图中第一行第一张图是四种类别的数据点，第二行是四次二分类的结果，第一行第二张图是最后分类的结果。 从图中可以看到，四个对角上的五边形中的数据点通过这种分类方法是确定的，而四个三角形中的数据都同时被分到了两类，中间的四边形则是同时被分到了四类，从这个模型中无法区别三角形和四边形区域中数据点的具体类别。 为了解决这个问题，我们寄希望于找到一个能够类别概率的模型，这样就可以轻易地将数据点分到其最有可能所属的类别。 之前学过的logistic回归可以做到这一点。 新的方法是建立四次logistic回归模型，每次还是以某一类和其他所有数据进行二分类，结果如下： 由于logistic回归函数θ(s)\\theta(s)θ(s)正比于sss，因此g(x)=arg⁡max⁡k∈yθ(w[k]Tx)g(x)=\\arg \\max_{k\\in y}\\theta(w^T_{[k]}x)g(x)=argmaxk∈y​θ(w[k]T​x)等价于g(x)=arg⁡max⁡k∈yw[k]Txg(x)=\\arg \\max_{k\\in y}w^T_{[k]}xg(x)=argmaxk∈y​w[k]T​x 这种算法的名字为One-Versus-All(OVA) Decomposition，具体流程如下： 这种算法的优点是高效，并且其中的logistic回归模型可以换成任何结果是软标签的方法。 这种方法的缺点也很明显，就是当类别过多或者类别间数据不平衡较大时，将数据分成一类对其他所有类可能非常不平衡，从而导致了分类结果的变差。例如如果较少的类别只占总数的1%，那么始终将所有数据分成其他类的错误就很小，对这种不平衡数据一般的二分类都是会失效的。logistic回归有一种专门适用于多分类的修改模型，被称为multinomial (‘coupled’) logistic regression 虽然有缺点，但OVA仍然是一个很实用且很有效的多分类模型。 4 Multiclass via Binary 针对上一节提出的OVA模型出现的数据不平衡问题，有一个简单的解决办法是每次只考虑两个类别的数据，这种方法被称作OVO(one-versus-one)模型。 每次考虑两个类别，这样N个类别就建立N(N−1)2\\frac{N(N-1)}{2}2N(N−1)​个分类器，用这么多分类器对数据进行分类，最终被分得次数最多的类别胜出，成为OVO模型的分类结果。 如上图，4个类别共建立6个二元分类器（图中第2行），当判断某个数据点的类别时，用六个分类器分别判断一次，最终得票最高的类别获胜（就像比赛冠军一样，因此图中写作tournament champion）。 OVO算法过程： 这种算法的优点是：高效、稳定、可以结合任何二元分类方法。 缺点是：由于需要训练的模型数量是O(K2)O(K^2)O(K2)的，因此类别太大时效率较低（占用空间更大，需要更多训练次数，预测更慢）。 最后看一个例子： 从这个例子中可以看出OVO有时可以显著降低OVA的算法复杂度。","permalink":"http://yangtf983.github.io/2020/03/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B011%EF%BC%9ALinear%20Models%20for%20Classification/","photos":[]},{"tags":[{"name":"机器学习基石听课笔记","slug":"机器学习基石听课笔记","permalink":"http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E5%90%AC%E8%AF%BE%E7%AC%94%E8%AE%B0/"}],"title":"机器学习基石笔记10：Logistic Regression","date":"2020/03/18","text":"0 说明 这一节探讨Logistic回归的问题，这种回归实际上是可以看做是一种soft binary classification问题的解法，这种问题是说数据的标签是硬的（确定是圈或叉），但理想的无噪声数据的标签其实是软的（只有是不同标签的概率值）。Logistic模型的形式给出的就是0到1之间的数字（对概率的估计）。 1 Logistic Regression Problem 现在有一个例子，知道一些人的一些身体指标和有没有患心脏病，要求学习一个函数估计任意一个人患心脏病的概率。该问题的Logistic模型的相关符号如下： s是各项指标的线性加权值，θ(s)\\theta(s)θ(s)是s的sigmoid函数，h(x)h(x)h(x)是最终的假设写法。 至于为什么sigmoid函数，课程中没有讲，实际上这也不是一个能够很清晰解释的问题，主要有三个原因，分别是：值域在0到1之间，适合表示概率；s型曲线从直观和实践上发现都是符合现实问题中的很多情况的；相对于其他满足前两点的s型曲线，sigmoid函数更加简单，实际性能也往往更好。 2 Logistic Regression Error PLA、线性回归、logistic回归三种模型可以简单对比如下： 计算这个模型的参数值用的是最大似然估计，这个是统计学基础内容，就不用解释了。我们需要最大化的概率是产生数据(xi,yi)(x_i,y_i)(xi​,yi​)联合概率∏iP(xi,yi)\\prod_{i}P(x_i,y_i)∏i​P(xi​,yi​)，由乘法法则这个概率等于∏iP(xi)P(yi,xi)\\prod_{i}P(x_i)P(y_i,x_i)∏i​P(xi​)P(yi​,xi​)，具体的表示和变换如图所示： 因此模型的目标可以写作： max⁡h likelihood(logistic h)∝∏n=1Nh(ynxn)\\max _{h} \\text { likelihood(logistic } h ) \\propto \\prod_{n=1}^{N} h\\left(y_{n} \\mathbf{x}_{n}\\right) hmax​ likelihood(logistic h)∝n=1∏N​h(yn​xn​) 由于h(x)=11+exp⁡(−wTx)h(x)=\\frac{1}{1+\\exp(-w^Tx)}h(x)=1+exp(−wTx)1​，而θ(wTx)=11+e−s\\theta(w^Tx)=\\frac{1}{1+e^{-s}}θ(wTx)=1+e−s1​，因此模型的目标等价地写作： max⁡w likelihood(logistic h)∝∏n=1Nθ(ynwTxn)\\max _{w} \\text { likelihood(logistic } h ) \\propto \\prod_{n=1}^{N} \\theta\\left(y_{n} w^T\\mathbf{x}_{n}\\right) wmax​ likelihood(logistic h)∝n=1∏N​θ(yn​wTxn​) 取对数： max⁡w ln⁡∏n=1Nθ(ynwTxn)\\max _{w} \\ \\ln \\prod_{n=1}^{N} \\theta\\left(y_{n} w^T\\mathbf{x}_{n}\\right) wmax​ lnn=1∏N​θ(yn​wTxn​) 最后取符号变成最小化： 如图中所示，称这样的error为cross-entropy error. 3 Gradient of Logistic Regression Error EinE_{in}Ein​表达式见上图，我们不加证明的告诉读者它具有连续、二阶可导、凸函数的性质。因此这里需要找的是梯度为零向量的点。 求梯度过程如下图： 梯度可以看作是一个由θ\\thetaθ对−ynxn-y_nx_n−yn​xn​加权平均的结果，梯度为0的一种可能是所有权重都为0，这就要求ynwTxn≫0y_nw^Tx_n\\gg 0yn​wTxn​≫0，从PLA的推导我们就知道这个要求达成的必要条件是数据是线性可分的，显然要求过高，很难达到。只能从加权和为0求解这个公式。 遗憾的是，这个公式的求解没有解析解，只能采用迭代方法，接下来一部分要介绍的是梯度下降法。这里先重写一下PLA算法并引进方向和步长的概念。 原先的算法是： 等价地重写为： 这里的η\\etaη是步长（PLA可以看做默认是1），v是方向，二者是迭代优化算法中必不可少的参数。 4 Gradient Descent 这里我想先写出梯度下降法的最终结果： wt+1←wt−η∇Ein (wt)∥∇Ein (wt)∥\\mathbf{w}_{t+1} \\leftarrow \\mathbf{w}_{t}-\\eta \\frac{\\nabla E_{\\text {in }}\\left(\\mathbf{w}_{t}\\right)}{\\left\\|\\nabla E_{\\text {in }}\\left(\\mathbf{w}_{t}\\right)\\right\\|} wt+1​←wt​−η∥∇Ein ​(wt​)∥∇Ein ​(wt​)​ 下面是推导： 假设此时已经有wtw_twt​，下一步要找出wt+1w_{t+1}wt+1​，我们的目标时找到一个合适的方向和步长使得： min⁡∥v∥=1Ein(wt+ηv)\\min _{\\|\\mathbf{v}\\|=1} E_{\\mathrm{in}}\\left(\\mathbf{w}_{t}+\\eta \\mathbf{v}\\right) ∥v∥=1min​Ein​(wt​+ηv) 对这个公式进行泰勒展开，一阶估计是： Ein(wt+ηv)≈Ein(wt)+ηvT∇Ein (wt)E_{\\mathrm{in}}\\left(\\mathbf{w}_{t}+\\eta \\mathbf{v}\\right)\\approx E_{in}(w_t)+\\eta v^T {\\nabla E_{\\text {in }}\\left(\\mathbf{w}_{t}\\right)} Ein​(wt​+ηv)≈Ein​(wt​)+ηvT∇Ein ​(wt​) 考虑贪婪方法，这一步的目标变成： 其中只有方向和步长是需要确定的参数，由于步长η\\etaη一定是正数，因此要求方向和梯度的内积最小，显然方向应当是梯度的反方向，即v=∇Ein (wt)∥∇Ein (wt)∥v=\\frac{\\nabla E_{\\text {in }}\\left(\\mathbf{w}_{t}\\right)}{\\left\\|\\nabla E_{\\text {in }}\\left(\\mathbf{w}_{t}\\right)\\right\\|}v=∥∇Ein ​(wt​)∥∇Ein ​(wt​)​ 这样就得到了最终的结果wt+1←wt−η∇Ein (wt)∥∇Ein (wt)∥\\mathbf{w}_{t+1} \\leftarrow \\mathbf{w}_{t}-\\eta \\frac{\\nabla E_{\\text {in }}\\left(\\mathbf{w}_{t}\\right)}{\\left\\|\\nabla E_{\\text {in }}\\left(\\mathbf{w}_{t}\\right)\\right\\|}wt+1​←wt​−η∥∇Ein ​(wt​)∥∇Ein ​(wt​)​ 关于η\\etaη的选取还需要探讨一下，选的太小会使得下降太慢，选的太大使得不稳定，良好的η\\etaη应当随着算法的迭代而变化。 从第三条曲线可以看出，梯度大的地方η\\etaη大一些、梯度小的地方η\\etaη小一些是较为合适。所以不妨假设η=η′∥∇Ein (wt)∥\\eta=\\eta&#x27;\\|{\\nabla E_{\\text {in }}\\left(\\mathbf{w}_{t}\\right)}\\|η=η′∥∇Ein ​(wt​)∥，其中η′\\eta&#x27;η′是一个固定的常数比例，这样可以得到： wt+1←wt−η′∇Ein (wt)∥∇Ein (wt)∥\\mathbf{w}_{t+1} \\leftarrow \\mathbf{w}_{t}-\\eta&#x27; \\frac{\\nabla E_{\\text {in }}\\left(\\mathbf{w}_{t}\\right)}{\\left\\|\\nabla E_{\\text {in }}\\left(\\mathbf{w}_{t}\\right)\\right\\|} wt+1​←wt​−η′∥∇Ein ​(wt​)∥∇Ein ​(wt​)​ 这个迭代式被称为固定学习率的梯度下降法（fixed learning rate gradient descent），η′\\eta&#x27;η′被称为学习率。","permalink":"http://yangtf983.github.io/2020/03/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B010%EF%BC%9ALogistic%20Regression/","photos":[]},{"tags":[{"name":"机器学习基石听课笔记","slug":"机器学习基石听课笔记","permalink":"http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E5%90%AC%E8%AF%BE%E7%AC%94%E8%AE%B0/"}],"title":"机器学习基石笔记9：Linear Regression","date":"2020/03/17","text":"0 说明 从这一节课开始讲几种算法。 这一节将线性回归，中间的计算步骤涉及到很多矩阵运算，由于这些矩阵运算一般是学过线性代数的人应该已经掌握的，所以文中对视频中对矩阵计算所做的一些解释就不再重复（实际上视频中的解释相当有限，直接看最终结果无碍）。未来几篇文章都会更加关注于对具体算法本身的理解，而对计算的解释将会尽可能忽略。 1 Linear Regression Problem 线性回归的假设是h(x)=wTxh(x)=w^Txh(x)=wTx，其中xxx向量的第一个元素是1，这样就将常数项并入了权重向量。 统计上最常用的误差衡量方式是平方误差： 2 Linear Regression Algorithm 我们想对上述EinE_{in}Ein​做一些变换，将其表示成矩阵形式，目的是用写出解的统一表达式，并且很多编程软件也矩阵运算，可以直接用矩阵形式进行编程。 改写方法如下： EinE_{in}Ein​可以证明是一个连续的可微的凸函数，可微函数的极小值点处梯度必然为0，因此可以转化为求梯度为0的点。 为了求出EinE_{in}Ein​梯度的矩阵表达式，先将EinE_{in}Ein​拆开，再用矩阵求导法则得到梯度表达式，中间步骤如下图： 再通过求逆得到最终的权重向量的表达式， 总结来看这个算法只需要下面三步即可完成： 其中的第二步可能存在由于XTXX^TXXTX不可逆而无法直接计算的问题，但是对于有线性回归内置函数的编程语言或者模块而言，都会有处理这种情况的内置方法，只需要将第一步的矩阵输入，无需考虑XTXX^TXXTX是否可逆。线性回归函数比较常见，因此建议采用这种方法，而不赞同自己造轮子。 3 Generalization Issue 从上一部分看来似乎线性回归的解是一个分析解，而不是通过迭代方法不断修正的，似乎和机器学习形式不太相同。 但也有一些理由支持它是一种机器学习方法。首先它可以找到一个比较小的EinE_{in}Ein​；其次，由于权重的参数数量是有限的，而dVCd_{VC}dVC​可以解释为自由度，因此线性回归算法有有限的dVCd_{VC}dVC​；最后，看似解析解的权重表达式中其中具有迭代的部分，例如求逆矩阵就要用到迭代方法，若对中间步骤都算一个权重，则这个权重应当是随着迭代越来越好的。 总的来说，若线性回归的EoutE_{out}Eout​是好的，learning就发生了。 通过一些步骤（课程中给了大致解释，包括一些简单的矩阵运算和线性代数的几何解释，这里省略），得到： Ein‾=σ2⋅(1−d+1N)Eout‾=σ2⋅(1+d+1N)\\begin{aligned} \\overline{E_{\\mathrm{in}}} &amp;=\\sigma^{2} \\cdot\\left(1-\\frac{d+1}{N}\\right) \\\\ \\overline{E_{\\mathrm{out}}} &amp;=\\sigma^{2} \\cdot\\left(1+\\frac{d+1}{N}\\right) \\end{aligned}Ein​​Eout​​​=σ2⋅(1−Nd+1​)=σ2⋅(1+Nd+1​)​ 从图中看二者的关系： 4 for Binary Classification PLA算法是一个NP难问题，只能得到近似解，但是线性回归是一个有解析解公式的算法，看起来结果的可靠性更能够保证，那么能否用线性回归算法解决二分类问题呢？ 可以通过二者的误差衡量函数来看，PLA是0/1误差，线性回归是平方误差，二者的公式和图如下所示： 从图中可以看出，当平方误差很小时，0/1误差也很小，并且平方误差总是大于等于0/1误差。 第7堂课中已经得到： 因此可以写出PLA的EoutE_{out}Eout​与线性回归的EinE_{in}Ein​的不等式关系： 可以看出，当线性回归的EinE_{in}Ein​足够小时，也就约束了PLA的EinE_{in}Ein​和EoutE_{out}Eout​足够小，这也就解释了可以用线性回归做二分类问题。 并且由于线性回归的err更好解（有解析式），因此可以先用线性回归得到一个相对不错的解，再将其作为初始解采用PLA或pocket算法寻找更好的解。","permalink":"http://yangtf983.github.io/2020/03/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B09%EF%BC%9ALinear%20Regression/","photos":[]},{"tags":[{"name":"机器学习基石听课笔记","slug":"机器学习基石听课笔记","permalink":"http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E5%90%AC%E8%AF%BE%E7%AC%94%E8%AE%B0/"}],"title":"机器学习基石笔记8：Noise and Error","date":"2020/03/16","text":"0 说明 之前的VC bound和VC Dimension是建立在一定的假设上的，这一堂课将学习如何放宽这些假设，使其能够适用于更多不同的问题。 1 Noise and Probabilistic Target （这一部分的内容都没有给出证明，只是给了一些解释，对这些解释可以直接跳过，结论就是加上噪声以后仍然可以learning到东西） 之前的机器学习流程图已经展示过多次了，现在的问题是给数据加上noise会对我们的理论造成什么样的影响？ 首先来看一下noise的三种类型： 图中以信用卡的例子为例，描述了噪声（就是noise）的三种类型： 第一种是关于y的噪声，特点是将标签标反，本来是+1的标签标成了-1； 第二种也是关于y的噪声，其特点是给同一个数据标上了不同的标签，这种情况常常出现在请多个专家标标签的情况，不同专家给同一个数据标的标签可能是不相同的，这样反映在图上就出现了同一个点既是圈又是叉的情况； 第三种是关于x的噪声，即对消费者信息的采集可能是不精确的。 问题是有了这些噪声的情况下VC bound还能不能作用的很好。 回忆一下VC bound证明的核心： VC bound推导的核心在于一个装满了两种颜色弹珠的罐子。其中的弹珠符合一个分布P(x)，当f(x)=h(x)时，对应弹珠的颜色是绿色的，否则是橙色的。 现在考虑一种特别的弹珠，其颜色可能变化，现在从每一颗弹珠来看，其颜色可能是蓝色也可能是绿色，但是从整个罐子来看，大部分情况下还是有一个大概的接近真实情况的颜色比例。现在有一种方式是几率抽出来瞬间变色弹珠的颜色，用这个记录中的比例来估计整个罐子中的颜色比例。既然这个估计是可以的，那么就可以把整个证明过程重写一次，这时原来的x仍然来源于P(x)，但原来的y现在来源于P(y|x)而不是f(x)，P(y|x)就相当于f(x)+noise. 现在的P(y|x)虽然在大部分情况下还是等于正确的标签值（因为噪声一般不会太大），但是也会有一个概率等于错误的标签值，我们用这样的标签值来做机器学习，实际上是我们learning的目标变成了在常见的点（来源于P(x)）上尽量做得好（目标值来源于P(y|x)） 总之，最终结论是VC bound仍然适用。 这样就解释了pocket算法的合理性，因为pocket算法就是将目标值看作来源于P(y|x)而搜索使得Ein(h)E_{in}(h)Ein​(h)最小的h作为g. 噪声情况下的算法流程图如下： 2 Error Measure 整个机器学习可行性证明的核心在于证明g≈fg\\approx fg≈f，之前衡量的方法是采用： Eout (g)=Ex∼P[g(x)≠f(x)]⏟err (g(x),f(x))E_{\\text {out }}(g)=\\underset{\\mathbf{x} \\sim P}{\\mathcal{E}} \\underbrace{[g(\\mathbf{x}) \\neq f(\\mathbf{x})]}_{\\text {err }(g(\\mathbf{x}), f(\\mathbf{x}))} Eout ​(g)=x∼PE​err (g(x),f(x))[g(x)​=f(x)]​​ 其中E\\mathcal{E}E是求期望的符号，中括号表示示性函数，每个点上G(x)与f(x)是否相同记为err(g(x),f(x))err(g(x),f(x))err(g(x),f(x))，样本点上的误差衡量是Ein(g)=1N∑n=1Nerr(g(xn),f(xn))E_{in}(g)=\\frac1N\\sum_{n=1}^Nerr(g(x_n),f(x_n))Ein​(g)=N1​∑n=1N​err(g(xn​),f(xn​))，整个样本空间上的误差衡量是Eout (g)=Ex∼P[g(x)≠f(x)]E_{\\text {out }}(g)=\\underset{\\mathbf{x} \\sim P}{\\mathcal{E}}[g(\\mathbf{x}) \\neq f(\\mathbf{x})]Eout ​(g)=x∼PE​[g(x)​=f(x)]，这种衡量方法属于pointwise的误差衡量方式，这也是这门课将会一直沿用的一种衡量方式。 下面是两种重要的pointwise的衡量方式： 0/1 error： err⁡(y~,y)=[y~≠y]\\operatorname{err}(\\tilde{y}, y)=[\\tilde{y} \\neq y]err(y~​,y)=[y~​​=y]，衡量正/误，常用于分类问题； squared error: err⁡(y~,y)=(y~≠y)2\\operatorname{err}(\\tilde{y}, y)=(\\tilde{y} \\neq y)^2err(y~​,y)=(y~​​=y)2，衡量距离，常用于回归。 上图中展示了两种误差衡量方式对同一个例子衡量误差的结果。若$\\tilde{y}=1$，采用第一种衡量方法的误差是$0.7*[1\\neq 2]+0.1*[1\\neq 3]=0.8$，采用第二种衡量方法的误差是$0.7*(1-2)^2+0.1*(1-3)^2=1.1$ 第一种衡量方法1.9的误差最大，但是第二种衡量方法1.9的误差却最小，可见误差衡量方法影响着衡量结果，进而影响着我们对最理想的结果的判断（因为最理想的结果往往都是衡量的误差最小的）。 图中紫色部分是两种衡量方式下最优（使得误差最小）预测的计算方法，前一个是选择概率最大的y，后一个是选择平均值。 这样算法流程图就变成了上图中的样子。由于不同误差衡量方法选出来的结果不一定相同，所以必须提前给出一个适合当前问题的误差衡量方法，最后还要用这个衡量方法对选出来的g做检验。 VC Dimension通过细微的修改可以适用于绝大多数H和err，后面不再各自推导。 3 Algorithmic Error Measure 误差衡量方法是哪里来的？ 一个指纹识别系统，可能会识别错误，有两种错误的情况，一种是错误的接受（本来不应接受），一种是错误的拒绝（本来不应拒绝）。 先考虑将这个系统用在一个超市识别折扣的系统上，若错误的拒绝给客户折扣，会给客户很不好的印象，若错误的给了用户折扣，则不过是少赚点钱，但是维护了超市的形象，这是更大的利益。这样一来就很清楚，在这个例子中错误的拒绝影响更坏，我们就应当在误差衡量中把错误拒绝的误差权重加大。 而若把这个系统应用于CIA情报系统，则错误的接受的影响一定更加严重（因为会泄露国家机密），因此一定要加大错误接受的误差权重。 因此错误衡量方法是要根据不同的应用和不同的决策者的喜好而变化的，即便都是二元分类也可能差别非常大。 因此在设计算法的时候要想办法用到合适的错误衡量方法。但是要量化这样的误差衡量方法常常是比较困难的，常常要用一些替代方式。 有两种常见的替代方式。一种是从常见的误差衡量方法中找一个看似合理的(plausible)，例如前述PLA采用0/1误差衡量，又如之后会讲的回归采用squared误差衡量，这种方法采用的误差衡量记为err^\\hat{err}err^，它不一定是最好的，但应当是最好的误差衡量方法的一种估计；另一种方法是找一个易于设计算法的(friendly)，例如容易求出解析解的衡量方法或者凸函数（可以采用很多方法得到较优解），这样选出来的误差衡量函数也被记为err^\\hat{err}err^。 这样我们就又能更新一次算法流程图： err是真正意义上最合适的误差衡量函数，但我们在算法中用的是err^\\hat{err}err^ CIA问题的err写法如下： &lt;img src=“https://myfoundationnote-1257754469.cos.ap-nanjing.myqcloud.com/机器学习基石笔记/08/fig07.png” 4 Weighted Classification 不同的错误有不同的重要性（前述超市和CIA的例子），称称这样的分类为weighted classification. 由上图可以知道CIA问题的EoutE_{out}Eout​表示为： Eout (h)=E(x,y)∼P{1 if y=+11000 if y=−1}⋅[y≠h(x)]E_{\\text {out }}(h)=\\underset{(\\mathbf{x}, y) \\sim P}{\\mathcal{E}}\\left\\{\\begin{array}{cl} 1 &amp; \\text { if } y=+1 \\\\ 1000 &amp; \\text { if } y=-1 \\end{array}\\right\\} \\cdot[y \\neq h(\\mathbf{x})]Eout ​(h)=(x,y)∼PE​{11000​ if y=+1 if y=−1​}⋅[y​=h(x)] EinE_{in}Ein​表示为： Ein (h)=1N∑n=1N{1 if y=+11000 if y=−1}⋅[y≠h(x)]E_{\\text {in }}(h)=\\frac1N\\sum_{n=1}^N\\left\\{\\begin{array}{cl} 1 &amp; \\text { if } y=+1 \\\\ 1000 &amp; \\text { if } y=-1 \\end{array}\\right\\} \\cdot[y \\neq h(\\mathbf{x})]Ein ​(h)=N1​n=1∑N​{11000​ if y=+1 if y=−1​}⋅[y​=h(x)] 我们记加权分类问题的EinE_{in}Ein​为EinWE_{in}^WEinW​，下面看一下这种问题的的解法（也就是如何最小化EinWE_{in}^WEinW​）： 若数据是线性可分的，采用PLA算法自不必说，一定可以找到最小的EinW=0E_{in}^W=0EinW​=0，可是若用修正的pocket算法（把Ein0/1E_{in}^{0/1}Ein0/1​换成EinWE_{in}^WEinW​），还能否保证得到好的结果？ 可以通过把数据按照权重复制成相应的份数再采用未修改过的pocket算法来做解释这个问题，总之结果是这个修改的pocket算法是有保证的，我们称之为weighted pocket算法，这个算法相对于原始的pocket算法有两步的更改，分别是： 用最小化EinWE_{in}^WEinW​替代最小化Ein0/1E_{in}^{0/1}Ein0/1​； 选择不同标签点的可能性与其标签的权重成正比（选到的点其实是按照一个点放入EinWE_{in}^WEinW​中计算总误差值的）。 这种 修改算法的方法被称为systematic route或reduction，可以用来将很多算法修改成有权重的情况。","permalink":"http://yangtf983.github.io/2020/03/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B08%EF%BC%9ANoise%20and%20Error/","photos":[]},{"tags":[{"name":"机器学习基石听课笔记","slug":"机器学习基石听课笔记","permalink":"http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E5%90%AC%E8%AF%BE%E7%AC%94%E8%AE%B0/"}],"title":"机器学习基石笔记7：The VC Dimension","date":"2020/03/16","text":"0 说明 这节课对VC Dimension做出定义和说明。 1 Definition of VC Dimension 首先回忆一下上节课所讲的VC bound： 从中可以看出，若同时具备有一个好的mH(N)m_H(N)mH​(N)和一个好的N这两个条件，则VC bound可以变得很小，也就是使得Eout=EinE_{out}=E_{in}Eout​=Ein​的概率很大。这里说的mH(N)m_H(N)mH​(N)好是指存在break point，N好是指N足够大。 在具备上面两个条件的同时，若再有一个好的算法A（这个好是指能够选出EinE_{in}Ein​很小的g），则可以说这个机器学习算法很可能学到了东西（不是绝对的，是几率问题，需要一点运气）。 下面定义VC Dimension：假设H下最大的non-break point，记作dvc(H)d_{vc}(H)dvc​(H) 可见，dvc(H)d_{vc}(H)dvc​(H)就是假设H下能够shatter的最多的点，dvc(H)=′minimum k′−1d_{vc}(H)=&#x27;minimum\\ k&#x27;-1dvc​(H)=′minimum k′−1；当N≤dvc(H)N\\leq d_{vc}(H)N≤dvc​(H)时，有可能H可以shatter这N个点（并不是一定的，例如PLA可以 shatter三个不共线的点，但没办法shatter三个共线的点）；当k&gt;dvc(H)k&gt;d_{vc}(H)k&gt;dvc​(H)时，k就是H的一个break point，只是不一定是最小的那个。 因此，从本文第一张图中的公式可以轻易得到：当N≥2N\\geq2N≥2且dvc≥2d_{vc}\\geq2dvc​≥2时，可以得到mH(N)≤NdVCm_H(N)\\leq N^{d_{VC}}mH​(N)≤NdVC​ 下面给出几种情况下的VC Dimension: 当VC Dimension有限时，无论算法、数据分布和目标函数f是什么，我们都可以使得Eout(g)≈Ein(g)E_{out}(g)\\approx E_{in}(g)Eout​(g)≈Ein​(g) 2 VC Dimension of Perceptrons 回忆之前的二维PLA，用VC Dimension的理论解释其可行性。 当数据是线性可分的时（服从某个未知的分布并且有一个未知的模型f），存在分类器能够使得Ein(g)=1E_{in}(g)=1Ein​(g)=1，已知此时的dvc=3d_{vc}=3dvc​=3，因此可以写出VC bound约束，当N足够大时，可以保证Eout(g)≈Ein(g)E_{out}(g)\\approx E_{in}(g)Eout​(g)≈Ein​(g)很可能发生，这样就使得Eout(g)≈0E_{out}(g)\\approx 0Eout​(g)≈0也很可能发生。 下面看一下在更高维度下的PLA的情况： 首先说明结论：d维情况下PLA的VC Dimendion为dVC=d+1d_{VC}=d+1dVC​=d+1 我们在先前的学习已经知道了一维情况下dVC=2d_{VC}=2dVC​=2，二维情况下dVC=3d_{VC}=3dVC​=3，都是符合这个结论的，但这并不足以证明这个结论，下面将证明这个结论，证明过程分别两步，依次是dVC≥d+1d_{VC}\\geq d+1dVC​≥d+1和dVC≤d+1d_{VC}\\leq d+1dVC​≤d+1 第一步用到的方法是用一定的方法在d维空间中找到d+1个点，再证明这样找到的d+1个点总是可以被shatter的。 上图中橙色部分表示的就是之前说的d+1个点，只是在最左侧加上了一列1，这等于是将PLA算法的threshold并入权重后的数据点写法，容易知道这个方阵是可逆的，因此无论标签值向量yyy是什么（当然指的是元素都是由+1和-1组成的），都可以令w=X−1yw=X^{-1}yw=X−1y，这样就有Xw=yXw=yXw=y，进而sign(Xw)=ysign(Xw)=ysign(Xw)=y。这样，将yyy取遍所有的可能情况都能得到合适的www，则证明了这样取到的d+1个点是可以被shatter的，从而也就证明了第一步的结论。 下面证明第二步，用的方法是证明任何d+2个点都是不能被shatter的。 首先我们表示出d维PLA情况下由任意d+2个点组成的一个矩阵，表示如下： 由于矩阵XXX是d+2行d+1列的，因此组成矩阵的d+2个行向量必然是线性相关的。 这里不妨先假设前d+1个行向量是线性无关的（课程中没有假设，不严谨），则可以通过前 d+1个行向量的线性组合表示第d+2个行向量（正如图中蓝色部分在乘权重wTw^TwT之前一样），此时系数aia_iai​是唯一确定的，这时定义一个yyy，其特征是前d+1个元素就是a1,...,ad+1a_1,...,a_{d+1}a1​,...,ad+1​的符号，根据第一步的结果前d+1个点是可以被shatter的，这样可以找到一个wTw^TwT使得前d+1个wTxiw^Tx_iwTxi​的符号都和aia_iai​相同，此时得到的等号右边的式子的结果一定是大于0的，也就是说若yyy的d+1个值为-1，则一定没办法在确保前d+1个点的分类都正确的情况下让wTxd+2&lt;0w^Tx_{d+2}&lt;0wTxd+2​&lt;0，因此证明了这d+2个点是无法被shatter **补充证明：**当前d+1个点线性相关时，可以找到其中的一个最大线性无关组，用这个无关组唯一表示出随便一个剩余不在无关组中的向量，然后采用刚刚的方法，可以证明这个无关组加上一个不在无关组中的向量一定是不能够被shatter的，由于更少的点都不能被shatter，所以原矩阵中的d+2个点一定不能被shatter，综合之前的部分，就证明了任意d+2个点都不能够被shatter 这样，我们就证明了d维情况下PLA的VC Dimendion为dVC=d+1d_{VC}=d+1dVC​=d+1，也就是说PLA在高维情况下仍然是可以学习到东西的。 3 Physical Intuition of VC Dimension 从直觉上可以对VC Dimension做出一些解释： d维情况下PLA所求的权重www有d+1个任意参数，称之为自由度，这个例子中自由度就等于dVC(H)d_{VC}(H)dVC​(H)。 dVC(H)d_{VC}(H)dVC​(H)指的是假设H下最多能够shatter的点，因此可以将dVC(H)d_{VC}(H)dVC​(H)理解为假设H的强度（powerfulness）。 通过更多的例子（这里不再列举）可以得到一条经验法则：dVC≈#free parametersd_{VC}\\approx \\#free\\ parametersdVC​≈#free parameters，也就是说VC Dimension大概（但不一定）等于自由参数的数量（自由度）。 这样，第五讲在描述M和两个重要问题的关系的时候的M就可以换成dVCd_{VC}dVC​，关系如下图： 4 Interpreting VC Dimension 现在希望更深入地了解VC Dimension是什么。 如图中所示，将VC bound记作δ\\deltaδ，则可以通过如下变换求出ϵ\\epsilonϵ的表达式： 因此，好事情（∣Ein−Eout∣≤ϵ|E_{in}-E_{out}|\\leq \\epsilon∣Ein​−Eout​∣≤ϵ）以概率1−δ1-\\delta1−δ发生，进而能够得到如下图所示的类似置信区间的EoutE_{out}Eout​的表达式： 这个区间的下界不重要（用灰色表示），上界重要，可以看出在样本量N和概率δ\\deltaδ固定的情况下上界由VC Dimension决定。 引入一个变量：模型复杂度。当VC Dimension变大时，H能够shatter更多的点，模型自然更复杂。 随着样本VC Dimension变大，可以选择的h变多，因此EinE_{in}Ein​变小。 开始的时候EinE_{in}Ein​变小的速度快，上图中的上界受EinE_{in}Ein​变小的影响大，也变小，之后受EinE_{in}Ein​变小的影响小，随着后一项变大的影响大，逐渐变大。 这样就可以得到下图： 其中Ω\\OmegaΩ是模型复杂度，几个变量的关系如图中蓝色区块所示。 从图中可以看出，虽然当模型复杂度增加时，EinE_{in}Ein​一直在减小，但是∣Ein−Eout∣≤ϵ|E_{in}-E_{out}|\\leq \\epsilon∣Ein​−Eout​∣≤ϵ却是在增大，实际问题中我们必须兼顾这两项都比较小，因此合理的模型复杂度不能太大也不能太小。 VC bound给出的上界是比较比较宽的，若我们要求达到上面两项都很小，从VC bound算出来的结果往往要是dVCd_{VC}dVC​的上万倍，但是实际上，经验法则告诉我们只要N≈10dVCN\\approx 10d_{VC}N≈10dVC​就可以得到较好的结果。 VC bound之所以宽松，可以解释为我们获得这个bound过程中的各种条件非常的宽松，进而导致了这样宽松的bound，分别包括下面几项： （1）hoeffding不等式对数据的分布P和目标函数f都没有任何的约束； （2）实际问题中能够选取的只是手头上的有限资料，但是在计算成长函数的时候却是考虑的空间上所有的点； （3）最终计算并不是直接用的成长函数mH(N)m_H(N)mH​(N)，而是其上界NdVCN^{d_{VC}}NdVC​； （4）用的是union bound，估计的是最坏的情况。 以后用的最多的并不是VC bound的公式，而是它带给我们的哲学上的信息，例如刚刚的要选择合适的模型复杂度（不能太大也不能太小），这是后面讲算法常会用到的东西，","permalink":"http://yangtf983.github.io/2020/03/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B07%EF%BC%9AThe%20VC%20Dimension/","photos":[]},{"tags":[{"name":"机器学习基石听课笔记","slug":"机器学习基石听课笔记","permalink":"http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E5%90%AC%E8%AF%BE%E7%AC%94%E8%AE%B0/"}],"title":"机器学习基石笔记6：Theory of Generalization","date":"2020/03/15","text":"0 说明 之前说明了假设的数量M可能达到无穷，这个时候会给机器学习的解释造成问题。之后我们定义了一个成长函数mH(N)m_H(N)mH​(N)，希望能够用这个成长函数替代M，并且在上一节的最后说明了如果成长函数mH(N)m_H(N)mH​(N)是多项式量级的，那么就可以达到之前用来解释有限M的条件：随着N的增大，好事情发生的概率可以无限趋近于1. 接下来要看一下成长函数的增长能不能不超过多项式量级。 1 Restriction of Break Point 成长函数mH(N)m_H(N)mH​(N)的定义是可以产生的dichotomy的最大数量，break point的定义是成长函数小于2N2^N2N的最小数据量。 下面从一个例子出发，这个例子中有N=3个数据点，break point k=2，也就是说mH(1)=21=2m_H(1)=2^1=2mH​(1)=21=2，而mH(2)&lt;22=4m_H(2)&lt;2^2=4mH​(2)&lt;22=4，接下来我们看一看mH(3)m_H(3)mH​(3)是多少： 由于要shatter两个点需要4个dichotomy，所以任意给出三个dichotomy都不能shatter三个点中的任意两个点（如上图），但是再增加一个dichotomy就可能出现shatter两个点的情况： 通过增加别的dichotomy可以达到4种dichotomy但是不shatter任意两个点的情况： 通过不断尝试，此时最多只能有4种dichotomy，再增加任何一种都会导致有两个点会shatter 当N=2时，最大的mH(2)=3&lt;22=4m_H(2)=3&lt;2^2=4mH​(2)=3&lt;22=4；当N=3时，最大的mH(3)=4≪23=8m_H(3)=4\\ll 2^3=8mH​(3)=4≪23=8，可见对于超过break point的点的数量，成长函数增加非常慢，似乎有可能降到多项式级别。 2 Bounding Function-Basic Cases 定义bounding函数B(N,k)，其意义是当break point=k时成长函数mH(N)m_H(N)mH​(N)的最大值。 注意这个函数是与假设HHH无关的，如果我们能够找到这样的函数，那么就不必管具体的假设，只要有berak point，就可以用同样的方法分析。 下面要说明的问题是bounding函数是不是多项式级别成长的。 根据之前的分析，已经可以得到B(2,2)=3, B(3,2)=4. 此外，若break point k=1，不难发现B(N,1)=1,∀ N=1,2,...B(N,1)=1,\\forall\\ N=1,2,...B(N,1)=1,∀ N=1,2,...，因此可以得到下表 由于当N&lt;k的时候N个点是一定可以shatter的，所以图中的上半部分可以直接用2N2^N2N填充： 当N=k时，由于刚刚不能shatter,因此bounding函数不可能超过2N−12^N-12N−1，因此不严格地可以就把对角线写成2N−12^N-12N−1（事实上这个结果应该是严格的，但是课程中并没有加以证明）： 3 Bounding Function-Inductive 现在填写表格下半部分： 首先我们从一个具体的地方开始，比如B(4,3)，我们通过计算机搜索的方法可以找到此时最多有11种情况，在下图左侧，按照一定的方法排序得到右侧的图： 右图中的排序方法是将x4x_4x4​单独拿出来看，发现1和5、2和8、3和10、4和11这四对在其余三个点上的情况分别是一样的，都是只改变x4x_4x4​ 若将拿出一个数据出来其余能够成对的数据量记作2α2\\alpha2α，不能成对的数据量记作β\\betaβ，则由于B(4,3)的情况下任意4个点不能被shatter，因此α+β≤B(3,3)\\alpha+\\beta\\leq B(3,3)α+β≤B(3,3) 又由于α\\alphaα是在x4x_4x4​成对的情况下(x1,x2,x3)(x_1,x_2,x_3)(x1​,x2​,x3​)的dichotomy数量，所以橙色部分不能够shatter(x1,x2,x3)(x_1,x_2,x_3)(x1​,x2​,x3​)中的任意两个，否则我们加上x4x_4x4​后一定能够shatter(x1,x2,x3,x4)(x_1,x_2,x_3,x_4)(x1​,x2​,x3​,x4​)中的某三个，所以α≤B(3,2)\\alpha\\leq B(3,2)α≤B(3,2)，这样我们就得到了： B(4,3)=α+(α+β)≤B(3,3)+B(3,3)B(4,3)= \\alpha + (\\alpha+\\beta)\\leq B(3,3)+B(3,3) B(4,3)=α+(α+β)≤B(3,3)+B(3,3) 显然这样的推导具有普适性，也就是说当N&gt;k时，有不等式B(N,k)≤B(N−1,k)+B(N−1,k−1)B(N,k)\\leq B(N-1,k)+B(N-1,k-1)B(N,k)≤B(N−1,k)+B(N−1,k−1) 通过数学归纳法容易得到： B(N,k)≤∑i=0k−1(Ni)B(N,k)\\leq \\sum_{i=0}^{k-1}\\left(\\begin{array}{c}N\\\\ i\\end{array}\\right) B(N,k)≤i=0∑k−1​(Ni​) 课程中没有给出这一公式的证明，这里给出补充证明如下： 补充证明： 数学归纳法 （1）当N&gt;1时，B(N,1)=1=(N0)B(N,1)=1=\\left(\\begin{array}{c}N\\\\ 0\\end{array}\\right)B(N,1)=1=(N0​)； 当N=k时，B(N,k)=2N−1=∑i=0N−1(Ni)=∑i=0k−1(Ni)B(N,k)=2^N-1=\\sum_{i=0}^{N-1}\\left(\\begin{array}{c}N\\\\ i\\end{array}\\right)=\\sum_{i=0}^{k-1}\\left(\\begin{array}{c}N\\\\ i\\end{array}\\right)B(N,k)=2N−1=∑i=0N−1​(Ni​)=∑i=0k−1​(Ni​)； （2）当N&gt;k&gt;1时： B(N,k)⩽∑i=0k−1(N−1i)+∑i=0k−2(N−1i)=(N−10)+[(N−10)+(N−11)]+⋯+[(N−1k−2)+(N−1k−1)]=(N0)+⋯+(Nk−1)=∑i=0k−1(Ni)\\begin{aligned} B(N, k) &amp; \\leqslant \\sum_{i=0}^{k-1}\\left(\\begin{array}{c} N-1 \\\\ i \\end{array}\\right)+\\sum_{i=0}^{k-2}\\left(\\begin{array}{c} N-1 \\\\ i \\end{array}\\right) \\\\ &amp;=\\left(\\begin{array}{c} N-1 \\\\ 0 \\end{array}\\right)+\\left[\\left(\\begin{array}{c} N-1 \\\\ 0 \\end{array}\\right)+\\left(\\begin{array}{c} N-1 \\\\ 1 \\end{array}\\right)\\right]+\\cdots+\\left[\\left(\\begin{array}{l} N-1 \\\\ k-2 \\end{array}\\right)+\\left(\\begin{array}{l} N-1 \\\\ k-1 \\end{array}\\right)\\right] \\\\ &amp;=\\left(\\begin{array}{c} N \\\\ 0 \\end{array}\\right)+\\cdots+\\left(\\begin{array}{c} N\\\\ k-1\\end{array}\\right) \\\\ &amp;=\\sum_{i=0}^{k-1}\\left(\\begin{array}{c} N \\\\ i \\end{array}\\right) \\end{aligned}B(N,k)​⩽i=0∑k−1​(N−1i​)+i=0∑k−2​(N−1i​)=(N−10​)+[(N−10​)+(N−11​)]+⋯+[(N−1k−2​)+(N−1k−1​)]=(N0​)+⋯+(Nk−1​)=i=0∑k−1​(Ni​)​ 证明结束 事实上再经过严格的数学证明，上式中的不等号是可以换成等号的。但这里只需要这样的不等式的结论就可以说明问题，∑i=0k−1(Ni)\\sum_{i=0}^{k-1}\\left(\\begin{array}{c}N \\\\ i\\end{array}\\right)∑i=0k−1​(Ni​)的最高次项是Nk−1N^{k-1}Nk−1，因此我们就证明了当break point存在时mH(N)m_H(N)mH​(N)最多是多项式增长的。 4 A Pictorial（形象化的） Proof 经过了这么多推导，那么是不是直接就可以将 成长函数换到hoeffding不等式中的M呢？实际上这么做是不严谨的（个人猜测的理由是这里不仅仅是用成长函数换掉M那么简单，因为我们要解决的是M是无限值的情况，这个时候直接换成一个有限只肯定是不严谨的，所以应当通过更加严谨地推导得到严谨的结果），真正严谨的结果应该是下图中的第二个公式： 注意第二个公式不是始终成立的，而是当N足够大的时候才成立。 这个公式的证明技巧性太强，课程中没有具体讲，只是讲了一下证明的概要，下面我们来说一说（一下说明略去的证明过程都是课程中本就略去的）。 第一步是消掉Eout(h)E_{out}(h)Eout​(h)，用的方式是再找一个样本集D′D&#x27;D′产生一个Ein′E_{in}&#x27;Ein′​，略去中间证明过程可以得到下式： 12P[∃h∈H s.t. ∣Ein (h)−Eout (h)∣&gt;ϵ]≤P[∃h∈H s.t. ∣Ein (h)−Ein ′(h)∣&gt;ϵ2]\\begin{aligned} &amp; \\frac{1}{2} \\mathbb{P}\\left[\\exists h \\in \\mathcal{H} \\text { s.t. }\\left|E_{\\text {in }}(h)-E_{\\text {out }}(h)\\right|&gt;\\epsilon\\right] \\\\ \\leq &amp; \\mathbb{P}\\left[\\exists h \\in \\mathcal{H} \\text { s.t. }\\left|E_{\\text {in }}(h)-E_{\\text {in }}^{\\prime}(h)\\right|&gt;\\frac{\\epsilon}{2}\\right] \\end{aligned}≤​21​P[∃h∈H s.t. ∣Ein ​(h)−Eout ​(h)∣&gt;ϵ]P[∃h∈H s.t. ∣Ein ​(h)−Ein ′​(h)∣&gt;2ϵ​]​ 之所以这么换，是因为EoutE_{out}Eout​可能是无限多种，但Ein,Ein′E_{in},E_{in}&#x27;Ein​,Ein′​有有限多种，其种类数由mH(N)m_H(N)mH​(N)限制。 第二步是用mH(2N)m_H(2N)mH​(2N)换掉M，个人理解这里可以把EoutE_{out}Eout​看作是恒为0的常数，把Ein−Ein′E_{in}-E_{in}&#x27;Ein​−Ein′​看作是新的EinE_{in}Ein​，显然Ein−Ein′E_{in}-E_{in}&#x27;Ein​−Ein′​只有有限多种可能性，由∣H(x1,...,xN,x1′,...xN′)∣|H(x_1,...,x_N,x_1&#x27;,...x_N&#x27;)|∣H(x1​,...,xN​,x1′​,...xN′​)∣限制（也就是mH(2N)m_H(2N)mH​(2N)），由于EoutE_{out}Eout​看作是恒为0的常数，这里就可以把这个新的公式完全看作是之前介绍的有限情况下的问题，采用 与之前的推导类似的方法就可以得到： BAD≤2P[∃h∈H s.t. ∣Ein(h)−Ein′(h)∣&gt;ϵ2]≤2mH(2N)P[ fixed h s.t. ∣Ein(h)−Ein′(h)∣&gt;ϵ2]\\begin{aligned} \\mathrm{BAD} &amp; \\leq 2 \\mathrm{P}\\left[\\exists h \\in \\mathcal{H} \\text { s.t. }\\left|E_{\\mathrm{in}}(h)-E_{\\mathrm{in}}^{\\prime}(h)\\right|&gt;\\frac{\\epsilon}{2}\\right] \\\\ &amp; \\leq 2 m_{H}(2 N) \\mathbb{P}\\left[\\text { fixed } h \\text { s.t. }\\left|E_{\\mathrm{in}}(h)-E_{\\mathrm{in}}^{\\prime}(h)\\right|&gt;\\frac{\\epsilon}{2}\\right] \\end{aligned}BAD​≤2P[∃h∈H s.t. ∣Ein​(h)−Ein′​(h)∣&gt;2ϵ​]≤2mH​(2N)P[ fixed h s.t. ∣Ein​(h)−Ein′​(h)∣&gt;2ϵ​]​ 第三步是得到最终的公式： BAD≤2mH(2N)P[ fixed h s.t.∣Ein(h)−Ein′(h)∣&gt;ϵ2]≤2mH(2N)⋅2exp⁡(−2(ϵ4)2N)\\begin{aligned} \\mathrm{BAD} &amp; \\leq 2 m_{\\mathcal{H}}(2 N) \\mathrm{P}\\left[\\text { fixed } h\\ \\mathrm{s.t.}\\left|E_{\\mathrm{in}}(h)-E_{\\mathrm{in}}^{\\prime}(h)\\right|&gt;\\frac{\\epsilon}{2}\\right] \\\\ &amp; \\leq 2 m_{H}(2 N) \\cdot 2 \\exp \\left(-2\\left(\\frac{\\epsilon}{4}\\right)^{2} N\\right) \\end{aligned}BAD​≤2mH​(2N)P[ fixed h s.t.∣Ein​(h)−Ein′​(h)∣&gt;2ϵ​]≤2mH​(2N)⋅2exp(−2(4ϵ​)2N)​ 关于最后一步并没有给出证明，只是举了一个hoeffding without replacement的例子。这个例子是说假设有一个只有2N个弹珠的小瓶子，每次取出来N个计算EinE_{in}Ein​，另外N个计算Ein′E_{in}&#x27;Ein′​，显然此时Eout=Ein+Ein′2E_{out}=\\frac{E_{in}+E_{in}&#x27;}{2}Eout​=2Ein​+Ein′​​，而∣Ein−Ein′∣&gt;ϵ2|E_{in}-E_{in}&#x27;|&gt;\\frac{\\epsilon}{2}∣Ein​−Ein′​∣&gt;2ϵ​等价于说EinE_{in}Ein​离二者的平均数距离超过ϵ4\\frac{\\epsilon}{4}4ϵ​（否则无法达到∣Ein−Ein′∣&gt;ϵ2|E_{in}-E_{in}&#x27;|&gt;\\frac{\\epsilon}{2}∣Ein​−Ein′​∣&gt;2ϵ​），也就是说∣Ein−Ein′∣&gt;ϵ2|E_{in}-E_{in}&#x27;|&gt;\\frac{\\epsilon}{2}∣Ein​−Ein′​∣&gt;2ϵ​等价于∣Ein−Ein+Ein′2∣&gt;ϵ4|E_{in}-\\frac{E_{in}+E_{in}&#x27;}{2}|&gt;\\frac{\\epsilon}{4}∣Ein​−2Ein​+Ein′​​∣&gt;4ϵ​，此时用有限假设情况下的hoeffding不等式可以轻易地得到第三步的结果，只是将这个结果推广到普适的情况这里不予证明。 最终，我们就得到了Vapnik-Chervonenkis(VC) bound: P[∃h∈H s.t. ∣Ein(h)−Eout′(h)∣&gt;ϵ]≤4mH(2N)exp⁡(−18ϵ2N)\\mathrm{P}\\left[\\exists h \\in \\mathcal{H} \\text { s.t. }\\left|E_{\\mathrm{in}}(h)-E_{\\mathrm{out}}&#x27;(h)\\right|&gt;\\epsilon\\right]\\leq 4m_\\mathcal{H}(2N)\\exp(-\\frac18\\epsilon^2N) P[∃h∈H s.t. ∣Ein​(h)−Eout′​(h)∣&gt;ϵ]≤4mH​(2N)exp(−81​ϵ2N) 最后用一个例子来看一下这个bound的约束效果： 从中可以看出，当我们拥有10000个数据并把误差放大到0.1时，给出的bound仍然算不上小（接近0.3）。 实际上，VC bound是一个约束不严格的上界，那么这个上界究竟有什么重要作用值得这样推导？留待下一节解释。","permalink":"http://yangtf983.github.io/2020/03/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B06%EF%BC%9ATheory%20of%20Generalization/","photos":[]},{"tags":[{"name":"机器学习基石听课笔记","slug":"机器学习基石听课笔记","permalink":"http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E5%90%AC%E8%AF%BE%E7%AC%94%E8%AE%B0/"}],"title":"机器学习基石笔记5：Training versus Testing","date":"2020/03/15","text":"0 说明 从这一节开始说明无限个假设情况下机器学习的可行性。 1 Recap and Preview 首先回顾一下前几趟课学习的知识。 第一堂课说的是机器学习的假设是存在一个未知的未知的模式f，机器学习算法的目的是找到一个g使其与f十分接近。 第四堂课把这个问题写成了Eout(g)≈0E_{out}(g)\\approx 0Eout​(g)≈0 第二堂课说可以想一些办法使得Ein(g)≈0E_{in}(g)\\approx 0Ein​(g)≈0，如PLA算法。 第三堂课说我们的问题是在很特殊的情况下做的，如在批次分类下做二元分类问题。 第四堂课最后想办法得到了Eout(g)≈Ein(g)E_{out}(g)\\approx E_{in}(g)Eout​(g)≈Ein​(g)的问题。在hoeffding不等式上界不大的情况下，使得Ein(g)E_{in}(g)Ein​(g)越来越小就很有可能使得Eout(g)E_{out}(g)Eout​(g)越来越小。 到这里我们把learning拆成了两个问题： 到底Eout(g)E_{out}(g)Eout​(g)与Ein(g)E_{in}(g)Ein​(g)能不能很接近？ 如何使得Ein(g)E_{in}(g)Ein​(g)尽可能小？ 当假设的数量很小的时候，也就是M很小的时候，很容易达到第一个问题的约束，但是很难达到第二个约束，因为选择有限，往往不一定能够找到能够使得Ein(g)E_{in}(g)Ein​(g)很小的g 当假设的数量M很大的时候，第二个问题很容易满足，但是第一个问题不容易满足，坏事情（Ein(g)E_{in}(g)Ein​(g)与Eout(g)E_{out}(g)Eout​(g)相离较远）发生的概率增加了。 由此可见M很重要，不能太大也不能太小。 那是不是说明无限大的M就是没办法机器学习的？PLA算法是难以保证合理性的？ 解决上面问题的思路是想办法将无限大的M换成一个有限大的mHm_{H}mH​，如果能够达到这一目标，就可以用上一堂课的结果说明无限大的M的机器学习算法的合理性。 1 Effective Number of Lines 上一堂课说明M其实是坏事情发生的次数，当时没有考虑到其实有时不同的坏事情是重叠的，也就是说有些坏事情是可能同时发生在不同假设上的。 现在我们的目的是找到这些坏事情重叠的部分。 思路是考虑能不能把无线的假设分成有限的类。 首先考虑一个平面上只知道一个数据点的情况： 从图中可以看出，此时平面上所有的直线可以仅仅分为两类，分别是使得x1x_1x1​标签为圈和叉的直线。 再看两个点的情况： 此时有4种线。 不难知道三个点时最多有8种线： 但是当三个点在同一条线上时就只有六种线。 当四个点任意三个不共线时有14种情况，其中的七种如下，另外其中是将这七种的符号反过来： 在二维平面上，仅从输入点来讲，线的种类是可以得到的，我们称线的最大种类为effective number of lines 记N个点的effective number of lines为effective(N)，则有： P[∣Ein (g)−Eout (g)∣&gt;ϵ]≤2⋅ effective (N)⋅exp⁡(−2ϵ2N)\\begin{aligned} &amp; \\mathbb{P}\\left[\\left|E_{\\text {in }}(g)-E_{\\text {out }}(g)\\right|&gt;\\epsilon\\right] \\\\ \\leq &amp; 2 \\cdot \\text { effective }(N) \\cdot \\exp \\left(-2 \\epsilon^{2} N\\right) \\end{aligned}≤​P[∣Ein ​(g)−Eout ​(g)∣&gt;ϵ]2⋅ effective (N)⋅exp(−2ϵ2N)​ 由于点的所有情况有共有2N2^N2N种，因此首先能知道effective number of lines≤2N\\leq2^N≤2N，但是这样还不够，因为若采用2N2^N2N作为上界，则上式的右侧只能估计到小于等于2exp⁡{N(ln⁡2−2ϵ2)}2\\exp\\{N(\\ln2-2\\epsilon^2)\\}2exp{N(ln2−2ϵ2)}，显然，当ϵ\\epsilonϵ足够小时，(ln⁡2−2ϵ2)(\\ln2-2\\epsilon^2)(ln2−2ϵ2)大于0，此时随着数据量N的变大不等式右侧的估计值居然是上升的，也就是说N再大也没办法说明坏事情发生的概率降低。而我们想要证明的结论是右侧随着N的变大可以无限趋近于0（也就是坏事情发生的概率可以趋近于0），用到的effective(N)的上界估计的数量级必须缩小。（这一段是课程中没有的补充解释） 2 Effective Number of Hypotheses 假设N个数据，一个二分类的假设，每一种可能情况记为一个dichotomy，整个空间上的直线分类器是无限多个，但是dichotomies H(x1,...,xN)dichotomies \\ H(x_1,...,x_N)dichotomies H(x1​,...,xN​)不会超过2N2^N2N 显然∣H(x1,...,xN)∣|H(x_1,...,x_N)|∣H(x1​,...,xN​)∣依赖于N个数据点的选取（如共线不共线会影响dichotomy的数量），而我们只需要找到最大的那个dichotomy的数量，记mH(N)=max⁡x1,...,xN∈X∣H(x1,...,xN)∣m_{H}(N)=\\max_{x_1,...,x_N\\in X}|H(x_1,...,x_N)|mH​(N)=maxx1​,...,xN​∈X​∣H(x1​,...,xN​)∣ 下面看一下能不能写出来mH(N)m_{H}(N)mH​(N)的函数（称为成长函数）。 在单方向的数轴上情况如下： 此时mH(N)=N+1≪2N, when N large!m_H(N)=N+1\\ll 2^N,\\ when\\ N\\ large!mH​(N)=N+1≪2N, when N large! 若确定一个区间，中间是圈，外面是叉，这种设定下的diochotomy情况如下： &lt;img src=“https://myfoundationnote-1257754469.cos.ap-nanjing.myqcloud.com/机器学习基石笔记/05/fig7.png&quot;width=&quot;70%” height=“70%”&gt; 再举一个例子，用凸区域来shatter二维平面上的点： 3 Break Point 从前面加粗部分的分析我们知道：当mH(N)m_H(N)mH​(N)的上界取2N2^N2N时无法证明机器学习算法是可行的。 那么若能够证明其上界可以由多项式给出呢？ 容易知道Nkexp⁡(−2ϵ2N)=exp⁡(kln⁡N−2ϵ2N)N^k\\exp(-2\\epsilon^2N)=\\exp(k\\ln N-2\\epsilon^2N)Nkexp(−2ϵ2N)=exp(klnN−2ϵ2N)，无论k多么大，当n足够大时，总能保证指数项是负的且随着N增大而减小。因此多项式是可以说明机器学习的可行性的。 那么PLA算法的成长函数的上界可以由多项式给出吗？ 这个问题的答案要等到下节课才能真正给出，这里先找一下成长函数里面第一个看起来“有一些希望”的点，也就是第一次无法做出所有的dichotomy的点的数量，记为break point，根据先前的分析，二维平面上的PLA的break point是4（最多14种情况，小于24=162^4=1624=16）。 另外，若N个点可以做出所有dichotomy的情形，则遮住一个点时剩下的点也可以做出所有dichotomy的情形。也就是说，当N&gt;break point时，一定没办法做出来2N2^N2N种情形，否则遮住N-break point个点应该能做出做出所有dichotomy的情形，也就是说break point点找错了。 前述几种情况的break point与成长函数如下（第三种情况没有break point）： 这里可以预先透露一下，break point与成长函数是有关系的，事实上mH(N)=O(Nk−1)m_H(N)=O(N^{k-1})mH​(N)=O(Nk−1)，具体情况下节再谈。","permalink":"http://yangtf983.github.io/2020/03/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B05%EF%BC%9ATraining%20versus%20Testing/","photos":[]},{"tags":[{"name":"机器学习基石听课笔记","slug":"机器学习基石听课笔记","permalink":"http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E5%90%AC%E8%AF%BE%E7%AC%94%E8%AE%B0/"}],"title":"机器学习基石笔记4：Feasibility of Learning","date":"2020/03/13","text":"0 说明 本节的问题是探究learning的可行性。 1 Learning is Impossible? 首先看一个不可行的例子： 图中，第一行的三个不同输入的y值是-1，第二行的三个不同输入的y值是+1，目标是正确回答第三行的输入的y值。 若根据轴对称规则判断，第一行都不是轴对称图形，第二行都是轴对称图形，则可以判断g(x)=+1；若根据左上角方块是否为黑色判断，第一行左上角方块都为黑色，第二行左上角方块都为白色，则可以判断g(x)=-1. 若机器学习产生的结果是按照轴对称规则，而实际上的规则是左上角是否为黑色，则学习错误；若机器学习到的结果是左上角是否为黑色，而实际上的规则是是否轴对称，则学习也错误。 从这个例子中来看，通过数据学习一个正确的规则似乎是不可能的。 再看一个用数字表达的例子： 图中DDD是已知的数据集，y是标签值，真正的模式f未知，现在要训练一个g估计f. 输出只有两类（圈或叉），输入是一个三维的0-1数组，有八种。每一个输入x值有两种可能的标签，所以在没有信息的情况下g可以有28=2562^8=25628=256种可能性。此处已知了五种x的输出值，还有三种无法确定，因此此处备选的g可能有23=82^3=823=8种，看起来已知的数据集确实减少了g的可能性并且提高了g与f的相同概率，但是事实上，已知的五种情况并不是机器学习得到的结果而是本来就知道的，剩余的三种情况并没有从中学习到任何规则，而这三种情况才是机器学习的目的，这里的机器学习算法是“无用”的。 之所以说这里剩余的三种情况没有学习到任何规则，是因为我们没有额外的信息或假设判断其倾向于哪种规则。事实上，这正是no free lunch theorem所说的事情。 no free lunch theorem: 不存在一个与具体算法无关的，普遍适用的“最优分类器”； 学习算法必须作出一个与问题领域有关的“假设”，分类器必须与问题域相适应。 从这两个例子来看，至少存在一些情况learning是做不到的。 2 Probability to the Rescue 现在思考是否有什么工具可以帮助我们对一些未知的东西进行推论/估计。 再看一个例子： 图中罐子中有非常多的绿弹珠和橙弹珠，现在要求罐子中橙色弹珠的比例μ\\muμ是多少，但是无法全部拿出来数一遍，一种常见的方法是从罐子中随机取一把弹珠，用其中橙色弹珠的比例ν\\nuν来估计整个罐子中橙色弹珠的比例μ\\muμ，二者应该比较接近。 但是即便罐子中橙色弹珠占大多数，我们抓一把出来也可能全是绿色弹珠，也就是说估计值和真实值差别可能很大，那么这种估计方法的合理性何在？样本中真的体现了总体中的某些信息吗？ 答案是肯定的，只是这种关系应当用概率的语言来描述：当样本数量N非常大时，ν\\nuν很可能很接近μ\\muμ，数学表达式为： P[∣ν−μ∣&gt;ϵ]≤2exp⁡(−2ϵ2N)\\mathbb{P}[|\\nu-\\mu|&gt;\\epsilon] \\leq 2 \\exp \\left(-2 \\epsilon^{2} N\\right) P[∣ν−μ∣&gt;ϵ]≤2exp(−2ϵ2N) 对于任何一个大于0的ϵ\\epsilonϵ，当N足够大时，总有上式成立，这一公式被称为Hoeffding’s Inequality，ν\\nuν与μ\\muμ的关系被称为probably approximately correct(PAC). 这里稍微扩展一下：学过概率论的同学应该能感觉到这个公式与大数定律非常像，事实上这个公式与大数定律的本质是一样的，因为μ\\muμ就是ν\\nuν的期望值，大数定律的一般形式是: lim⁡n→∞P(∥1n∑i=1nXi−1n∑i=1nEXi∥&lt;ϵ)=1,∀ ϵ&gt;0\\lim_{n\\rightarrow\\infty}P(\\|\\frac1n\\sum^n_{i=1}X_i-\\frac1n\\sum^n_{i=1}EX_i\\|&lt;\\epsilon)=1,\\forall\\ \\epsilon&gt;0 n→∞lim​P(∥n1​i=1∑n​Xi​−n1​i=1∑n​EXi​∥&lt;ϵ)=1,∀ ϵ&gt;0 本例中样本是二项分布，可以用伯努利大数定律$$\\lim_{n\\rightarrow\\infty}P(|\\frac{S_{n}}{n}-p|&lt;\\epsilon)=1,\\forall\\ \\epsilon&gt;0$$直接得到Hoeffding’s Inequality. 由此也可以知道ν\\nuν与μ\\muμ的关系就是ν\\nuν依概率收敛于μ\\muμ，因为大数定律的结论就是依概率收敛（convergence in probability）。 值得注意的是，虽然我们知道有PAC这样的关系存在，但是要算Hoeffding’s Inequality右边的界一般是不可能的，因为我们要知道μ\\muμ，而在实际问题中当我们知道μ\\muμ时就没有必要再用μ\\muμ估计它。 3 Connection to Learning 下面就从刚刚的例子转化到learning的问题中。 仍然假设机器学习的目标是f(x)f(x)f(x)，现在已有假设h(x)h(x)h(x)，罐子中的每一个弹珠代表一个数据xxx，若h(x)=f(x)h(x)=f(x)h(x)=f(x)，则弹珠颜色是绿色的，否则弹珠颜色是橘色的，此时整个罐子中的橘色弹珠的比例就代表了在全部数据上h(x)h(x)h(x)的表现，随机取出的一把弹珠中橘色弹珠的比例就代表了样本数据上h(x)h(x)h(x)的表现。 此时可以将之前的机器学习流程图进行扩充如下： 其中在整个数据集上有两次抽样，分别用来做训练集和测试h(x)≈f(x)h(x)\\approx f(x)h(x)≈f(x) 此时在整个数据集上的表现记为EoutE_{out}Eout​，是未知的；样本数据集上的表现记为EinE_{in}Ein​，是已知的。 unknown Eout (h)=Ex∼P[∣h(x)≠f(x)] by known Ein (h)=1N∑n=1N[h(xn)≠yn]\\begin{aligned} &amp;\\text { unknown } E_{\\text {out }}(\\mathrm{h})=\\underset{\\mathbf{x} \\sim P}{\\mathcal{E}}[| h(\\mathbf{x}) \\neq f(\\mathbf{x})]\\\\ &amp;\\text { by known } E_{\\text {in }}(\\mathrm{h})=\\frac{1}{N} \\sum_{n=1}^{N}\\left[h\\left(\\mathbf{x}_{n}\\right) \\neq y_{n}\\right] \\end{aligned}​ unknown Eout ​(h)=x∼PE​[∣h(x)​=f(x)] by known Ein ​(h)=N1​n=1∑N​[h(xn​)​=yn​]​ 二者分别对应于前述Hoeffding’s Inequality中的μ\\muμ和ν\\nuν，因此得到新的表示形式为： P[∣Ein(h)−Eout(h)∣&gt;ϵ]≤2exp⁡(−2ϵ2N)\\mathbb{P}[|E_{in}(h)-E_{out}(h)|&gt;\\epsilon] \\leq 2 \\exp \\left(-2 \\epsilon^{2} N\\right) P[∣Ein​(h)−Eout​(h)∣&gt;ϵ]≤2exp(−2ϵ2N) 这说明Ein(h)=Eout(h)E_{in}(h)=E_{out}(h)Ein​(h)=Eout​(h)是probably approximately correct的关系，当Ein(h)E_{in}(h)Ein​(h)很小的时候，很大概率上Eout(h)E_{out}(h)Eout​(h)也很小，h≈fh\\approx fh≈f 机器学习流程图中最终选择的规则是ggg，那么如何知道是否g≈fg\\approx fg≈f呢？ ggg是从假设集中选择出来的，若假设集中存在hhh能使得Ein(h)E_{in}(h)Ein​(h)很小（此时也就有这样的h≈fh\\approx fh≈f），而我们恰好选择了这样的hhh作为最终的ggg，那么就有g≈fg\\approx fg≈f，若没有这样的hhh或者说没有能选出这样的hhh的算法，也就是说Ein(h)E_{in}(h)Ein​(h)很小的条件达不到，自然无法选出来一个’g≈fg\\approx fg≈f'PAC. 用这样的算法需要有一种方法能够确认某个待定的hhh是好的还是不好的，这个确认的过程我们称为verification，它的流程图为： 通过这种方法可以确认某个假设hhh的表现是否是好的。 4 Connection to Real Learning 当有很多假设hhh时如何做？ 显然可以分别进行验证。但是有一个问题是，若某个假设在验证集上的验证结果是全对，那么要不要立即把这个假设作为ggg？ 先思考另一个问题：请150个同学丢铜板，每个人丢5次，至少有一个同学5次丢到的结果都是正面，这样的概率是多少？ 当硬币是公平的时，对一个人来说得到五个正面的概率是3132\\frac{31}{32}3231​，对150个人来说至少一个人得到5个正面的概率是1−(3132)150&gt;0.991-(\\frac{31}{32})^{150}&gt;0.991−(3231​)150&gt;0.99 Hoeffding’s Inequality告诉我们当样本数量N比较大时，EinE_{in}Ein​与EoutE_{out}Eout​很大概率上是接近的，但也有差别较大的情况，我们称这种情况是坏的，得到这样的结果的样本称为bad sample. 当数据在某个假设hhh上是坏数据，我们就说 这组数据是坏数据。 对于每一个假设hhh，Hoeffding’s Inequality告诉我们坏数据的可能性很小，也就是下图中一行中标出bad的概率是小的。 现在还想知道在所有的假设上坏数据的概率是多少，其上界可以采用下图所示方法估计： 从中可以看出，在所有的假设上Ein(g)=Eout(g)E_{in}(g)=E_{out}(g)Ein​(g)=Eout​(g)仍然是PAC的（与算法无关）。 当假设的数量M是有限的时，只要N足够大，无论用什么样的算法，仍然可以保证Ein(g)≈Eout(g)E_{in}(g)\\approx E_{out}(g)Ein​(g)≈Eout​(g)；若Ein(g)≈0E_{in}(g)\\approx 0Ein​(g)≈0，则PAC条件保证了Eout(g)≈0E_{out}(g)\\approx 0Eout​(g)≈0 此时可以说，当M有限大时机器可以学习到规则，本问中前述的机器学习流程图中的验证假设h≈fh\\approx fh≈f的线可以直接连到g≈fg\\approx fg≈f上。 但是如同之前所讲的PLA这样的算法其假设hhh有无穷个，这样的情况留待后续说明。","permalink":"http://yangtf983.github.io/2020/03/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B04%EF%BC%9AFeasibility%20of%20Learning/","photos":[]},{"tags":[{"name":"Semi-supervised Learning","slug":"Semi-supervised-Learning","permalink":"http://yangtf983.github.io/tags/Semi-supervised-Learning/"}],"title":"听课笔记：半监督学习（李宏毅）","date":"2020/03/11","text":"0 说明 本文是一篇半监督学习的听课笔记，主讲者是李宏毅，课程视频网址 1 半监督学习介绍 半监督学习的数据集由有标签数据集{(xr,y^r)}r=1R\\left\\{\\left(x^{r}, \\hat{y}^{r}\\right)\\right\\}_{r=1}^{R}{(xr,y^​r)}r=1R​和无标签数据集{xu}u=RR+U\\left\\{x^{u}\\right\\}_{u=R}^{R+U}{xu}u=RR+U​共同组成，通常二者数量U≫RU\\gg RU≫R. 半监督学习可以分为tranductive learning和inductive learning两类，二者最简单的区分方法是transductive learning的无标签数据就是它的测试集，只不过相对于监督学习而言transductive learning使用了测试集的feature，而inductive learning的无标签数据不是测试集，无法事先知道测试集是什么，建立的模型需要能够预测整个样本空间上的点。 为什么要用半监督学习？ （1）在机器学习过程中从来不缺乏data，缺的是labeled data，例如网络上有数不清的图片，但其中有标签的图片占比却很少，且给如此大量的图片标注label的代价过于昂贵； （2）人类也是常常在做半监督学习，例如见到以前未曾见过的狗的相片进一步丰富对够和其他动物之间区别的认知。这说明无标签数据是可以用来提高用有标签数据学习的结果的，并且这种现象在人类学习中也很常见。 为什么半监督学习可能有用？ 无标签数据的分布能传递出一定信息，但是否有用取决于假设是否合理。 2 Semi-supervised Generative Model 已给标签数据集xr∈C1,C2x^{r}\\in C_{1},C_{2}xr∈C1​,C2​ 寻找一个最有可能的先验概率P(Ci)P(C_{i})P(Ci​)与所属类别的条件概率P(x∣Ci)P(x|C_{i})P(x∣Ci​) P(x∣Ci)P(x|C_{i})P(x∣Ci​)是一个参数为μi\\mu^{i}μi和Σ\\SigmaΣ的高斯分布。 P(C1∣x)=P(x∣C1)P(C1)P(x∣C1)P(C1)+P(x∣C2)P(C2)P\\left(C_{1} | x\\right)=\\frac{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)}{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)+P\\left(x | C_{2}\\right) P(C_{2})} P(C1​∣x)=P(x∣C1​)P(C1​)+P(x∣C2​)P(C2​)P(x∣C1​)P(C1​)​ 1. 若用上无标签数据（用浅绿色表示），图像可能会变成： 从图中可以看出，虽然浅绿色数据没有标签信息，但我们已经倾向于根据它们将决策边界由椭圆改为圆形，在这个过程中重新估计了$P(C_{1}),P(C_{2}),\\mu^{1},\\mu^{2},\\Sigma$，从而影响了decision boundary. 从上述过程可以看出无标签数据对生成模型做决策产生的作用，可以利用如下方法利用这种影响： 1. 初始化$\\theta=\\{P(C_{1}),P(C_{2}),\\mu^1,\\mu^2,\\Sigma\\}$; 2. E步：计算每个无标签数据的后验概率$P_{\\theta}(C_{1}|x^\\mu)$; 3. M步：更新模型 $$P(C_1)=\\frac{N_{1}+\\Sigma_{x^\\mu}P(C_1|x^\\mu)}{N}$$ $$\\mu^{1}=\\frac{1}{N_{1}} \\sum_{x^{r} \\in C_{1}} x^{r}+\\frac{1}{\\sum_{x^{u}} P\\left(C_{1} | x^{u}\\right)} \\sum_{x^{u}} P\\left(C_{1} | x^{u}\\right) x^{u}$$ $N$是总数据量，$N_1$是属于$C_1$的数据量。 4. 重复EM步。 理论上可以保证收敛，但是初始值会影响收敛的结果。 **为什么采取这种做法？** 只存在有标签数据时用最大似然法求参数的表达为$\\log L(\\theta)=\\sum_{x^{r}} \\log P_{\\theta}\\left(x^{r}, \\hat{y}^{r}\\right)$，其中$P_\\theta(x^r,\\hat{y}^r)=P_\\theta(x^r|\\hat{y}^r)P(\\hat{y}^r)$，此最大似然函数有解析解。 当加上无标签数据时，最大似然函数的对数函数变为$\\log L(\\theta)=\\sum_{x^{r}} \\log P_{\\theta}\\left(x^{r}, \\hat{y}^{r}\\right)+\\sum_{x^u}\\log P_\\theta(x^u)$，其中$P_\\theta(x^u)=P_\\theta(x^u|C_1)P(C_1)+P_\\theta(x^u|C_2)P(C_2)$（由于$x^u$可能在任何一个类中），此最大似然函数只有数值解，采用迭代方法。 ## 3 Low-Density Separation 思想：非黑即白。即不同类标签中有明显的分界线。 **Self-training** self-training是低密度分割假设最具代表性的最简单的算法。 1. 用有标签数据训练一个模型$f^*$ 2. 将模型$f^*$用在无标签数据集上，产生的标签称为pseudo-label 3. 从无标签数据集中移出一个数据集放入有标签数据集 4. 重复前三步 self-training是hard label，也就是模型结果一定是属于某一个类，而generative model则是soft label，只要算出属于某类的概率即可以迭代。 若用soft label的模型来训练self-training，则迭代不会更新$f^*$，因为新放入有标签数据集的无标签数据本身就是在$f^*$上的。 若用neural network训练self-training，则一定要用hard label，因为soft label不会更新模型。 **Entropy-based Regularization** 模型在无标签数据上对每一个数据属于每个类的可能性都会有一个估计值，这些估计值越集中，我们认为这样的模型越好，这些估计值越不集中，我们认为这样的估计值越不好。 上图中前两个分布的概率比较集中，是好的，最后一个概率分散，是不好的。 衡量$y^u$的集中度可以用熵（entropy），熵的计算公式是$E(y^u)=-\\sum^5_{m=1}y^u_m\\ln (y^u_m)$，前两种情况的$E(y^u)=0$，第三种情况的$E(y^u)=-\\ln (\\frac{1}{5})=\\ln 5$，是$E(y^u)$的最大值。 对于有标签数据，显然目的是使得模型结果与真实标签最为接近，这二者的距离表示为$\\sum_{x^r}C(y^r,\\hat{y}^r)$ 因此熵准则下目的是最小化： $$L=\\sum_{x^r}C(y^r,\\hat{y}^r)+\\lambda \\sum_{y^u}E(y^u)$$ 其中$\\lambda$的选取依赖于有标签数据与无标签数据相对重要程度。 由于$L$可以计算微分，因此可以用梯度下降法训练。 **Semi-supervised SVM** SVM的目的是找到一个能够最小化误差的最宽的线作为分类器。 半监督学习SVM的思路如上图所示，无标签数据有很多不同的标签可能，目的是找到一种情况使得SVM分类器的误差最小同时找到最粗的那条线。 当无标签数据点较多时，情况非常多以至于无法穷举。一种近似的思路是首先给每个无标签数据确定一个标签，之后每次改一些标签，若修改后能够产生更好的分类器，则修改被保留 ，否则重新修改。 4 Semi-supervised Learning Smoothness Assumption 思想：近朱者赤近墨者黑。即相似的x有相似的y^\\hat{y}y^​ 定义：若一个高密度区域中的两个点x1,x2x_{1},x_{2}x1​,x2​距离很近，那么其对应的输出y1,y2y_{1},y_{2}y1​,y2​也应当接近或一样。 文件分类问题常符合此假设。下图中did_idi​代表文章，实心点代表文章中包含的词汇类别。左图中很容易看出来文章d1d_1d1​和d3d_3d3​类别相近，d4d_4d4​和d2d_2d2​类别相近。右图中则看不出四者哪些相近。 但是当搜集到更多文章数据时，可以产生下图： 根据半监督学习平滑性假设，从上图中又可以看出文章d1d_1d1​和d3d_3d3​类别相近，d4d_4d4​和d2d_2d2​类别相近。 Cluster and then Label 上图中红色和绿色点分别表示两类数据，蓝色点表示无标签数据。先聚类再标注，可以将数据分为三簇，一簇属于第一类，两簇属于第二类。 Graph-based Approach 光滑性假设的应用前提是要知道怎么判断不同点是否在同一个高密度区域中很接近。 在图中可以较好地表现出来这一特征，但如何建立这样的图？ 有时这种图可以很自然得到。如进行网页的分类，网页和网页之间可以通过有超链接的关系连接起来；又如进行论文间的联系，可以 利用论文之间引用的关系进行连接不同论文。 但有时需要根据一些经验和方法建立图： step 1. 定义点xix^ixi与xjx^jxj之间的相似度s(xi,xj)s(x^i,x^j)s(xi,xj) step 2. 建立graph: K Nearest Neighbor：将每个点与距其最近的k个点连起来； e-Neighborhood：将每个点与距离其距离小于 e的点全部连起来； step 3. 边的权重与相似度s(xi,xj)s(x^i,x^j)s(xi,xj)成正比 一种建议的定义相似度的函数是Gaussian Radial Basis Function: s(xi,xj)=exp⁡(−γ∥xi−xj∥2)s(x^i,x^j)=\\exp(-\\gamma\\|x^i-x^j\\|^2) s(xi,xj)=exp(−γ∥xi−xj∥2) 经验上用这样的函数效果一般较好。这个函数对距离增加的惩罚很大。 图建立后，就可以使用图方法。 如假定有标签数据的标签会影响到其邻近点，标签会通过图传播，则会产生下图的情况： 一种常见的定义图的平滑性的方法： S=12∑i,j(yi−yj)2,i,j for all data(no matter labbelled or not)S=\\frac{1}{2}\\sum_{i,j}(y^i-y^j)^2,i,j\\ for\\ all\\ data(no\\ matter \\ labbelled\\ or\\ not) S=21​i,j∑​(yi−yj)2,i,j for all data(no matter labbelled or not) SSS越小越光滑。 图中左图S=0.5S=0.5S=0.5，右图S=3S=3S=3，左图更光滑。 用矩阵方法表示光滑度函数如下图： 其中矩阵WWW每个元素是边缘权重，DDD的对角线元素是WWW对应行元素的和。 SSS的值取决于网络的参数。 同样可以通过梯度下降最小化LLL","permalink":"http://yangtf983.github.io/2020/03/11/%E5%90%AC%E8%AF%BE%E7%AC%94%E8%AE%B0%EF%BC%9A%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%88%E6%9D%8E%E5%AE%8F%E6%AF%85%EF%BC%89/","photos":[]},{"tags":[{"name":"机器学习基石听课笔记","slug":"机器学习基石听课笔记","permalink":"http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E5%90%AC%E8%AF%BE%E7%AC%94%E8%AE%B0/"}],"title":"机器学习基石笔记笔记3：学习类型","date":"2020/01/24","text":"【0】说明 之前在笔记1中的机器学习三要素*（ 1. 存在一个可以被学习的潜在模式； 2. 不知道如何定义规则并写入程序； 3. 可以获得大量数据。）*中已经讲过，机器学习可以执行的前提是存在可以学习的模式以及足够的数据。在实际情况中，学习的模式和输入的数据都可能有很多不同的类型，相应的也诞生了很多不同的学习算法和思想，根据这些不同，机器学习有很多分类，这一节我们介绍一些分类方法和机器学习类型。 【1】Learning wiwh Different Output Space Y\\mathcal{Y}Y 上一节我们介绍了PLA算法和SVM算法，它们只能将一个空间分成两部分，也就是两个不同的类，即输出空间只有+1和-1,称这样的分类问题为二元分类。 此外，根据输出空间的更多不同情况，还存在多种类别，总结如下： 二元分类 binary classification 输出空间：Y={−1,+1}\\mathcal{Y}=\\{-1,+1\\}Y={−1,+1} 举例：判断是否发放信用卡（输出空间：发放；不发放） 多元分类 multiclass classification 输出空间：Y={1,2，…,K}(abstractly)\\mathcal{Y}=\\{1,2，\\ldots,K\\}(abstractly)Y={1,2，…,K}(abstractly) 举例：手写数字识别（输出空间：0,1,…,9） 回归 regression 输出空间：Y=R or Y=[lower,upper]⊂R\\mathcal{Y}=\\mathbb{R}\\ or\\ \\mathcal{Y}=[lower,upper]\\subset\\mathbb{R}Y=R or Y=[lower,upper]⊂R 举例：根据公司数据预测股价 结构化学习 structured learning 输出空间：Y=structures\\mathcal{Y}=structuresY=structures 距离：蛋白质测序；语句结构识别（自动识别 一个句子中哪些是动词哪些是名词等） 除了以上四种，还有更多分类，这里不再进行介绍。但是值得一提的是，在这么多问题中，最核心的是二分类和回归，许多其他的问题的算法都是由这两种问题的算法进行改进得到的，甚至对于回归问题做一些处理也可以解决而分类问题（如逻辑斯谛回归）。 【2】Learning with Different Data Label yny_{n}yn​ 根据标签值的不同特点可以将机器学习分成以下几类： 监督学习 supervised learning 所有的数据都有标签，学习的目的是给出正确的标签值。 无监督学习 unsupervised learning 所有数据都没有标签，学习的目的是找出感兴趣的数据结构，比如概率密度等。 半监督学习 semi-supervised learning 一部分数据有标签，另一部分数据无标签，无标签数据可以辅助提高学习的精度，一般而言无标签数据远多于有标签数据。 强化学习 reinforcement learning 输出结果是一个行为而不是一个标签，建立奖励函数对这个行为的好坏进行判断。一般而言强化学习更加适合有明确规则的情况，如围棋。规则明确并且简单时容易建立奖励函数。强化学习需要知道三个信息，分别是：action(行为),observation(观测值),reward(奖励)。目前其最广为人知的应用可能是alphago. 【3】Learning with Different Protocol batch learning 特点：每次抽取一批数据进行训练，这要求所有的数据在一开始就是确定的，计算机通过一定的程序每次从中随机抽取一批进行训练。 online learning 特点：数据一个个进来，每次数据的更新都能对模型进行优化，因此称作在线学习。例如在线邮件过滤系统可以根据当前算法判断下一封邮件的内容，再根据用户反馈（如用户反馈判断错误）及时对模型进行优化。 avtive learning 特点：是一种新的机器学习类型，其特点是让机器主动问问题来提升模型的性能。如手写数字识别中，可以让机器对自己判断困难的数字进行提问，由人来对其打标签，这类判断困难的 数据往往对于提升模型性能更加有效。其优势之一是在获取样本标签困难的时候可以节约时间和成本，只对一些重要的数据打标签。 以上三种学习类型可以分别类比为：填鸭式、老师教学和主动问问题。 【4】Learning with Different Input Space X\\mathcal{X}X 根据输入的值类型不同，可以分为具体特征、原始特征和抽象特征，下面分别介绍： concrete feature 具体特征是指输入值具有清晰的实际意义，例如输入用户的收入和存款，判断是否应该发放信用卡。这样的例子中输入数据的实际意义是很明显的，与需要判断的结果联系很密切。具体特征的选择要求实验者有一定的相关知识，这样才能选择最合适的特征。也由于具体特征与判断目标的相关性较大，无关信息少，因此相应的算法往往更加简单，计算难度也较小。 raw feature 原始特征是指具有一些简单的实际意义的数据。例如输入图片的像素值组成的数组来判断图像中是什么。这样的数据中包含与判断结果相关的信息，例如在鉴别数字1和5时可以通过像素值得到图像的对称性和密度，这两个特征对于判别1和5显然是较为有效的，如果提取出来这两个特征并用其来作为判别的数据，那么就变成了具体特征，这个提取特征的过程有一个好听的名字叫做“特征提取”。在传统机器学习中，这一步往往需要认为来进行尝试，在深度学习中已经可以自动提取，不过需要更加大量的数据。 abstract feature 抽象特征是指数据没有或者很少有实际意义，例如输入数据是用户ID，ID数字的大小和接近程度并没有实际的意义，不同的ID只是代表不同的用户，显然其中包含的信息非常少。抽象特征也是三种输入空间中最难进行机器学习的 一种，也需要特征工程。","permalink":"http://yangtf983.github.io/2020/01/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B03%EF%BC%9A%E5%AD%A6%E4%B9%A0%E7%B1%BB%E5%9E%8B/","photos":[]},{"tags":[{"name":"机器学习基石听课笔记","slug":"机器学习基石听课笔记","permalink":"http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E5%90%AC%E8%AF%BE%E7%AC%94%E8%AE%B0/"}],"title":"机器学习基石笔记2：从PLA到SVM","date":"2020/01/23","text":"【0】说明 PLA，全称perceptron learning algorithm，是一种二分类算法，也可以认为是单层神经网络，与SVM有着密不可分的关系。SVM，即support vector machine，中文名是支持向量机，也是一种常见的二分类算法，这种算法的 一种推导方式就是从PLA出发，找到一条“最好的”PLA分类器。在机器学习基石课程中林轩田老师只介绍了PLA，而SVM是他在后续的另一门课程——机器学习技法——中介绍的。由于二者具有紧密的联系，故在此一并介绍。 【1】PLA介绍 PLA是解决二分类问题的一种算法，其来源可以从几何上理解：自变量可以投影到一个超平面，通过构造这个超平面的一个线性分割，尽可能好地将不同的类别的点分开。 PLA(perceptron learning algorithm)，中文名：线性感知机算法，又称线性分类器。 【2】符号表示 已有数据指标X=(x1,…,xd)\\mathcal{X}=(x_{1},\\ldots,x_{d})X=(x1​,…,xd​)，数据标签Y=(+1,−1)\\mathcal{Y}=(+1,-1)Y=(+1,−1)，目标是通过PLA算法 计算出一系列权重W=(w1,…,wd)\\mathcal{W}=(w_{1},\\ldots,w_{d})W=(w1​,…,wd​)，使得当Σi=1dwixi&gt;threshold\\Sigma^{d}_{i=1}w_{i}x_{i}&gt;thresholdΣi=1d​wi​xi​&gt;threshold时，预测标签值y=1，当Σi=1dwixi&lt;threshold\\Sigma^{d}_{i=1}w_{i}x_{i}&lt;thresholdΣi=1d​wi​xi​&lt;threshold，预测标签值y=-1，即h(x)=sign((Σi=1dwixi)−threshold),h∈Hh(x)=sign((\\Sigma^{d}_{i=1}w_{i}x_{i})-threshold),h\\in\\mathcal{H}h(x)=sign((Σi=1d​wi​xi​)−threshold),h∈H h(x)h(x)h(x)的形式可以进行简化： h(x)=sign((Σi=1dwixi)+(−threshold)∗(+1))=sign(Σi=0dwixi)=sign(wTx),h∈H\\begin{array}{rl} h(x)&amp;=sign((\\Sigma^{d}_{i=1}w_{i}x_{i})+(-threshold)*(+1))\\\\ &amp;=sign(\\Sigma^{d}_{i=0}w_{i}x_{i})\\\\ &amp;=sign(w^{T}x),h\\in\\mathcal{H}\\end{array}h(x)​=sign((Σi=1d​wi​xi​)+(−threshold)∗(+1))=sign(Σi=0d​wi​xi​)=sign(wTx),h∈H​ 容易看出上述简化其实是将thresholdthresholdthreshold记为了w0w_{0}w0​，显然x0=1x_{0}=1x0​=1. 简化成这种形式有两个好处，一是可以把threshold放到权重w中一起求出，二是在几何表示上更加直观，付出了将指标维数+1的牺牲，但是可以将超平面转移到一定经过原点，这对于我们后面的图形表示以及理解PLA算法都很有帮助。 【3】PLA过程 从表达式h(x)=sign(wTx),h∈Hh(x)=sign(w^{T}x),h\\in\\mathcal{H}h(x)=sign(wTx),h∈H中可以看出，对于和向量www呈锐角的数据xxx，由于二者内积是正的，所以用该分类器给出的预测值是1；反之，对于呈钝角的数据，给出的预测值是-1，对于呈直角的数据，该式未给出合理的预测值，此时可以理解为从该分类器中无法辨别该数据究竟更加符合哪一类，可以随机制定一个类别。 根据上述分析，若将自变量xxx在空间中的位置标出，并在其中画出以www为法向量并经过原点的（超）平面，则平面一侧的所有点代表的向量都与www呈锐角，另一侧的所有点代表的向量都与www呈钝角，平面上的所有点代表的向量都与www呈直角，即该（超）平面就是由上式h(x)h(x)h(x)定义的分类器。 下面以二维情况为例对PLA算法进行解释。 显然，二维情况下H\\mathcal{H}H应当包含平面上所有的直线，最理想的算法应当是从平面上所有的直线中找到一条最好的直线作为ggg，不过我们无法将平面上所有的直线遍历，一个合理的办法是设定初值再进行迭代。这也是PLA算法的基本思路。下面看一下迭代过程： For t=0,1,… step1. find the next mistake of wtw_{t}wt​ called (xn(t),yn(t))(x_{n(t)}, y_{n(t)})(xn(t)​,yn(t)​) sign(wtTxn(t)≠yn(t))sign(w^{T}_{t}x_{n(t)}\\neq y_{n(t)}) sign(wtT​xn(t)​​=yn(t)​) step2. correct the mistake by wt+1←wt+yn(t)xn(t)w_{t+1}\\leftarrow w_{t}+y_{n(t)}x_{n(t)} wt+1​←wt​+yn(t)​xn(t)​ …until a full cycle of not encountering mistakes (“next” can follow navie cycle (1,…,N) or precomputed random cycle) 上述算法又被称为循环PLA，因为它必须经过一个确定的循环没有错误后才会终止。 从中可以看到每次迭代的规则是： wt+1←wt+yn(t)xn(t)w_{t+1}\\leftarrow w_{t}+y_{n(t)}x_{n(t)} wt+1​←wt​+yn(t)​xn(t)​ 由于只对sign(wtTxn(t)≠yn(t))sign(w^{T}_{t}x_{n(t)}\\neq y_{n(t)})sign(wtT​xn(t)​​=yn(t)​)的点进行迭代得到新的www，即找到yn(t)wtTxn(t)&lt;0y_{n(t)}w^{T}_{t}x_{n(t)}&lt;0yn(t)​wtT​xn(t)​&lt;0的点时进行一次迭代，因此最终得到的ggg是满足yn(t)wtTxn(t)&gt;0,∀ty_{n(t)}w^{T}_{t}x_{n(t)}&gt;0,\\forall tyn(t)​wtT​xn(t)​&gt;0,∀t的线性函数。 且每次迭代后，yn(t)wt+1Txn(t)≥yn(t)wtTxn(t)y_{n(t)}w^{T}_{t+1}x_{n(t)}\\geq y_{n(t)}w^{T}_{t}x_{n(t)}yn(t)​wt+1T​xn(t)​≥yn(t)​wtT​xn(t)​，即yn(t)wt+1Txn(t)y_{n(t)}w^{T}_{t+1}x_{n(t)}yn(t)​wt+1T​xn(t)​一定在变大，所以模型确实随着不断迭代在优化。 从图形上看，用向量运算的方法容易看出，每次迭代的结果都使得wt+1w_{t+1}wt+1​和xn(t)x_{n(t)}xn(t)​之间的夹角变得更优了，如下图： 不过值得一提的是，这种优化只是对于预测错误的那个点的优化，在总的数据集上的表现有没有优化是不确定的，有可能在某一步迭代过后得到分类器在总的数据集上的表现更差。下面是一个PLA迭代过程中每一步的表示： 从这个实例可以看到，第七到第八步迭代时，用来计算迭代式的点进行了优化并通过迭代取得了正确的结果，但是新的分类器在数据集上的错误却增加了一个，即表现变差了。此外，这个迭代 最终产生了一个在数据集上表现完美的分类器，说明了前述的循环PLA算法是有效的，我们的机器确实学到了东西。 【4】有穷性与Pocket算法 因为终止条件没有时间或步数限制，所以上述算法的有穷性有待考虑。 循环PLA算法能够终止的前提是对所有的数据点都得到正确的分类，因此显然其终止前提是至少存在一个分类器可以将不同类别的点 分开，因为PLA是线性分类器，所以这个前提的等价说法是不同类别的区域应当是线性可分的，也就是可以用线性分类器将其分开。这个要求可以用数学中一个概念来表述，那就是不同类别点组成的区域应当是凸区域。 上述表述说明线性可分（凸区域）是终止的前提条件，下面我们说明，当数据点线性可分时，我们可以找到算法步数的上界的表达式，从而证明线性可分是算法有界的充分条件；当数据点非线性可分时，我们应当对算法的终止条件进行修改避免死循环。 线性可分时 线性可分时，存在理想模式fff，记对应的分类器的权重为wfw_{f}wf​，满足yn=sign(wfTxn),∀ny_{n}=sign(w^{T}_{f}x_{n}),\\forall nyn​=sign(wfT​xn​),∀n，按照循环PLA算法流程，仍取w0=(0,…,0)w_{0}=(0,\\ldots,0)w0​=(0,…,0)，可以做如下推导： yn(t)wfTxn(t)≥minn{ynwfTxn}&gt;0wfTwt+1=wfT(wt+yn(t)xn(t))≥wfTwt+minn{ynwfTxn}≥…≥wfTw0+(t+1)∗minn{ynwfTxn}=(t+1)∗minnynwfTxn∣∣wt∣∣2=∣∣wt−1+yn(t−1)xn(t−1)∣∣2=∣∣wt−1∣∣2+2yn(t−1)wt−1Txn(t−1)+∣∣yn(t−1)xn(t−1)∣∣2≤∣∣wt−1∣∣2+0+∣∣yn(t−1)xn(t−1)∣∣2≤∣∣wt−1∣∣2+maxn{∣∣xn∣∣2}≤…≤∣∣w0∣∣2+t∗maxn{∣∣xn∣∣2}=t∗maxn{∣∣xn∣∣2}wfT∥wf∥wT∥wT∥≥T∗min⁡n{ynwfTxn}∥wfT∥∗∥wT∥≥T∗min⁡n{ynwfTxn}∥wfT∥∗T∗max⁡n∥xn∥≥T∗min⁡n{ynwfTxn}∥wfT∥∗max⁡n∥xn∥=T∗ constant \\begin{array}{rl} y_{n(t)}w^{T}_{f}x_{n(t)}&amp;\\geq min_{n}\\{ y_{n}w^{T}_{f}x_{n} \\}\\\\ &amp;&gt;0\\\\ w^{T}_{f} w_{t+1} &amp;= w^{T}_{f}(w_{t}+y_{n(t)}x_{n(t)}) \\\\ &amp;\\geq w^{T}_{f}w_{t} + min_{n}\\{ y_{n}w^{T}_{f}x_{n} \\} \\\\ &amp;\\geq \\ldots \\\\ &amp;\\geq w^{T}_{f}w_{0}+(t+1)*min_{n}\\{ y_{n}w^{T}_{f}x_{n} \\} \\\\ &amp;=(t+1)*min_{n}{y_{n}w^{T}_{f}x_{n}}\\\\ ||w_{t}||^{2} &amp;= || w_{t-1}+y_{n(t-1)x_{n(t-1)}} ||^{2}\\\\ &amp;= ||w_{t-1}||^{2}+2y_{n(t-1)}w^{T}_{t-1}x_{n(t-1)}+|| y_{n(t-1)}x_{n(t-1)} ||^{2}\\\\ &amp;\\leq ||w_{t-1}||^{2}+0+|| y_{n(t-1)}x_{n(t-1)} ||^{2} \\\\ &amp;\\leq ||w_{t-1}||^{2}+max_{n}\\{ ||x_{n}||^{2} \\}\\\\ &amp;\\leq \\ldots\\\\ &amp;\\leq ||w_{0}||^{2}+t*max_{n}\\{ ||x_{n}||^{2} \\}\\\\ &amp;= t*max_{n}\\{ ||x_{n}||^{2} \\}\\\\ \\frac{w_{f}^{T}}{\\left\\|w_{f}\\right\\|} \\frac{ w_{T} }{\\left\\|w_{T}\\right\\|} &amp;\\geq \\frac{T * \\min _{n}\\left\\{y_{n} \\mathrm{w}_{f}^{T} x_{n}\\right\\}}{\\left\\|w_{f}^{T}\\right\\|*\\left\\|w_{T}\\right\\|}\\\\ &amp;\\geq \\frac{T * \\min _{n}\\left\\{y_{n} w_{f}^{T} x_{n}\\right\\}}{\\left\\|w_{f}^{T}\\right\\| * \\sqrt{T} * \\max _{n}\\left\\|x_{n}\\right\\|} \\\\ &amp;\\geq \\frac{\\sqrt{T} * \\min _{n}\\left\\{y_{n} \\mathrm{w}_{f}^{T} x_{n}\\right\\}}{\\left\\|w_{f}^{T}\\right\\|*\\max _{n}\\left\\|x_{n}\\right\\|}\\\\ &amp;=\\sqrt{T} * \\text { constant } \\end{array}yn(t)​wfT​xn(t)​wfT​wt+1​∣∣wt​∣∣2∥wf​∥wfT​​∥wT​∥wT​​​≥minn​{yn​wfT​xn​}&gt;0=wfT​(wt​+yn(t)​xn(t)​)≥wfT​wt​+minn​{yn​wfT​xn​}≥…≥wfT​w0​+(t+1)∗minn​{yn​wfT​xn​}=(t+1)∗minn​yn​wfT​xn​=∣∣wt−1​+yn(t−1)xn(t−1)​​∣∣2=∣∣wt−1​∣∣2+2yn(t−1)​wt−1T​xn(t−1)​+∣∣yn(t−1)​xn(t−1)​∣∣2≤∣∣wt−1​∣∣2+0+∣∣yn(t−1)​xn(t−1)​∣∣2≤∣∣wt−1​∣∣2+maxn​{∣∣xn​∣∣2}≤…≤∣∣w0​∣∣2+t∗maxn​{∣∣xn​∣∣2}=t∗maxn​{∣∣xn​∣∣2}≥∥wfT​∥∗∥wT​∥T∗minn​{yn​wfT​xn​}​≥∥wfT​∥∗T​∗maxn​∥xn​∥T∗minn​{yn​wfT​xn​}​≥∥wfT​∥∗maxn​∥xn​∥T​∗minn​{yn​wfT​xn​}​=T​∗ constant ​ 由于wfT∥wf∥wT∥wT∥≤1\\frac{w_{f}^{T}}{\\left\\|w_{f}\\right\\|} \\frac{ w_{T} }{\\left\\|w_{T}\\right\\|} \\leq 1∥wf​∥wfT​​∥wT​∥wT​​≤1，故T∗min⁡n{ynwfTxn}∥wfT∥∗max⁡n∥xn∥≤1\\frac{\\sqrt{T} * \\min _{n}\\left\\{y_{n} \\mathrm{w}_{f}^{T} x_{n}\\right\\}}{\\left\\|w_{f}^{T}\\right\\|*\\max _{n}\\left\\|x_{n}\\right\\|}\\leq 1∥wfT​∥∗maxn​∥xn​∥T​∗minn​{yn​wfT​xn​}​≤1，进而得到T≤(T∗min⁡n{ynwfTxn}∥wfT∥∗max⁡n∥xn∥)2=constant′.T\\leq (\\frac{\\sqrt{T} * \\min _{n}\\left\\{y_{n} \\mathrm{w}_{f}^{T} x_{n}\\right\\}}{\\left\\|w_{f}^{T}\\right\\|*\\max _{n}\\left\\|x_{n}\\right\\|})^{2}=\\text{constant}&#x27;.T≤(∥wfT​∥∗maxn​∥xn​∥T​∗minn​{yn​wfT​xn​}​)2=constant′. 至此，我们证明了在存在wfw_{f}wf​的情况下T存在上界，但是由于机器学习问题中我们不可能提前知道wfw_{f}wf​，因此无法算出这个上界的精确值，只能知道其存在上界。此外，wfw_{f}wf​也只有在数据点线性可分的情况下才可能存在（不考虑noise）。 实际问题中我们还常常面临着测量误差（noise的一种）的问题，一组线性可分的数据，可能因为测量误差而并非线性可分，但是在测量误差不大的情况下，使用PLA算法仍然可以找到一个合适的近似函数ggg，只是此时需要对算法的终止条件进行改动，否则就会陷入死循环。 一个自然的想法是：既然g≈fg\\approx fg≈f，不妨求出一个在数据集上表现最好（判断错误数最少）的ggg作为fff的近似。此时，PLA问题等价于优化： wg←argmin⁡w∑n=1N∥yn≠sign⁡(wTxn)∥\\mathbf{w}_{g} \\leftarrow \\underset{\\mathbf{w}}{\\operatorname{argmin}} \\sum_{n=1}^{N} \\| y_{n} \\neq \\operatorname{sign}\\left(\\mathbf{w}^{T} \\mathbf{x}_{n}\\right)\\| wg​←wargmin​n=1∑N​∥yn​​=sign(wTxn​)∥ 这个问题经过证明是一个N-P难问题，我们求解这样的问题一般采用求近似解的方法，不是想办法找到该优化问题的最优解，而是找近似最优解。求PLA问题的近似最优解的算法又称作Pocket算法，其想法是我们总之把当前最好的一个分类器放在口袋中，只有迭代后的新的分类器的表现比当前口袋中分类器的表现更好时才会将当前口袋中的分类器扔掉将新的分类器放入口袋，这里评价好坏的标准就是看谁的wg\\mathbf{w}_{g}wg​更小，此外，为了避免当数据点过多时遍历花费太多时间，一般的选择是仅仅选取一个随机子集来判断分类器的好坏,再通过设定最大迭代步数对算法进行终止。根据这种思想，可以写出Pocket算法流程如下： initialize pocket weight w^\\hat{\\mathbf{w}}w^ For t=0,1,…\\ldots… step1. find a (random) mistake of wt\\mathbf{w}_{t}wt​ called (xn(t),yn(t))(\\mathbf{x}_{n(t)},y_{n(t)})(xn(t)​,yn(t)​) step2. (try to) correct the mistake by wt+1←wt+yn(t)xn(t)\\mathbf{w}_{t+1}\\leftarrow \\mathbf{w}_{t}+y_{n(t)}x_{n(t)} wt+1​←wt​+yn(t)​xn(t)​ step3. if wt+1\\mathbf{w}_{t+1}wt+1​ makes fewer mistakes than w^\\hat{\\mathbf{w}}w^, replace w^\\hat{\\mathbf{w}}w^ by wt+1\\mathbf{w}_{t+1}wt+1​ …until enough iterations return w^\\hat{\\mathbf{w}}w^ (called wPOCKET\\mathbf{w}_{POCKET}wPOCKET​) as ggg 从PLA到SVM PLA算法的可能结果不唯一，如下图展示了同一组数据的多个最优PLA分类器（这里的最优是对所有数据点均判断正确，后续会定义新的“最优”）： 在这种情况下，我们希望定义 更加“严格”的最优，最好是使得上述情况只能有一种最优解。一种合理的方法是将数据测量的误差考虑在内，尽量使得数据测量有误差时不对类别的判断产生影响，也称之为更稳定（more robust）。 一般而言数据测量的误差不大，在真实数据周围较近的位置，因此可以以不同数据点为圆心画等半径圆并要求所有圆不能与分类器相交，此时找出一个分类器使得能够做出最大的半径，这个分类器就是我们要找的最优分类器，如下图第三个分类器： 等价地，我们可以用线宽来表现这种稳定性，不断增加分割线的宽度，当分割线恰好与数据点相交时，线宽最大的线最稳定，二者的等价性是显然的，线宽的表示如图： 从图中也可以看出，最大线宽有时不是由所有点决定的，上例中最大线宽仅仅由三个数据点即可决定，还有一个数据点没有起作用。类似于概率统计中将概率密度不为0的集合定义为支撑集（support set），这里将这些对线宽起决定作用的点定义为支持向量（support vector），将求这个具有最大线宽的线性分类器的方法称为支持向量机（support vector machine）. 用公式表示这个最优化问题： max⁡wmargin⁡(w) subject to every ynwTxn&gt;0margin⁡(w)=min⁡n=1,…,Ndistance⁡(xn,w)\\begin{array}{rl} {\\max _{\\mathbf{w}}} &amp; {\\operatorname{margin}(\\mathbf{w})} \\\\ {\\text { subject to }} &amp; {\\text { every } y_{n} \\mathbf{w}^{T} \\mathbf{x}_{n}&gt;0} \\\\ {} &amp; {\\operatorname{margin}(\\mathbf{w})=\\min _{n=1, \\ldots, N} \\operatorname{distance}\\left(\\mathbf{x}_{n}, \\mathbf{w}\\right)} \\end{array}maxw​ subject to ​margin(w) every yn​wTxn​&gt;0margin(w)=minn=1,…,N​distance(xn​,w)​ 下面处理这个最优化问题。 在本篇文章的【2】符号表示一节中我们将thresholdthresholdthreshold并入了www中： h(x)=sign((Σi=1dwixi)+(−threshold)∗(+1))=sign(Σi=0dwixi)=sign(wTx),h∈H\\begin{array}{rl} h(x)&amp;=sign((\\Sigma^{d}_{i=1}w_{i}x_{i})+(-threshold)*(+1))\\\\ &amp;=sign(\\Sigma^{d}_{i=0}w_{i}x_{i})\\\\ &amp;=sign(w^{T}x),h\\in\\mathcal{H}\\end{array}h(x)​=sign((Σi=1d​wi​xi​)+(−threshold)∗(+1))=sign(Σi=0d​wi​xi​)=sign(wTx),h∈H​ 这里，我们将xxx和www缩短，也就是去掉x0x_{0}x0​和w0w_{0}w0​，将thresholdthresholdthreshold再单独出来，此时h(x)=sign(wTx+b)h(x)=sign(w^{T}x+b)h(x)=sign(wTx+b) 数据点x\\mathbf{x}x到超平面的距离由(x,b,w)( \\mathbf{x},b,\\mathbf{w} )(x,b,w)决定，记为distance⁡(x,b,w)\\operatorname{distance}\\left(\\mathbf{x}, b, \\mathbf{w}\\right)distance(x,b,w) 超平面上的点由wTx+b=0\\mathbf{w}^{T}\\mathbf{x}+b=0wTx+b=0确定。 若记x′\\mathbf{x}&#x27;x′为超平面上的点，则distance⁡(x,b,w)\\operatorname{distance}\\left(\\mathbf{x}, b, \\mathbf{w}\\right)distance(x,b,w)为x′−x\\mathbf{x}&#x27;-\\mathbf{x}x′−x到超平面的法向量w\\mathbf{w}w的映射，即： distance⁡(x,b,w)=project(x′−x)to⊥hyprtplane=∣wT∥w∥(x−x′)∣=(1)1∥w∥∣wTx+b∣\\begin{array}{rl} \\operatorname{distance}\\left(\\mathbf{x}, b, \\mathbf{w}\\right) &amp;= project(\\mathbf{x}&#x27;-\\mathbf{x})to\\perp hyprtplane\\\\ &amp;=\\left|\\frac{\\mathbf{w}^{T}}{\\|\\mathbf{w}\\|}\\left(\\mathbf{x}-\\mathbf{x}^{\\prime}\\right)\\right| \\\\ &amp;\\stackrel{(1)}{=} \\frac{1}{\\|\\mathbf{w}\\|}\\left|\\mathbf{w}^{T} \\mathbf{x}+b\\right| \\end{array}distance(x,b,w)​=project(x′−x)to⊥hyprtplane=∣∣∣​∥w∥wT​(x−x′)∣∣∣​=(1)∥w∥1​∣∣​wTx+b∣∣​​ 这样，原问题就转化为： max⁡b,wmargin⁡(b,w) subject to every yn(wTxn+b)&gt;0margin⁡(b,w)=min⁡n=1,…,N1∥w∥yn(wTxn+b)\\begin{array}{rl} {\\max _{b,\\mathbf{w}}} &amp; {\\operatorname{margin}(b, \\mathbf{w})} \\\\ {\\text { subject to }} &amp; {\\text { every } y_{n} (\\mathbf{w}^{T} \\mathbf{x}_{n}+b)&gt;0} \\\\ &amp; {\\operatorname{margin}(b,\\mathbf{w})}=\\min _{n=1, \\ldots, N} \\frac{1}{\\|\\mathbf{w}\\|}y_{n}(\\mathbf{w}^{T} \\mathbf{x}_{n}+b) \\end{array}maxb,w​ subject to ​margin(b,w) every yn​(wTxn​+b)&gt;0margin(b,w)=minn=1,…,N​∥w∥1​yn​(wTxn​+b)​ 易知wTx+b=0\\mathbf{w}^{T} \\mathbf{x}+b=0wTx+b=0与3wTxn+3b=03\\mathbf{w}^{T} \\mathbf{x}_{n}+3b=03wTxn​+3b=0表示相同的超平面，因此wT\\mathbf{w}^{T}wT与bbb的大小不影响分类器，其比例才影响分类器，故可以通过改变其大小使得min⁡n=1,…,Nyn(wTxn+b)=1\\min _{n=1, \\ldots, N} y_{n}(\\mathbf{w}^{T} \\mathbf{x}_{n}+b)=1minn=1,…,N​yn​(wTxn​+b)=1，此时优化目标可以表示为margin⁡(b,w)=1∥w∥{\\operatorname{margin}(b,\\mathbf{w})}=\\frac{1}{\\|\\mathbf{w}\\|}margin(b,w)=∥w∥1​，优化问题可以转化如下： max⁡b,w1∥w∥ subject to every yn(wTxn+b)&gt;0min⁡n=1,…,Nyn(wTxn+b)=1\\begin{array}{cl} {\\max _{b, \\mathbf{w}}} &amp; {\\frac{1}{\\|\\mathbf{w}\\|}} \\\\ {\\text { subject to }} &amp; {\\text { every } y_{n}\\left(\\mathrm{w}^{T} \\mathrm{x}_{n}+b\\right)&gt;0} \\\\ {} &amp; {\\min _{n=1, \\ldots, N} y_{n}\\left(\\mathbf{w}^{T} \\mathrm{x}_{n}+b\\right)=1} \\end{array} maxb,w​ subject to ​∥w∥1​ every yn​(wTxn​+b)&gt;0minn=1,…,N​yn​(wTxn​+b)=1​ 进一步将问题转化为： min⁡b,w12wTw subject to yn(wTxn+b)≥1for all n\\begin{array}{cl} {\\min _{b, \\mathbf{w}}} &amp; {\\frac{1}{2}\\mathbf{w}^{T}\\mathbf{w}} \\\\ {\\text { subject to }} &amp; { y_{n}\\left(\\mathrm{w}^{T} \\mathrm{x}_{n}+b\\right)\\geq 1} for\\ all\\ n \\end{array} minb,w​ subject to ​21​wTwyn​(wTxn​+b)≥1for all n​ 上述表达为二次规划标准型，二次规划的求解已经是一个比较成熟的领域，在很多软件中都有求解该问题的函数，按照对应的格式将数据传入相应函数即可求出SVM的解。","permalink":"http://yangtf983.github.io/2020/01/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B02%EF%BC%9A%E4%BB%8EPLA%E5%88%B0SVM/","photos":[]},{"tags":[{"name":"机器学习基石听课笔记","slug":"机器学习基石听课笔记","permalink":"http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E5%90%AC%E8%AF%BE%E7%AC%94%E8%AE%B0/"}],"title":"机器学习基石笔记1：机器学习简介","date":"2020/01/23","text":"【0】 说明 机器学习基石是一门很好的机器学习入门课，能够让人在短时间内掌握大量的机器学习的基础理论和原理。从这篇文章开始，我会用大约十几篇文章的内容将机器学习基石我听机器学习基石这门课时候的所学和思考表达出来，我的笔记中会包含大部分基石课程中的内容，并且不会略去任何课程中的证明。对于课程中证明讲的不清楚的一些地方，我还会附上我的理解。关于这门课程的课件可以从课程主页上下载。 【1】机器学习概念 人类的学习行为可以看作是从观察和实践中学习获得技能，类似地，可以定义机器学习为机器从数据中学习获得技能。 【2】应用领域 人类无法手动编程的系统——探索火星的系统； 人类难以轻易写出程序规则的系统——语音/图像识别； 人类无法做到的快速决策系统——高速股票交易系统； 个性化服务系统——广告精准投放。 【3】机器学习三要素 一般认为，一个系统要想应用机器学习方法，需要具备三个条件： 存在一个可以被学习的潜在模式； 不知道如何定义规则并写入程序； 可以获得大量数据。 这三个要素缺一不可：首先，被学习的问题中必须有一个可以被学习的模式，否则机器学习算法永远不可能学到有用的内容，例如用机器学习算法预测一个公平骰子下一次掷出几点；其次，这个规则应当是不知道如何定义，或至少不知道如何准确定义的，否则直接用程序写出规则更加简单和节省算力；最后，要有机器学习的来源，也就是大量的数据，后面我们将会提到，机器学习的可信度与数据量密不可分，而且随着模型变得复杂，达到同样的可信度需要的数据量会剧增。 【4】例子 预测婴儿在某一时刻会不会哭（没有规则，不适合机器学习）； 判断一个 已给的图像中 是否包含一个圆（可以写出规则，不适合机器学习）； 决定是发否应该发给某人信用卡（满足三个条件，适合机器学习）； 预测接下来的十年地球是否会被核武器毁灭（没有充足的数据，不适合机器学习）； 【5】符号化表示 以上述例3为例，将机器学习中的要素进行符号化表示如下： 输入：x∈Xx\\in \\mathcal{X}x∈X（刻画消费者特征的相关指标） 输出：y∈Yy\\in \\mathcal{Y}y∈Y（适合/不适合发放信用卡） 未知的模式：f:X→Yf:\\mathcal{X}\\rightarrow \\mathcal{Y}f:X→Y（理想的信用卡发放公式） 数据（训练集）：D={(x1,y1),…,(xN,yN}\\mathcal{D}=\\{ (x_{1},y_{1}),\\ldots,(x_{N},y_{N} \\}D={(x1​,y1​),…,(xN​,yN​}（银行的历史数据） 假设：g:X→Yg:\\mathcal{X}\\rightarrow \\mathcal{Y}g:X→Y（g可以通过对数据进行学习逼近fff） 模型的假设：g∈Hg\\in \\mathcal{H}g∈H，ggg是假设H\\mathcal{H}H中表现最好的那个； 学习流程：fff（未知）→ {(xn,yn)}\\rightarrow \\ \\{(x_{n},y_{n})\\}→ {(xn​,yn​)}（来自fff的训练集）$\\rightarrow\\ $算法 → g≈f\\rightarrow\\ g\\approx f→ g≈f 【6】机器学习 vs 数据挖掘 vs 人工智能 vs 统计学 机器学习 数据挖掘 使用数据得到一个逼近目标f的近似模式g 从大量数据中寻找感兴趣的特性 当感兴趣的性质就是寻找估计的目标函数时，二者一致 当感兴趣的性质与寻找估计的目标函数时有关时，两者的方法可以相互帮助 机器学习 人工智能 使用数据得到一个逼近目标f的近似模式g 使机器能够做一些智能行为 得到近似于f的g的过程就是一个显示智能的过程，因此，机器学习是实现人工智能的一种方法 机器学习 统计学 使用数据得到一个逼近目标f的近似模式g 使用数据推断未知的参数或目标 更重视如何通过计算机得到结果 在一定的假设下通过数学证明得到一些可信的结果，很少关注计算的实现 当g是需要推断的结果，f是未知的目标时，统计学与机器学习的目标和方法是一致的，因此统计学的很多方法和工具可以用于机器学习。","permalink":"http://yangtf983.github.io/2020/01/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B01%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/","photos":[]},{"tags":[{"name":"Semi-supervised Learning","slug":"Semi-supervised-Learning","permalink":"http://yangtf983.github.io/tags/Semi-supervised-Learning/"}],"title":"论文翻译：半监督学习介绍","date":"2020/01/09","text":"0 说明 本文是一篇关于半监督学习介绍性论文集的第一篇的翻译，大部分是人工翻译，少部分是机器翻译加人工修改。不过限于知识有限，其中很多内容并不了解，所以一些地方翻译不到位，仅供参考。 论文集电子版链接onedrive 1 监督学习、无监督学习与半监督学习 为了理解半监督学习的本质，有必要首先了解一下监督学习和无监督学习。 1.1 监督学习与无监督学习 一般来说，传统的机器学习有两种不同类型，即无监督学习（Unsupervised Learning ）和监督学习（Supervised Learning）。 首先看一下无监督学习。假设存在一个包含n个数据点的集合X=(x1,…,xn)X=(x_{1},\\ldots,x_{n})X=(x1​,…,xn​)，其中xi∈X,i∈[n]:={1,…,n}x_{i}\\in\\mathcal{X},i\\in[n]:=\\{1,\\ldots,n\\}xi​∈X,i∈[n]:={1,…,n}.通常这些数据点被认为是独立同分布于分布X\\mathcal{X}X，常将其方便地定义为n∗dn*dn∗d维矩阵X=(xiT)i∈[n]TX=(x_{i}^{T})^{T}_{i\\in[n]}X=(xiT​)i∈[n]T​，该矩阵的每一行也就是一个数据点，无监督学习的目标就是从这样的数据XXX中找到一个有趣的结构。通常可以认为无监督学习的基本的问题是找到一个产生数据XXX的概率密度。不过无监督学习也有一些相对更弱的形式，例如分位数估算（quantile estimation），聚类（clustering），离群值检测（outlier detection）和降维（dimensionality reduction）。 接下来看一下监督学习。它的目标是从一个由成对数据(xi,yi)(x_{i},y_{i})(xi​,yi​)组成的数据集中学习一个从xxx到yyy的映射。其中yi∈Yy_{i}\\in\\mathcal{Y}yi​∈Y被称作数据xxx的标签或目标值。如果标签也是数字，就可以用y=(yi)i∈[n]Ty=(y_{i})^{T}_{i\\in[n]}y=(yi​)i∈[n]T​表示由标签组成的列向量。与无监督学习类似，一个基本的要求是数据(xi,yi(x_{i},y_{i}(xi​,yi​独立同分布取样于X×Y\\mathcal{X}\\times\\mathcal{Y}X×Y.由于得到的映射很容易通过其在测试集上的表现进行评估，所以监督学习的任务可以被很好的定义。当Y=R\\mathcal{Y}=\\mathbb{R}Y=R或Y=Rd\\mathcal{Y}=\\mathbb{R}^{d}Y=Rd时（也可以直接说当标签值的取值范围是连续的数字时），这个任务又被叫做回归。本书的大部分内容都是关于分类的，即Y\\mathcal{Y}Y的值来源于一个有限集，或者说标签是分离的。书中也有一些关于回归问题的探讨，在第23章。监督学习有两类算法：生成式算法（Generative algorithms）和判别式算法（Discriminative algorithm）。生成式算法试图用一些无监督学习过程构造条件密度模型p(x∣y)p(x|y)p(x∣y)（这里我们假定所有的分布都是有概率密度函数的，这样可以简化问题，我们将自己限定在处理概率密度的问题上），通过这个概率密度，再结合贝叶斯公式p(y∣x)=p(x∣y)p(y)∫Yp(x∣y)p(y)dyp(y|x)=\\frac{p(x|y)p(y)}{\\int_{\\mathcal{Y}}p(x|y)p(y)dy}p(y∣x)=∫Y​p(x∣y)p(y)dyp(x∣y)p(y)​就可以预测新的数据的标签值。事实上生成式算法中的p(x∣y)p(y)=p(x,y)p(x|y)p(y)=p(x,y)p(x∣y)p(y)=p(x,y)就是生成成对数据(xi,yi)(x_{i},y_{i})(xi​,yi​)的联合概率密度。而判别式算法则并不试图估计产生数据xix_{i}xi​的概率密度，它仅仅关注如何估计p(y∣x)p(y|x)p(y∣x)。一些判别式算法的目的甚至限制在仅仅关注p(y∣x)p(y|x)p(y∣x)是否大于0.5，例如支持向量机算法（SVM）。通常认为由于判别式算法与监督学习的目标之间的关联更加直接，在实际问题中判别式算法往往更加有效。这两种框架的更多细节会在2.2.1和2.2.2部分进一步讨论。 1.2 半监督学习 半监督学习（Semi-Supervised Learning），简记为SSL，是一种介于监督学习与无监督学习之间的机器学习类型。其特点是数据以无标签为主，再加上一部分的有标签信息的数据，通常所说的标签信息就是这部分数据的标签。此时，数据集x=(xi)i∈[n]x=(x_{i})_{i\\in[n]}x=(xi​)i∈[n]​可以被分成两部分，一部分是标签Yl:=(y1,…,yn)Y_{l}:=(y_{1},\\ldots,y_{n})Yl​:=(y1​,…,yn​)已知的点Xl:=(x1,…,xl)X_{l}:=(x_{1},\\ldots,x_{l})Xl​:=(x1​,…,xl​)，另一部分是标签未知的点Xu:=(xl+1,…,xl+u)X_{u}:=(x_{l+1},\\ldots,x_{l+u})Xu​:=(xl+1​,…,xl+u​).这种情况就是这本书所研究的“标准的”半监督学习的形式，书中大部分章节研究的问题都是这种形式。 其他形式的部分监督也是可能的。例如，我们已知的标签信息是某些点有（或者没有）相同的标签（参照Abu-Mostafa, 1995）.这种更广的设定会在第五章中被讨论。不同的设定将会导致对半监督学习产生不同的观点：在第五章中，SSL被看作在限制条件下的无监督学习。与之相反的是，大部分方法都把SSL看作是知道了额外的关于xxx的信息的有监督学习。后一种解释与SSL的大多数应用相一致，因为大部分时候SSL的目标与监督学习的目标相一致，都是预测所给数据xix_{i}xi​的目标值。然而，当类别的数量和本质不能提前知道，并且需要从数据中推断出这些的话，后一种观点就不适用了。此时，将SSL看作在限制条件下的无监督学习的观点则是适用的。 几十年前，Vapnik就提出了一个与SSL有关的问题，这就是所谓的转导学习（transductive learning）。在转导学习的设定下，实验者拥有一个有标签的训练集和一个无标签的测试集，其特点是预测的结果仅仅作用在测试集上。相对应的是归纳学习（inductive learning），其目的是得到一个定义在整个空间X\\mathcal{X}X上的预测函数，可以用这个函数预测整个空间上所有的点的标签值。这本书中的许多方法都是转导式的，尤其是基于图形类数据进行推断时，转导式方法更加自然，转导学习将在1.2.4部分被进一步解释。 1.3 半监督学习的历史简介 最早将无标签数据用于分类问题的方法很可能使自学习（self-learning），这种方法又被称为自训练（self-training）、自标签（self-labeling）或决策导向或学习（decision-directed learning）。这种方法从在有标签数据上训练开始，每一步都会用当前的决策函数（decision function）预测一部分无标签点的标签，然后把这部分被标注的点纳入训练集，重新应用有监督学习方法训练模型。这种方法很早就出现在一些文献中了（如：Scudder (1965); Fralick (1967); Agrawala (1970)）。 这种算法令人不满意的一个方面是包装（wrapper）效果依赖于所使用的监督学习方法。如果用实际风险最小化与1-0损失来做自学习，那原来的无标签数据加上后对新训练的模型没有任何影响。而如果用余量最大法（margin maximizing method）来做自学习，其结果是决策边界将被推离无标签点（参见第6章）。在其他情况下似乎还不清楚自学习方法真正的作用情况以及对应于哪种假设。 与 半监督学习关系密切的是转导推理（transductive inference或transduction），这种方法的先行者是Vapnik(Vapnik and Chervonenkis, 1974;Vapnik and Sterin, 1977).与归纳推理（inductive inference）相反，转导推理不追求推断出一般的决策规则，而是仅专注于预测无标签点（测试集），不做外推。一个早期的转导推理的例子在1968年就已经由Hartley和Rao提出来了，虽然当时还没有明确定义这个概念。他们建议对测试集的标签进行组合优化以最大化其模型的可能性。 半监督学习真正意义上的起飞似乎是从1970年代人们开始用无标签数据估计费舍尔线性判别式（Fisher linear discriminant）规则开始（Hosmer, 1973; McLachlan, 1977; O’Neill, 1978; McLachlan and Ganesalingam, 1982）.更确切地说，这种情况是指每一类的条件密度都是满足有着相同协方差矩阵的高斯条件的，这种模型可以通过诸如EM算法等迭代算法进行优化，同时使用有标签数据和无标签数据，求出数据标签的最大可能性。与混合高斯情况不同，Cooper和Freeman已经研究过了同时使用有标签数据和无标签数据估计多项分布的情况。 后来，每类一种成分的设定已经被推广到了每类几种成分（Shahshahani and Landgrebe, 1994），而后又被Miller和Uyar进一步推广（1997）。 在PAC（probably approximately correct）框架下，二元高斯混合情况下的学习率已经在1995由Ratsaby和Venkatesh得到。在混合情况可识别的情况下，Castelli和Cover证明，在无标签点的数量是有限个的情况下，错误率呈指数级收敛到贝叶斯风险。可识别的意思是说，在给定P(x)P(x)P(x)的情况下，其分解式ΣyP(y)P(x∣y)\\Sigma_{y}P(y)P(x|y)Σy​P(y)P(x∣y)是唯一的。这似乎是一个相对较强的假设，但在高斯混合条件下是满足的。Castelli和Cover在1996年发表的一篇论文中对此进行了分析，该论文中类别的条件密度是已知的而类别的先验密度是未知的。 之后，研究半监督学习的兴趣在1990年代增加了，其中大部分原因要归因于自然语言处理与文本分类问题的需要 (Yarowsky, 1995; Nigam et al., 1998; Blum and Mitchell, 1998; Collins and Singer, 1999; Joachims, 1999). 注意，就我们所知，Mert等人于1992年第一次使用术语“半监督学习”来描述同时使用有标签数据和无标签数据进行分类的问题。不过，事实上这种问题在之前就已经有研究了，但是研究的情况和本书中所要讲的有所不同，一个例子是(Board and Pitt, 1989). 2 半监督学习何时发挥作用 一个很自然的问题产生了：半监督学习有意义吗？更确切地说，与仅仅使用有标签数据进行学习的情况相比，考虑到无标签数据真的能够产生更加精确的预测吗？你可能已经从你手头这本书的厚度猜到了，答案是yes!不过有一个重要的先决条件：无标签数据与分类问题相关，它包含能够阐明数据分布的信息。 用更数学化的表达可以这么说：从无标签数据中获得的关于p(x)p(x)p(x)的信息中 必须包含有用于推断p(y∣x)p(y|x)p(y∣x)的信息。如果不满足这个先决条件，半监督学习无法产生比监督学习更好地结果，强行加入无标签数据进行推断甚至可能产生误导，降低预测的精确程度。这个效应的细节将在第4章深入研究。 只有在假设成立的情况下半监督学习才会有效，因此不必对半监督学习的作用过于惊讶。而且纯监督学习也必须满足某些假设才行。之后将会给出几种假设，事实上第22章会讨论一种在PAC风格的框架下形式化这些假设的方法，其中一种受欢迎的假设可以被形式化如下： 监督学习的平滑性假设（Smoothnss assumption of supervise learningSmoothnss\\ assumption\\ of\\ supervise\\ learningSmoothnss assumption of supervise learning）:如果两个点x1,x2x_{1},x_{2}x1​,x2​距离很近，那么其对应的输出y1,y2y_{1},y_{2}y1​,y2​也应当相应很近。 严格说来，这个假设仅仅指的是连续性而不是光滑性，不过smoothesssmoothesssmoothess这个术语却常常被使用在这里，或许是由于实际问题的回归中yyy常常被当作xxx的光滑函数建立模型。 说的更清楚一点，一旦不满足这个假设，就永远不可能讲一个在有限训练集上训练出来的模型一般化到一个测试集可能是无限情况下。 2.1 半监督学习平滑性假设 我们现在要提出一个对半监督学习有用的平滑性假设的一般化形式，我们称其为“半监督学习平滑性假设（semi-supervised smoothness assumption）”。在监督情况下，在我们先前的假设下，输出会随着距离而平滑变化，现在我们考虑加入输入的密度。这个假设是说预测函数在高密度区域将会比在低密度区域更加平滑： 半监督学习平滑性假设（semi−supervised smoothness assumptionsemi-supervised \\ smoothness\\ assumptionsemi−supervised smoothness assumption）：如果一个高密度区域中的两个点x1,x2x_{1},x_{2}x1​,x2​距离很近，那么其对应的输出y1,y2y_{1},y_{2}y1​,y2​也应当相应很近。 注意，根据传递性，这个假设隐含了一个信息：如果两个点可以通过一个高密度区域中的路径被连接，那么它们的输出值也可能很近。另一方面，如果它们被一个 低维区域隔开，那么它们的输出也就不必较近。 半监督学习平滑性假设同时适用于回归问题与分类问题。在下一部分，我们将展示在分类问题中将它简化为半监督问题常用的假设。目前尚不清楚该假设对于回归问题有多有用。作为替代方案，第23章提出了一种使用无标签数据进行模型选择的方法，该方法同时适用于回归和分类。 2.2 聚类假设 假设我们一直每一类别的点都倾向于聚成一团，那么无标签数据将会有助于精确化分类边界，做法是：可以运行一个聚类算法并用有标签数据给每个聚类指定类别（见第二章）。这个问题的基本的，现在也是经典的假设可以被描述如下： 聚类假设（Cluster assumptionCluster\\ assumptionCluster assumption）：在相同聚类中的点可能属于同一类。 这个假设在类别之间是陡峭的情况下可能是合理的，也就是说：如果分类点一个对象密集的连续体，那么就无法从中辨别出不同的类别。 聚类假设并不是说每一类数据形成一个单一的密集的区域，它仅仅意味着：在通常情况下，我们不会看到两中不同类别的点聚在同一类。 聚类假设可以被看作是上面提出的半监督学习平滑性假设的一个特例，因为一个类通常被定义为一个包含一堆互相之间可以由一条穿过高密度区域的短线连接起来的点组成的点集。 聚类假设的等价的规范化定义如下： 低密度分割（Low density separationLow\\ density\\ separationLow density separation）：决策边界应当落在低密度区域中。 很容易看出来二者的等价性：高密度区域中的决策边界将会把一个聚类切分成两类；同一聚类中的许多不同类别的点将需要决策边界来切割聚类，即穿过高密度区域。 尽管两种形式在概念上是等价的，但是却能产生出不同的算法，我们将在1.3节讨论相关问题。低密度版本还提供了直觉上的理解，为什么这种假设在许多实际问题中都是明智的。 例如，考虑数字识别，并假设人们想学习如何区分手写数字“ 0”与数字“ 1”。 准确地从决策边界获取的采样点将在0到1之间，最有可能是一个看起来像很长的零的数字。 但是有人写下这个“怪异”数字的可能性很小。 2.3 流形假设 流形假设是一个与前述假设不同但是相关，而且形成了几种半监督学习方法的基础的假设，定义如下： 流形假设（Manifold assumptionManifold\\ assumptionManifold assumption）：高维数据（大致）存在于低维流形上。 这个假设怎么发挥作用？一个广为认知的统计方法和学习算法的问题是所谓的维度诅咒（curse of dimensions）（参见11.6.2部分），维度诅咒基于一个事实：物体的体积随着其维度增加呈指数级增长。与之相应的是统计任务需要指数级增长的数据量来保证估计的可靠性。这个问题直接影响到基于输入空间的密度故居的生成式算法。对于判别式算法可能更严重的一个相关的问题是：随着维度的上升，成对的数据看起来更加相似，传递出更少的信息。 不过，如果数据恰好位于低维流形上，学习算法基本上就可以在相应的低维度空间上空间上操作，从而避免维度诅咒。 如上所述，可以认为使用流形的算法可能被视为近似实现了半监督平滑度假设：此类算法使用流形的度量来计算测地距离。 如果我们将流形视为高密度区域的近似值，那么很明显，在这种情况下，半监督平滑度假设降低为应用于流形的监督学习的标准平滑度假设。 请注意，如果流形以弯曲的方式嵌入到高维输入空间中（即，它不仅是子空间），则测地线距离将与输入空间中的测地线距离不同。 通过确保更准确的密度估计和更合适的距离，流形假设对于分类以及回归分析都可能有用。 2.4 转导 如前所述，某些算法本质上是在转导设定（transductive setting）下运行的。 根据Vapnik提出的哲学，高维估计问题应尝试遵循以下原则： VapnikVapnikVapnik 原则（Vapnik′s principleVapnik&#x27;s\\ principleVapnik′s principle）：当解决某个问题的时候，应避免将解决一个更加困难的问题作为中间的一个步骤。 以监督学习为例，其中需要对与某些点x相对应的标签y进行预测。生成式模型估计x的密度作为中间步骤，而判别式模型则直接估计标签。 同样地，如果仅对给定的测试集需要标签预测，则可以认为转导比归纳更直接：归纳方法在整个空间X\\mathcal{X}X上推导函数f:X→Yf:\\mathcal{X}\\rightarrow \\mathcal{Y}f:X→Y，然后在测试点上估计f(xi)f(x_{i})f(xi​)并返回，转导方法则直接估计有限的测试标签集，即仅在测试集上定义的函数f:Xu→Yf:\\mathcal{X}_{u}\\rightarrow \\mathcal{Y}f:Xu​→Y。 请注意，转导（如本书中所定义）并不等同于SSL：一些半监督算法是转导性的，而另一些是归纳性的。 现在假定给定了一种转导算法，该算法产生的解决方案优于对相同标记数据（丢弃未标记数据）训练的归纳算法，则此时的性能差异可能是由于以下两点之一（或其组合）引起的： 转导式方法比归纳式方法更加契合Vapnik原则； 转导式算法用一种类似于半监督学习算法的方式利用了无标签数据。 目前有充分的证据表明第二点对模型提升具有影响，不过尚不清楚有选择地支持第一点的经验结果。特别是，对本书相关基准的评估（第21章）似乎并未暗示转导方法的系统优势。 转导的性质仍然是争论的话题，第25章试图提出不同的意见。 3 算法类别与本书的架构 尽管许多方法不能明确地说来源于以上哪种假设，但大多数算法可以被看作对应或产生于上述假设之一或之几。我们尝试将本书中介绍的半监督学习方法组织为四个大致与基本假设相对应的类。 尽管分类并不总是唯一的，但我们希望这样的组织能够提供一个指导体系，使读者可以更轻松地阅读本书内容。出于同样的原因，本书按“部分”进行组织。每一类SSL算法都有一部分，此外还有一部分侧重于生成式方法。 另外两个部分专门介绍SSL的应用和前景。 在下文中，我们简要介绍每本书各部分所涵盖的内容。 3.1 生成式模型 第一部分展示了半监督学习中生成式模型的历史和前沿，第二章就从这个领域的概览开始谈起。用生成式模型估计条件密度p(x∣y)p(x|y)p(x∣y)， 使用生成模型进行推断涉及条件密度p(x∣y)p(x|y)p(x∣y)的估算。在这样的设定下，任何关于p(x)p(x)p(x)的额外信息都是有用的。一个简单的例子是假定p(x∣y)p(x|y)p(x∣y)是高斯分布的，这样就可以用EM找到每一种类别对应的高斯分布的参数的值。与聚类中使用的标准的标准的高斯分布的唯一不同之处在于与任何带标签的示例关联的“隐藏变量”实际上并未被隐藏，而是已知并且等于其类别标签。它实现了聚类假设（参见第2.2.1节），因为给定的聚类仅属于一个类。 这个小例子强调了了生成模型对半监督学习的不同解释： 在具有边缘密度的基础上添加了一些额外信息的分类问题； 具有额外信息的聚类问题。在标准设定下，这些额外信息就是一个数据子集的标签，但是它也可以采用更一般的约束形式。这是第5章的主题。 生成式方法的一个优点在于：关于问题或数据结构的知识可以通过建模自然地整合进模型中。在第3章中，将EM算法应用于文本数据的应用将展示这一优势。可以观察到，当建模假设不正确时，未标记的数据会降低预测准确性。在第4章中将对这种效果进行深入研究。在统计学习中，在进行推理之前，人们会选择一类函数或先验函数。必须根据事先对该问题的了解来选择它。在半监督学习环境中，如果能从数据的结构中推测到一些目标函数的信息，则在看到未标记的数据后可以更精确地选择先验函数 ：这样通常可以提高函数满足聚类假设的先验概率。从理论上讲，这是获取第22章所述的半监督学习边界的自然方法。 3.2 低维分割 第二部分聚焦于试图通过将决策边界推离无标签点而直接应用低维分割假设的算法。 最常用的达到这个目标的方法是用一个实现此目标的最常见方法是使用最大余量算法，例如支持向量机。最大化未标记点和标记点的边距的方法称为转导SVM（TSVM）。不过，相应的问题是非凸的，因此难以优化。 第6章介绍了一种TSVM的优化算法。从仅对标记数据进行训练的SVM解决方案开始，对未标记的点进行SVM预测标记，并对SVM进行所有点的重新训练。在未标记点的权重缓慢增加的同时进行迭代。另一种可能性是在第7章中建议的半定义SDP松弛编程。 然后，提出了TSVM的两种替代方法，分别用概率论和信息理论框架提出。在第8章中，通过引入空类来增加二进制高斯过程分类，该空类占用了两个常规类之间的空间。作为与TSVM相比的优势，这允许概率输出。 第9章介绍的熵最小化同样具有这一优势。它鼓励类条件概率P(x∣y)P(x|y)P(x∣y)在标记和未标记的点接近1或0。作为平滑度假设的结果，在任何高密度区域中，概率将趋于接近0或1，而类别边界对应于中间概率。 使用熵或信息的另一种方法是在第10章中开发的与数据相关的正则化（data-dependent regularization）。与TSVM相比，这似乎更直接地实现了低密度分离：标准平方范数正则化因子乘以一项反映接近决策边界的密度。 3.3 基于图的方法 在过去的两年中，半监督学习最活跃的领域是基于图的方法（graph-based methods），这是本书第三部分的主题。 这些方法的共同点是，数据由图的节点表示，图的边缘用入射节点的成对距离标记（而缺失的边缘则与无限距离相对应）。如果通过最小化连接两个点的所有路径上的总路径距离来计算两个点的距离，则这可以看作是两个点的测地线距离相对于数据点的流形的近似值。 因此，图方法可以看作是基于流形假设的。 大多数图方法通过利用图拉普拉斯算子（Laplacian）来引用图。令g=(V,E)g=(V,E)g=(V,E)是由w:E→Rw:E\\rightarrow \\mathbb{R}w:E→R给出实际边缘权重的图。这里，边缘e的权重w(e)w(e)w(e)表示入射节点的相似度（丢失的边缘对应于零）。现在，图g=(V,E)g=(V,E)g=(V,E)的加权邻接矩阵（weighted adjacency matrix）WWW定义如下： Wij:={w(e) if e=(i,j)∈E0 if e=(i,j)∈E\\mathbf{W}_{i j}:=\\left\\{\\begin{array}{ll} {w(e)} &amp; {\\text { if } e=(i, j) \\in E} \\\\ {0} &amp; {\\text { if } e=(i, j) \\in E} \\end{array}\\right. Wij​:={w(e)0​ if e=(i,j)∈E if e=(i,j)∈E​ 定义由Dii:=ΣjWijD_{ii}:=\\Sigma_{j}W_{ij}Dii​:=Σj​Wij​组成的诊断矩阵DDD被称作g的度矩阵（degree matrix），接下来就可以用不同的方法定义图拉普拉斯算子。两个最著名的图拉普拉斯算子分别是正规化拉普拉斯算子L\\mathcal{L}L和非正规化拉普拉斯算子LLL： L:=I−D−1/2WD−1/2L:=D−W\\begin{array}{l} {\\mathcal{L}:=\\mathbf{I}-\\mathbf{D}^{-1 / 2} \\mathbf{W D}^{-1 / 2}} \\\\ {L:=\\mathbf{D}-\\mathbf{W}} \\end{array} L:=I−D−1/2WD−1/2L:=D−W​ 许多惩罚加权图边缘的非光滑度的图方法可以看作是一个较为通用的算法系列的不同实例，这将在第11章进行阐述。第13章采用了更理论的观点，并将从连续情况的图形的光滑度转移到离散情况。由此，基于数据的图形表示，提出了不同的正则化器。通常，预测由未标记节点的标记组成。因此，这种算法本质上是转导的，即，它仅返回未标记点上决策函数的值，而不返回决策函数本身。但是，最近进行了一些工作，以扩展基于图的方法以产生归纳解，如第12章所述。 在图上的信息传播还可以考虑到未标记的数据，从而改善给定（可能受到严格监督）的分类。第14章介绍了以这种方式使用有向图的概率方法。 通常，图g是通过以某种其他表示形式计算对象的相似性来构造的，例如，使用欧几里得数据点上的核函数。但是有时原始数据已经具有图形形式。示例包括网页的链接模式和蛋白质的相互作用（请参见第20章）。在这种情况下，边缘的方向性可能很重要。 3.4 表示形式的改变 第四部分的主题是本质上不是半监督的算法，这类算法表现为两步学习： 在所有有标签和无标签数据上应用监督学习算法，但是不考虑已有的标签。例如，这可以是表示形式的更改，也可以是新metric或新kernel的构造。 忽略未标记的数据，并使用新的距离、表示形式或kernel执行纯监督学习。 这可以看作是半监督平滑度假设的直接实现，因为以一种方式更改了表示形式，从而可以节省高密度区域中的小距离。 请注意，基于图的方法（第3部分）与本部分中介绍的方法密切相关：根据数据构建图的过程可以看作是表示形式的无监督更改。因此，第四部分的第15章讨论了这种图形的频谱变换（spectral transforms），以构建kernel.频谱方法（spectral methods）也可以用于非线性降维，如第16章所述。此外，在第17章中，研究了从图形派生的度量，例如，从最短路径派生的度量。 3.5 半监督学习的实际应用 每当未标记数据比标记数据多得多时，半监督学习将是最有用的。如果获取数据点很便宜，但是获取标签会花费大量时间，精力或金钱，则很可能会发生这种情况。在机器学习的许多应用领域中就是这种情况，例如： 在语音识别中，录制大量语音几乎不需要花什么钱，但是标记它需要一些人工来听并键入笔录。 数十亿个网页可直接用于自动化处理，但是要可靠地对其进行分类，人们必须逐一浏览它们。 如今，蛋白质序列是以工业速度获得的（通过基因组测序，计算基因发现和自动翻译），但是要解析三维（3D）结构或确定单个蛋白质的功能可能需要多年的科学工作。 第3章在生成模型的背景下介绍了网页分类。 由于未标记的数据携带的信息少于标记的数据，因此需要大量使用它们才能显着提高预测准确性。这意味着需要快速有效的SSL算法。 第18章和第19章介绍了处理具有大量数据的问题的两种方法。在第18章中，开发了用于加快在第11章中介绍的标签传播方法的方法。在第19章中，显示了cluster kernel是一种有效的SSL方法。 第19章还介绍了半监督学习在重要的生物信息学应用中的两种方法中的第一种：蛋白质序列的分类。尽管这里的预测是基于蛋白质序列本身的，但第20章将进行一些更为复杂的设置：这里的信息假定以表征蛋白质相互作用的图表形式出现。存在几个这样的图，必须以适当的方式进行组合。 本书的结尾部分有一个非常实用的章节：与本书相关的基准的介绍和评估（第21章）。旨在给从业者一些提示，说明如何根据问题的性质选择合适的方法。 3.6 概览 本书的最后一部分，第六部分，致力于解释SSL中一些目前正在被研究的最有趣的方向。 到现在为止，这本书基本上只限于分类。 第23章介绍了另一种适用于分类和回归的SSL方法，并从中推导了算法。有趣的是，这似乎不需要第1章中提出的假设。 此外，本书主要介绍了SSL算法。 尽管上面讨论的假设提供了一些有关SSL何时以及为什么起作用的直觉，并且第4章研究了何时以及为什么SSL可能失败，但是对SSL有了透彻的理论理解显然会更令人满意。为此第22章提供了一个PAC框架，该框架产生SSL问题的错误边界。 在第24章中，依据VC bound以及其他理论和哲学概念对归纳式半监督学习和转导式学习进行了比较。 本书以三位机器学习研究人员之间关于半监督学习与转导学习之间的关系（以及两者之间的差异）的假设讨论（第25章）作为结尾。","permalink":"http://yangtf983.github.io/2020/01/09/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%EF%BC%9A%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D/","photos":[]},{"tags":[{"name":"Reinforcement Learning","slug":"Reinforcement-Learning","permalink":"http://yangtf983.github.io/tags/Reinforcement-Learning/"}],"title":"Imitation Learning & Dagger Algorithm","date":"2020/01/02","text":"[0] 说明 进入Reinforcement Learning的世界，一般而言应当是从tabular method学起，但是从imitation learning学起推广到Reinforcrmrnt Learning可以帮助我们更好的理解RL的解决问题的思路和发展目的，因此我们本次继续跟随cs294的脚步，学习Imitation learning的相关知识以及相对简单的Dagger算法。 值得一提的是，IL应当归类于监督学习(Supervised Learning)，但是其可以应用于IRL(Inverse Reinforcement Learning)领域，这也是IL与IRL的重要联系之一。 [1] Terminology &amp; Notation 首先我们应当明白我们探讨的是什么类型的问题，我们称之为时序决策问题(sequrntial decision)，这类问题的主要特点是具有马尔科夫性(markov property)，也即是说，在当前条件已知的情况下，未来和过去是独立的。换句话说，如果我掌握了现在的情况，那么从纯理性的角度讲，是否掌握过去的情况对我做出下一步决策毫无影响。其数学定义参照下式： P(X(tn)≤xn∣X(tn−1)=xn−1,…,X(t1)=x1)=P(X(tn)≤xn∣X(tn−1)=xn−1)P( X(t_{n} )\\leq x_{n} | X(t_{n-1})=x_{n-1}, \\ldots,X( t_{1} ) = x_{1} ) = P( X(t_{n} )\\leq x_{n} | X(t_{n-1})=x_{n-1}) P(X(tn​)≤xn​∣X(tn−1​)=xn−1​,…,X(t1​)=x1​)=P(X(tn​)≤xn​∣X(tn−1​)=xn−1​) 其中，t1&lt;t2&lt;…&lt;tnt_{1}&lt;t_{2}&lt;\\ldots &lt;t_{n}t1​&lt;t2​&lt;…&lt;tn​. 对于这样一个决策问题，系统首先需要一个输入，也即是系统的观测值，记为oto_{t}ot​。众所周知，神经网络会对oto_{t}ot​进行一系列处理，得到一个可能采取的行为的分布函数（对于有限行为决策，即为分布列），我们称这个结果称为我们的决策策略。我们将行为记作ata_{t}at​，策略记作πθ(at∣ot)\\pi_{\\theta}(a_{t}|o_{t})πθ​(at​∣ot​). 上面都很好理解，但是我们这里还要定义另外一个符号，这个符号叫做状态，记为sts_{t}st​. 定义sts_{t}st​是因为oto_{t}ot​并不总是无损的。举例来说，假如我有一张图片，图片上是一辆行驶的汽车，我们将这张图片输入给一个神经网络系统，oto_{t}ot​就是这张照片的像素，是一个或几个二维数组，我们可以通过神经网络对这张图片进行分析，得到这张图片上各种信息，但是我们永远不能判断车后面有没有一只动物，有可能这辆车刚刚好挡住了一只动物，但是这只动物不在照片里，或者说不在oto_{t}ot​里，但是我们说它在sts_{t}st​里，sts_{t}st​包含此刻决策相关的所有信息，但是oto_{t}ot​不一定。 我们再通过下图来理解这个决策系统： 我们将ot=sto_{t}=s_{t}ot​=st​的系统称为完全观察(fully observed)系统，将ot≠sto_{t} \\neq s{t}ot​​=st的系统称为部分观察(partial observed)系统。显然，对于一个partial observed问题，若我们只有oto_{t}ot​而没有sts_{t}st​，ot−1o_{t-1}ot−1​对我们做出下一步行动是有用的，因为它可能帮助我们发现车后究竟有没有动物。此时，我们需要一个决策系统可以不仅仅考虑到oto_{t}ot​，还可以考虑到ot−1o_{t-1}ot−1​甚至于更早的观测值。 [2] Introduce to Behavior Cloning 我们将模仿专家行为进行学习的一类算法又称为behavior cloning，这类算法以专家行为作为数据标签，通过对数据集进行监督学习得到模型πθ(at∣ot)\\pi_{\\theta}( a_{t}|o_{t} )πθ​(at​∣ot​)，以此进行决策。 我们以自动驾驶为例，behavior cloning的一个可能做法是找一些驾驶熟练的人类司机，让他们驾车行驶，并且通过摄像头拍摄沿路情况，并记录下司机的行为(左转或者右转)作为数据标签，接着用这些有标签的数据进行监督学习，因此本质上IL/behavior cloning是监督学习的分支，下图是我们所举例子的一个直观表示： [3] DAgger Algorithm DAgger算法的诞生是为了解决behavior cloning中一个很严重的问题，因此我们先来看看这个问题。 前面已经讲过，我们在使用behavior cloning时采用了监督学习算法。我们都知道，无论数据量有多大，采用的监督学习算法有多准确，都可能存在一种情况，就是在行驶过程中某一步和原来的专家行为产生偏差，因为我们不是完全的模仿专家行为，而是采用了一个监督学习模型(如果完全模仿专家行为，则模型的利用场景过于有限，一旦碰到任何没有碰到的情况，汽车就会手足无措，而在行驶过程中，可能发生的各种情况是难以模拟遍的)。一旦这个偏差产生，由于它不是在数据集中的，做出的进一步决策很可能使其朝着进一步偏差的方向进行，而随着汽车碰到的状况与数据集的偏差越来越大，其做出的决策则越来越不具有合理性，偏离原数据集的速度会越来越快，放在自动驾驶上，就是偏离车道的速度加快。这一现象在很多实验中得到了验证，下图是一张验证这一现象的模拟图： 针对之前提到的自动驾驶，NVIDIV提出了一种解决方案，就是同时拍摄左中右三个方向的情况，给左侧的数据打上标签“右”，右侧的数据打上标签“左”,这样当检测到车头方向偏离时，及时调整回去。经过实际检验，这种方案的效果很不错。 这个问题看似已经解决了，但是我们还是要思考一下，有没有更加通用的方法，使得我们不用重新获取这些数据，并且可以用于一些其他领域的behavior cloning问题？ 还是用自动驾驶举例，想像一下，如果我们能够得到整个轨迹的分布，也就是所有的汽车可能碰到的情况，然后让专家给出所有的label，这样无论汽车遇到什么情况，都可以精确按照专家给出的建议行驶。但是这种假设的问题在于，我们无法得到一个有无数种可能的情况的分布，即便是我们找出了所有情况，也无法承担让专家给所有情况打分的高额花费。 那我们能不能找到一种方法，来只对个数不多的有限种情况进行打分，并尽量让汽车在这些情况能够给出有效指导的范围内行动呢？ 我们将原始数据集记为data，我们知道，我们的模型能够应对的情况是Pdata(ot)P_{data}(o_{t})Pdata​(ot​)，而汽车在行驶过程中真正面对的情况是Pπθ(ot)P_{\\pi_{\\theta}}(o_{t})Pπθ​​(ot​) ，当data ≠πθ\\neq \\pi_{\\theta}​=πθ​ 时，自然有 Pdata(ot)≠Pπθ(ot)P_{data}(o_{t})\\neq P_{\\pi_{\\theta}}(o_{t})Pdata​(ot​)​=Pπθ​​(ot​)，因此产生了偏离。如果我们能找到一种方法使得Pdata(ot)≠Pπθ(ot)P_{data}(o_{t})\\neq P_{\\pi_{\\theta}}(o_{t})Pdata​(ot​)​=Pπθ​​(ot​)，便能解决这个问题。因此我们的一个思路是尽可能让data=πθdata=\\pi_{\\theta}data=πθ​. 依照这种思路，我们找到了DAgger(Dataset Aggregation)算法。DAgger是一种很简单的方法，并且已经被证明在在线学习的情况下这种算法是收敛的。下面介绍一下DAgger算法的流程： train πθ(at∣ot)\\pi_{\\theta}(a_{t}|o_{t})πθ​(at​∣ot​) from human data D= {o1,a1,…,oN,aN}\\{o_{1},a_{1},\\ldots,o_{N},a_{N}\\}{o1​,a1​,…,oN​,aN​}. run πθ(at∣ot)\\pi_{\\theta}(a_{t}|o_{t})πθ​(at​∣ot​) to get dataset Dπ={o1,…,oM}D_{\\pi}=\\{o_{1},\\ldots,o_{M}\\}Dπ​={o1​,…,oM​}. Ask human to label DπD_{\\pi}Dπ​ with actions ata_{t}at​. Aggregate: D→D⋃Dπ.D\\rightarrow D\\bigcup D_{\\pi}.D→D⋃Dπ​. repeat step1 ~ step4. 这里有一个问题，是cs294中的一个学生提出来的：为什么step4要将D和DπD_{\\pi}Dπ​聚合，而不是用DπD_{\\pi}Dπ​代替D？ 视频中老师给出了两个原因：(1)这样做效果不好;(2)DAgger算法收敛的基础是在线学习。 但是视频中没有给出更详细的解释，我理解了一下，可以大概给出一种解释方法： 首先，behavior cloning使用的是监督学习方法，DAggger算法的目的是尽可能让车的行驶路径在我们的模型的无偏差的计算范围内，即便是偏差了也要尽可能是我们考虑过的偏差情况。那么如果我们不聚合D和DπD_{\\pi}Dπ​，我们也能仅仅用DπD_{\\pi}Dπ​去训练一个新模型，因为这样同样是部分的，与用D训练一个新模型并没有本质差别，因此如果我们每一步仅仅用新的DπD_{\\pi}Dπ​训练模型，最后必然要将所有循环中得到的数据进行一次聚合再进行训练得到使用的模型，这样训练的模型收敛的可能显然不如每一次循环将所有已有的数据聚合起来进行训练的收敛的可能大（直观理解），而后者已经被证明是收敛的。 其次，我们也可以考虑在不聚合数据的情况下每次不去重新训练模型，而是在已有模型的基础上训练模型，相当于迁移学习。但是这种方法效果无法超过每次都聚合的方法，这是很多实验的结果。当然这种方法比较节省时间，所以当时间不充足或者计算资源较少的情况下可以使用这种方法，但是得到的效果不会太好。 理解完了DAgger算法，下面自然就要理解一下这个算法的缺陷。 事实上，除去一些所有监督学习方法的共同缺陷外，这个算法的缺陷并不多，其中值得我们认证考虑的缺陷只有一个，就是我们如何划算的给所有数据打标签？ 将DAgger算法迭代越多，则得到的数据量越大，得到的模型效果越好，但是面对增加的数据量，打标签的花费也在上升，有些时候打标签花费比较廉价，但有时可以很昂贵，尤其是面对巨大的数据量，一般最后都不会太廉价。我们还没有考虑其他方面带来的花费，比如采集数据。此外，打标签有时也不是一件简单的事情。仍然以自动驾驶为例，让司机通过看录像打标签，很可能比直接开车做出正确选择的概率会小一点，虽然我们无法直接得到这个概率差别是多少。 谈到这里，我们自然会诞生一个疑问，就是我们怎么克服数据量这个缺陷？显然数据量的需求来源于模型的要求，要克服这个缺陷，我们就要问一个问题，就是能不能找到一种不需要大量数据的模型来完成IL这件事？ 针对这个问题，我们可以提出一种思路，但是真正的解决还是要用到RL模型。下面是这种思路的想法： DAgger addresses the problem of distributional “drift” What if our model is so good that it doesn’t drift? Need to mimic expert behavior very accurately. But don’t overfit! [4] Why might we fail to fit the expert? 下面我们抛开具体算法来讨论一个问题：为什么我们拟合专家行为可能会失败？ 其中一个原因是专家行为可能是非马尔科夫行为(Non-Markovian behavior)。我们之前已经提到过，sequential decision的基本假设是markov property，这一特性在我们的模型中体现为我们使用的监督学习模型是无记忆性的，事实上，大多数监督学习方法都是无记忆性的。但是现实生活中，习惯、心情等事物都可能使一个人在某时刻更加偏爱某种决策，甚至于这种影响有时候其本人都无法察觉。如当我选择一条上班的路线时，我一般会选择最常走的那条，但是某天心情好，我就很想走一条之前未曾走过的路线。这两种选择都不是markovian behavior， 因为正确判断我要选择的路线不仅仅需要知道我当时的状态（心情），还需要知道我之前哪些路走得多，哪些路走得少。 IL面对的另一个重要问题是多方式行为(Multimodal behavior)。意思是说，有些情况下，我有多种方法达到同种效果，但是不能综合这些方法去达到这种效果，只能选择其一。例如，当我想绕过面前的一棵树时，我可以从左边绕过，也可以从右边绕过，但是我不能综合两种方法从中间绕过。 下面我们针对两种问题分别给出一些对应的解决思路，由于笔者能力有限，暂时对这些问题不能给出更深入的理解，其中大部分是对cs294课程的重述，有感兴趣的读者可以进一步研究，同时，笔者会在对此部分有进一步理解时在博文中更新本部分。 [a] Non-Markovian behavior 前面说过，这个问题导致的结果就是：我们在做出下一步决策时，不仅仅要考虑到现在的状况，还要考虑到之前的状况。也就是说，我们要从计算 πθ(at∣ot)\\pi_{\\theta}(a_{t}|o_{t})πθ​(at​∣ot​) 转变为计算 πθ(at∣o1,…,ot).\\pi_{\\theta}(a_{t}|o_{1},\\ldots,o_{t}).πθ​(at​∣o1​,…,ot​). 要达到这种效果也很简单，就是采用递归神经网络，这类神经网络已经有了很大发展，相信大家并不陌生，比如iphone的语音助手siri，它可以联系用户的前几句话来理解用户的意图，其根本原因就是采用了递归神经网络。 [b] Muitimodal behavior [I] 离散模型 对于离散模型的multimodal behavior行为，我们要做的是从一些有限的方案中选出一种方案，只需要在神经网络最后加上一层softmax层。 [II]连续结构 对于能够采取连续行为的问题，模型最后的输出应当能够对应连续行为的决策，这类问题一般采用高斯分布实现（有时也用均方误差，等价于使用高斯分布，因为均方误差即高斯分布的对数概率），实现方式一般有三种，各有优劣： [i] Output mixture of Gaussians π(a∣o)=ΣiωiN(μi,Σi)\\pi(a|o)=\\Sigma_{i}\\omega_{i}N(\\mu_{i},\\Sigma_{i})π(a∣o)=Σi​ωi​N(μi​,Σi​) 特点：对低维决策效果较好。 [ii] Latent varible models step1. 不改变输出结构，仍以单高斯分布模型的简单形式存在； step2. 在神经网络底部输入额外的随机数（分布不唯一）。 难点：如何让神经网络有效利用噪声。 [iii] Autoregressive discretization step1. 从一个决策维度开始，一个神经网络增加一个决策维度； step2. 每个网络都结合前一个网络的输出和新的条件得到新的输出，并且决策的维度增加一。 特点：简单，但对网络结构改变较大，需要重新设计。 [5] Other topics in imitation learning Structured prediction 这一领域对输出的结构往往有一定要求，应用比较广泛的如机器翻译领域等。 Inverse reinforcement learning 通过模仿，反向理解行为的目的，之后寻找更好的方法来达到该目的。 [6] Imitation learning: What’s the problem? Data is typical finite; Humans are not good at providing some kinds of actions; Humans can learn autonomously; can our mechines do the same? 参考资料 cs294-112, lec-2","permalink":"http://yangtf983.github.io/2020/01/02/Imitation_Learing&Dagger_Algorithm/","photos":[]},{"tags":[{"name":"Reinforcement Learning","slug":"Reinforcement-Learning","permalink":"http://yangtf983.github.io/tags/Reinforcement-Learning/"}],"title":"A simple introduce of reinforcement learning","date":"2019/01/30","text":"[0] 说明 这是本博客RL系列的第一篇文章，旨在对RL进行一个简单的介绍，不涉及高深的理论。作为一个RL初学者，本博客的一系列RL文章都将是我在学习过相应的课程后写的，我会在学习已有资料的基础上加上自己的理解，但是一般不会注明哪些是自己的理解，哪些是材料中的观点。当然我的理解难免会有差错，因此我会在每一篇博文后注明我的学习材料，有需要的读者可以自行寻找阅读，若发现我的差错，欢迎批评指正。 [1] How do we build intelligent machines? 首先我们尝试理解一个大问题，就是：How do we build intelligent machines?理解这个问题是为了解决本节一个更为核心的问题：What is reinforcement learning, and why should we care? 机器是能够根据设定的指令行事的一种无生命的形式，比如手表等物品，但是我们不说手表是智能机器。我们在智能手机名称前加了个智能，但是我们仍然不认为它是智能机器。甚至更复杂一点的，如飞机、火箭、宇宙飞船，集结了人类无数智慧结晶的东西，我们仍然不说它们是智能机器。但是，我们会说microsoft的情感机器人是智能机器、击败人类围棋冠军的alphago是智能机器，哪怕它们目前技术含量并不一定比得上火箭之类的“非智能机器”，但是它们能够对人类的行为做出反应，并且这些反应并不是人类提前用判断语句判断好写入的，而是它们自己学习并适应的，只要给他们足够的时间，他们可以适应越来越多的情况。所以我们可以说：Intelligent machines must be able to adapt. 要想建立这种智能机器，我们就必须要有一种能够让它们学习适应各种环境的方法，这种方法几十年前就有人提出了，但是受限于算力和数据的问题，没有得到很好的发展，直到近年这些问题被解决后才又一次受到大众的观众，这种方法就是**“deep learning”(简称DL)** DL是一种可以处理非结构化环境(unstructures environment)的方法，你也可以说它是一种黑箱算法，它改变了我们传统的自己定义特征的方法，转而由数据进行训练。在看不出过拟合现象（训练集效果很好，预测集效果很差）的前提下，你可以尽可能地增加层数，即便你也不知道每一层分别得到了什么特征。这种方法简化了解决问题的难度，同时具有很高的通用性（不是说迁移能力强），前提是你要有足够的数据。 但是在reinforcement learning(简称RL)诞生并与DL结合前，DL的应用仅仅是在处理感知信息上，比如对数据进行分类，不管数据是有标签的还是无标签的都可以做到，我们分别称之为supervised learning和unsupervised learning，直到RL诞生，才提供了一种让机器学习行为的方式，后来将RL于DL结合，等于是将“是什么”(DL)和“怎么做”(RL)结合到了一起，诞生了一种说法叫做deep reinforcement learning(简称DRL)，人类得以开始建造intelligient mechines.RL领域现在备受关注的主要原因也是由于其与DL方法的结合产生了很多新的成果，甚至于我们可以说RL取得的最大成功就是与神经网络和深度网络的结合。在后续系列博客中，我们将不再区分RL与DRL，一般均用RL代替。 Two ways to build intelligent learning 标准方法：解析→分块生成→组合 学习方法：建立学习算法→自动学习功能 [2] What is deep RL, and why should we care? 在上一部分中我们已经介绍了什么是DL以及什么是DRL，并且向大家简单说明了在使用DL之前的方法（我们称之为“标准方法”）是怎样的。简而言之，标准方法中，特征是人为定义和提取的，机器的行为策略也是人为定义的。下面我们将总结一下引入RL后的改变，帮助大家理解Why should we care about RL? 计算机视觉领域 标准方法：人为提取每一层特征，很复杂，甚至可以作为一个人整个博士期间的研究工作； DL：end-to-end training (端到端) 优点： 减少人工量；2. 找到的方法往往比用标准方法更好。 游戏领域 标准方法：人为建立许多特征与策略； DL：end-to-end training (端到端) 优点： 减少人工量；2. 找到的方法往往比用标准方法更好。 [3] What does end-to-end learning mean for sequential decision making? 我们先来解释一下什么是end-to-end learning. 这个问题的解释其实和你如何定义一个完整的过程有关，假如我们现在定义一个人的一个完整的反射过程是从他接收到环境信息到他对环境做出反应这一整个过程，那么一个end-to-end learning的意思就是我的学习系统只需要这两个端口的信息（也就是环境信息和人做出的反应）就可以进行学习，而不必对中间的过程再进行拆解。 举个例子，假如你现在在野外，你接受到的环境信息是看到了一只凶猛的野生老虎，一个end-to-end learning系统会直接告诉你快点逃跑（当然如果你有武器你可以选择一搏，我们这里只用一般情况做例子），如果你想问这个系统为什么发出这样的指令，它会告诉你不跑你很可能会死掉，但不会告诉你任何判断的中间分析过程。而如果是其他方法，可能会将这个反射过程分解，分解为信息处理系统与决策系统，在信息处理系统中，它专注于得到信息的精准描述，如判断是不是老虎，如果是，再判断是不是动物园的老虎，判断发现这是一只野生老虎，再判断你是否有武器，发现你没有，这个系统的工作就算结束了，接着把这些信息发送给决策系统，决策系统受到信息：你正面对一只野生老虎，没有武器。接着，给你下达了逃跑的指令。想一想如果你问这个系统你为什么需要逃跑它会怎么回答你？它很可能把信息处理系统的结果告诉你，然后说根据这个信息，判断结果你需要逃跑。这就是二者的区别。 再考虑一下它们的实现机理，发现什么不同了吗？在后面的决策过程中，这个系统根本不需要知道如果你不逃跑会有什么后果，我们只需要把指令写进去，它进行简单的逻辑判断就可以发出决策，但是在前者中，你必须知道哪一种行为会得到什么后果，并且知道你需要什么样的后果，当然这里的后果可以通过不断的尝试得到，因为我们可以通过计算机模拟，不像前面举的例子，计算机模拟中尝试后出现不满意后果是完全可以接受的。 实际上我们完全可以这样总结，end-to-end learning在sequential decision中最大的特点就是需要知道不同的结果是好是坏。 Conclusion1: Deep mdoels are what allow reinforcement learning algorithms to solve complex problems end-to-end. [4] 从机器学习到RL 前面的部分中我们已经讲了很多对RL的理解，下面我们引用RL领域一本经典书籍中对RL的一个说明作为其定义，为后面的叙述做铺垫。这本书是:Reinforcement learning: An introduce，关于该定义的详细解释可以自己从书中去找。 Beyond the agent and the environment, one can identify four main subelements of a reinforcement learning system: a policy, a reward signal, a value function, and, optionally, a model of the environment. 结合该定义与之前的例子，我们很容易知道，实现一个RL方法，我们至少需要三个信息：action(行为),observation(观测值),reward(奖励)。下面我们通过重构分类问题以及NPL问题来说明一种观点：RL问题是大多数其他机器学习的一种更一般的表现形式。 分类问题→RL action:输出的标签 observation:图像的像素 reward:分类正确率 NPL(以翻译为例)→RL action:翻译 observation:原语言 reward:翻译质量 [5] Why should we study this now? 这里，我们不加解释的给出三条理由，相信大家能够理解： Advances in deep learning Advances in reinforcement learning Advances in computational capability 一些成功的例子可能更能够让你感受到这个领域目前的发展前景： 用Q-learning学习玩游戏 用policy training控制机器人 alphago, alphastar [6] What other problems do we need to solve to enable real-world sequential decision making? Beyond learning from reward 基础RL方法处理最大化奖励问题，但是sequential decision还涉及其他方面，这对RL方法提出了新的要求。 Where do rewards come from 现实世界中的奖励函数很难得到，甚至在某些事情上，感知做完了和完成这件事一样困难。 Are there other forms of supervision Learning from demonstrations（模仿、推断） Learning from observing the world Prediction [7] Why deep RL? deep = can process complex sensory input and also compute really complex functions RL = can choose complex actions [8] What can deep learning &amp; RL do well now? 有明确的已知简单规则的事物，如围棋； 有原始感觉的输入以及足够经验的事物，如机器人； 通过专家行为的模仿学习. [9] What has proven challenging so far? 学习速度：DRL学习速度比人类慢很多； 迁移能力弱； 难以找到合适的奖励函数，奖励函数对于学习行为与学习速度都至关重要； 应该大力发展基于模型的or无模型的RL. 参考资料 cs294-112, lec-1 Reinforcement Learning: An Introduction","permalink":"http://yangtf983.github.io/2019/01/30/A_simple_introduce_of_RL/","photos":[]}]}