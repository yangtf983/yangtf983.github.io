{"meta":{"title":"Young's Blog","subtitle":"","description":"keep foolish, keep hungry","author":"Alexis Young","url":"http://yangtf983.github.io","root":"/"},"posts":[{"tags":[],"title":"台湾大学林轩田机器学习基石课程学习笔记16（完结） -- Three Learning Principles","date":"2020/01/02","text":"作者：红色石头 公众号：AI有道（ID：redstonewill） 上节课我们讲了一个机器学习很重要的工具——Validation。我们将整个训练集分成两部分：DtrainD_{train}Dtrain​和DvalD_{val}Dval​，一部分作为机器学习模型建立的训练数据，另一部分作为验证模型好坏的数据，从而选择到更好的模型，实现更好的泛化能力。这节课，我们主要介绍机器学习中非常实用的三个“锦囊妙计”。 ###一、Occam’s Razor 奥卡姆剃刀定律（Occam’s Razor），是由14世纪逻辑学家、圣方济各会修士奥卡姆的威廉（William of Occam，约1285年至1349年）提出。奥卡姆（Ockham）在英格兰的萨里郡，那是他出生的地方。他在《箴言书注》2卷15题说“切勿浪费较多东西去做用较少的东西同样可以做好的事情。” 这个原理称为“如无必要，勿增实体”（Entities must not be multiplied unnecessarily），就像剃刀一样，将不必要的部分去除掉。 Occam’s Razor反映到机器学习领域中，指的是在所有可能选择的模型中，我们应该选择能够很好地解释已知数据并且十分简单的模型。 上图就是一个模型选择的例子，左边的模型很简单，可能有分错的情况；而右边的模型非常复杂，所有的训练样本都分类正确。但是，我们会选择左边的模型，它更简单，符合人类直觉的解释方式。这样的结果带来两个问题：一个是什么模型称得上是简单的？另一个是为什么简单模型比复杂模型要好？ 简单的模型一方面指的是简单的hypothesis h，简单的hypothesis就是指模型使用的特征比较少，例如多项式阶数比较少。简单模型另一方面指的是模型H包含的hypothesis数目有限，不会太多，这也是简单模型包含的内容。 其实，simple hypothesis h和simple model H是紧密联系的。如果hypothesis的特征个数是l，那么H中包含的hypothesis个数就是2l2^l2l，也就是说，hypothesis特征数目越少，H中hypothesis数目也就越少。 所以，为了让模型简单化，我们可以一开始就选择简单的model，或者用regularization，让hypothesis中参数个数减少，都能降低模型复杂度。 那为什么简单的模型更好呢？下面从哲学的角度简单解释一下。机器学习的目的是“找规律”，即分析数据的特征，总结出规律性的东西出来。假设现在有一堆没有规律的杂乱的数据需要分类，要找到一个模型，让它的Ein=0E_{in}=0Ein​=0，是很难的，大部分时候都无法正确分类，但是如果是很复杂的模型，也有可能将其分开。反过来说，如果有另一组数据，如果可以比较容易找到一个模型能完美地把数据分开，那表明数据本身应该是有某种规律性。也就是说杂乱的数据应该不可以分开，能够分开的数据应该不是杂乱的。如果使用某种简单的模型就可以将数据分开，那表明数据本身应该符合某种规律性。相反地，如果用很复杂的模型将数据分开，并不能保证数据本身有规律性存在，也有可能是杂乱的数据，因为无论是有规律数据还是杂乱数据，复杂模型都能分开。这就不是机器学习模型解决的内容了。所以，模型选择中，我们应该尽量先选择简单模型，例如最简单的线性模型。 ###二、Sampling Bias 首先引入一个有趣的例子：1948年美国总统大选的两位热门候选人是Truman和Dewey。一家报纸通过电话采访，统计人们把选票投给了Truman还是Dewey。经过大量的电话统计显示，投给Dewey的票数要比投个Truman的票数多，所以这家报纸就在选举结果还没公布之前，信心满满地发表了“Dewey Defeats Truman”的报纸头版，认为Dewey肯定赢了。但是大选结果公布后，让这家报纸大跌眼镜，最终Truman赢的了大选的胜利。 为什么会出现跟电话统计完全相反的结果呢？是因为电话统计数据出错还是投票运气不好？都不是。其实是因为当时电话比较贵，有电话的家庭比较少，而正好是有电话的美国人支持Dewey的比较多，而没有电话的支持Truman比较多。也就是说样本选择偏向于有钱人那边，可能不具有广泛的代表性，才造成Dewey支持率更多的假象。 这个例子表明，抽样的样本会影响到结果，用一句话表示“If the data is sampled in a biased way, learning will produce a similarly biased outcome.”意思是，如果抽样有偏差的话，那么学习的结果也产生了偏差，这种情形称之为抽样偏差Sampling Bias。 从技术上来说，就是训练数据和验证数据要服从同一个分布，最好都是独立同分布的，这样训练得到的模型才能更好地具有代表性。 ###三、Data Snooping 之前的课程，我们介绍过在模型选择时应该尽量避免偷窥数据，因为这样会使我们人为地倾向于某种模型，而不是根据数据进行随机选择。所以，Φ\\PhiΦ应该自由选取，最好不要偷窥到原始数据，这会影响我们的判断。 事实上，数据偷窥发生的情况有很多，不仅仅指我们看到了原始数据。什么意思呢？其实，当你在使用这些数据的任何过程，都是间接地偷看到了数据本身，然后你会进行一些模型的选择或者决策，这就增加了许多的model complexity，也就是引入了污染。 下面举个例子来说明。假如我们有8年的货比交易数据，我们希望从这些数据中找出规律，来预测货比的走势。如果选择前6年数据作为训练数据，后2年数据作为测试数据的话，来训练模型。现在我们有前20天的数据，根据之前训练的模型，来预测第21天的货比交易走势。 现在有两种训练模型的方法，如图所示，一种是使用前6年数据进行模型训练，后2年数据作为测试，图中蓝色曲线表示后2年的预测收益；另一种是直接使用8年数据进行模型训练，图中红色曲线表示后2年的预测收益情况。图中，很明显，使用8年数据进行训练的模型对后2年的预测的收益更大，似乎效果更好。但是这是一种自欺欺人的做法，因为训练的时候已经拿到了后2年的数据，用这样的模型再来预测后2年的走势是不科学的。这种做法也属于间接偷窥数据的行为。直接偷窥和间接偷窥数据的行为都是不科学的做法，并不能表示训练的模型有多好。 还有一个偷窥数据的例子，比如对于某个基准数据集D，某人对它建立了一个模型H1，并发表了论文。第二个人看到这篇论文后，又会对D，建立一个新的好的模型H2。这样，不断地有人看过前人的论文后，建立新的模型。其实，后面人选择模型时，已经被前人影响了，这也是偷窥数据的一种情况。也许你能对D训练很好的模型，但是可能你仅仅只根据前人的模型，成功避开了一些错误，甚至可能发生了overfitting或者bad generalization。所以，机器学习领域有这样一句有意思的话“If you torture the data long enough, it will confess.”所以，我们不能太“折磨”我们的数据了，否则它只能“妥协”了~哈哈。 在机器学习过程中，避免“偷窥数据”非常重要，但实际上，完全避免也很困难。实际操作中，有一些方法可以帮助我们尽量避免偷窥数据。第一个方法是“看不见”数据。就是说当我们在选择模型的时候，尽量用我们的经验和知识来做判断选择，而不是通过数据来选择。先选模型，再看数据。第二个方法是保持怀疑。就是说时刻保持对别人的论文或者研究成果保持警惕与怀疑，要通过自己的研究与测试来进行模型选择，这样才能得到比较正确的结论。 ###四、Power of Three 本小节，我们对16节课做个简单的总结，用“三的威力”进行概括。因为课程中我们介绍的很多东西都与三有关。 首先，我们介绍了跟机器学习相关的三个领域： Data Mining Artificial Intelligence Statistics 我们还介绍了三个理论保证： Hoeffding Multi-Bin Hoeffding VC 然后，我们又介绍了三种线性模型： PLA/pocket linear regression logistic regression 同时，我们介绍了三种重要的工具： Feature Transform Regularization Validation 还有我们本节课介绍的三个锦囊妙计： Occam’s Razer Sampling Bias Data Snooping 最后，我们未来机器学习的方向也分为三种： More Transform More Regularization Less Label ###五、总结 本节课主要介绍了机器学习三个重要的锦囊妙计：Occam’s Razor, Sampling Bias, Data Snooping。并对《机器学习基石》课程中介绍的所有知识和方法进行“三的威力”这种形式的概括与总结，“三的威力”也就构成了坚固的机器学习基石。 整个机器学习基石的课程笔记总结完毕！后续将会推出机器学习技法的学习笔记，谢谢！ 注明： 文章中所有的图片均来自台湾大学林轩田《机器学习基石》课程","permalink":"http://yangtf983.github.io/2020/01/02/%E5%8F%B0%E6%B9%BE%E5%A4%A7%E5%AD%A6%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E8%AF%BE%E7%A8%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016%EF%BC%88%E5%AE%8C%E7%BB%93%EF%BC%89%20--%20Three%20Learning%20Principles/","photos":[]},{"tags":[{"name":"Reinforcement Learning","slug":"Reinforcement-Learning","permalink":"http://yangtf983.github.io/tags/Reinforcement-Learning/"}],"title":"Imitation Learning & Dagger Algorithm","date":"2020/01/02","text":"[0] 说明 进入Reinforcement Learning的世界，一般而言应当是从tabular method学起，但是从imitation learning学起推广到Reinforcrmrnt Learning可以帮助我们更好的理解RL的解决问题的思路和发展目的，因此我们本次继续跟随cs294的脚步，学习Imitation learning的相关知识以及相对简单的Dagger算法。 值得一提的是，IL应当归类于监督学习(Supervised Learning)，但是其可以应用于IRL(Inverse Reinforcement Learning)领域，这也是IL与IRL的重要联系之一。 [1] Terminology &amp; Notation 首先我们应当明白我们探讨的是什么类型的问题，我们称之为时序决策问题(sequrntial decision)，这类问题的主要特点是具有马尔科夫性(markov property)，也即是说，在当前条件已知的情况下，未来和过去是独立的。换句话说，如果我掌握了现在的情况，那么从纯理性的角度讲，是否掌握过去的情况对我做出下一步决策毫无影响。其数学定义参照下式： P(X(tn)≤xn∣X(tn−1)=xn−1,…,X(t1)=x1)=P(X(tn)≤xn∣X(tn−1)=xn−1)P( X(t_{n} )\\leq x_{n} | X(t_{n-1})=x_{n-1}, \\ldots,X( t_{1} ) = x_{1} ) = P( X(t_{n} )\\leq x_{n} | X(t_{n-1})=x_{n-1}) P(X(tn​)≤xn​∣X(tn−1​)=xn−1​,…,X(t1​)=x1​)=P(X(tn​)≤xn​∣X(tn−1​)=xn−1​) 其中，t1&lt;t2&lt;…&lt;tnt_{1}&lt;t_{2}&lt;\\ldots &lt;t_{n}t1​&lt;t2​&lt;…&lt;tn​. 对于这样一个决策问题，系统首先需要一个输入，也即是系统的观测值，记为oto_{t}ot​。众所周知，神经网络会对oto_{t}ot​进行一系列处理，得到一个可能采取的行为的分布函数（对于有限行为决策，即为分布列），我们称这个结果称为我们的决策策略。我们将行为记作ata_{t}at​，策略记作πθ(at∣ot)\\pi_{\\theta}(a_{t}|o_{t})πθ​(at​∣ot​). 上面都很好理解，但是我们这里还要定义另外一个符号，这个符号叫做状态，记为sts_{t}st​. 定义sts_{t}st​是因为oto_{t}ot​并不总是无损的。举例来说，假如我有一张图片，图片上是一辆行驶的汽车，我们将这张图片输入给一个神经网络系统，oto_{t}ot​就是这张照片的像素，是一个或几个二维数组，我们可以通过神经网络对这张图片进行分析，得到这张图片上各种信息，但是我们永远不能判断车后面有没有一只动物，有可能这辆车刚刚好挡住了一只动物，但是这只动物不在照片里，或者说不在oto_{t}ot​里，但是我们说它在sts_{t}st​里，sts_{t}st​包含此刻决策相关的所有信息，但是oto_{t}ot​不一定。 我们再通过下图来理解这个决策系统： 我们将ot=sto_{t}=s_{t}ot​=st​的系统称为完全观察(fully observed)系统，将ot≠sto_{t} \\neq s{t}ot​​=st的系统称为部分观察(partial observed)系统。显然，对于一个partial observed问题，若我们只有oto_{t}ot​而没有sts_{t}st​，ot−1o_{t-1}ot−1​对我们做出下一步行动是有用的，因为它可能帮助我们发现车后究竟有没有动物。此时，我们需要一个决策系统可以不仅仅考虑到oto_{t}ot​，还可以考虑到ot−1o_{t-1}ot−1​甚至于更早的观测值。 [2] Introduce to Behavior Cloning 我们将模仿专家行为进行学习的一类算法又称为behavior cloning，这类算法以专家行为作为数据标签，通过对数据集进行监督学习得到模型πθ(at∣ot)\\pi_{\\theta}( a_{t}|o_{t} )πθ​(at​∣ot​)，以此进行决策。 我们以自动驾驶为例，behavior cloning的一个可能做法是找一些驾驶熟练的人类司机，让他们驾车行驶，并且通过摄像头拍摄沿路情况，并记录下司机的行为(左转或者右转)作为数据标签，接着用这些有标签的数据进行监督学习，因此本质上IL/behavior cloning是监督学习的分支，下图是我们所举例子的一个直观表示： ![behavior cloning diagram](Imitation_Learing&Dagger_Algorithm/behavior.png) [3] DAgger Algorithm DAgger算法的诞生是为了解决behavior cloning中一个很严重的问题，因此我们先来看看这个问题。 前面已经讲过，我们在使用behavior cloning时采用了监督学习算法。我们都知道，无论数据量有多大，采用的监督学习算法有多准确，都可能存在一种情况，就是在行驶过程中某一步和原来的专家行为产生偏差，因为我们不是完全的模仿专家行为，而是采用了一个监督学习模型(如果完全模仿专家行为，则模型的利用场景过于有限，一旦碰到任何没有碰到的情况，汽车就会手足无措，而在行驶过程中，可能发生的各种情况是难以模拟遍的)。一旦这个偏差产生，由于它不是在数据集中的，做出的进一步决策很可能使其朝着进一步偏差的方向进行，而随着汽车碰到的状况与数据集的偏差越来越大，其做出的决策则越来越不具有合理性，偏离原数据集的速度会越来越快，放在自动驾驶上，就是偏离车道的速度加快。这一现象在很多实验中得到了验证，下图是一张验证这一现象的模拟图： ![behavior cloning diagram](Imitation_Learing&Dagger_Algorithm/simulation~1.png) 针对之前提到的自动驾驶，NVIDIV提出了一种解决方案，就是同时拍摄左中右三个方向的情况，给左侧的数据打上标签“右”，右侧的数据打上标签“左”,这样当检测到车头方向偏离时，及时调整回去。经过实际检验，这种方案的效果很不错。 这个问题看似已经解决了，但是我们还是要思考一下，有没有更加通用的方法，使得我们不用重新获取这些数据，并且可以用于一些其他领域的behavior cloning问题？ 还是用自动驾驶举例，想像一下，如果我们能够得到整个轨迹的分布，也就是所有的汽车可能碰到的情况，然后让专家给出所有的label，这样无论汽车遇到什么情况，都可以精确按照专家给出的建议行驶。但是这种假设的问题在于，我们无法得到一个有无数种可能的情况的分布，即便是我们找出了所有情况，也无法承担让专家给所有情况打分的高额花费。 那我们能不能找到一种方法，来只对个数不多的有限种情况进行打分，并尽量让汽车在这些情况能够给出有效指导的范围内行动呢？ 我们将原始数据集记为data，我们知道，我们的模型能够应对的情况是Pdata(ot)P_{data}(o_{t})Pdata​(ot​)，而汽车在行驶过程中真正面对的情况是Pπθ(ot)P_{\\pi_{\\theta}}(o_{t})Pπθ​​(ot​) ，当data ≠πθ\\neq \\pi_{\\theta}​=πθ​ 时，自然有 Pdata(ot)≠Pπθ(ot)P_{data}(o_{t})\\neq P_{\\pi_{\\theta}}(o_{t})Pdata​(ot​)​=Pπθ​​(ot​)，因此产生了偏离。如果我们能找到一种方法使得Pdata(ot)≠Pπθ(ot)P_{data}(o_{t})\\neq P_{\\pi_{\\theta}}(o_{t})Pdata​(ot​)​=Pπθ​​(ot​)，便能解决这个问题。因此我们的一个思路是尽可能让data=πθdata=\\pi_{\\theta}data=πθ​. 依照这种思路，我们找到了DAgger(Dataset Aggregation)算法。DAgger是一种很简单的方法，并且已经被证明在在线学习的情况下这种算法是收敛的。下面介绍一下DAgger算法的流程： train πθ(at∣ot)\\pi_{\\theta}(a_{t}|o_{t})πθ​(at​∣ot​) from human data D= {o1,a1,…,oN,aN}\\{o_{1},a_{1},\\ldots,o_{N},a_{N}\\}{o1​,a1​,…,oN​,aN​}. run πθ(at∣ot)\\pi_{\\theta}(a_{t}|o_{t})πθ​(at​∣ot​) to get dataset Dπ={o1,…,oM}D_{\\pi}=\\{o_{1},\\ldots,o_{M}\\}Dπ​={o1​,…,oM​}. Ask human to label DπD_{\\pi}Dπ​ with actions ata_{t}at​. Aggregate: D→D⋃Dπ.D\\rightarrow D\\bigcup D_{\\pi}.D→D⋃Dπ​. repeat step1 ~ step4. 这里有一个问题，是cs294中的一个学生提出来的：为什么step4要将D和DπD_{\\pi}Dπ​聚合，而不是用DπD_{\\pi}Dπ​代替D？ 视频中老师给出了两个原因：(1)这样做效果不好;(2)DAgger算法收敛的基础是在线学习。 但是视频中没有给出更详细的解释，我理解了一下，可以大概给出一种解释方法： 首先，behavior cloning使用的是监督学习方法，DAggger算法的目的是尽可能让车的行驶路径在我们的模型的无偏差的计算范围内，即便是偏差了也要尽可能是我们考虑过的偏差情况。那么如果我们不聚合D和DπD_{\\pi}Dπ​，我们也能仅仅用DπD_{\\pi}Dπ​去训练一个新模型，因为这样同样是部分的，与用D训练一个新模型并没有本质差别，因此如果我们每一步仅仅用新的DπD_{\\pi}Dπ​训练模型，最后必然要将所有循环中得到的数据进行一次聚合再进行训练得到使用的模型，这样训练的模型收敛的可能显然不如每一次循环将所有已有的数据聚合起来进行训练的收敛的可能大（直观理解），而后者已经被证明是收敛的。 其次，我们也可以考虑在不聚合数据的情况下每次不去重新训练模型，而是在已有模型的基础上训练模型，相当于迁移学习。但是这种方法效果无法超过每次都聚合的方法，这是很多实验的结果。当然这种方法比较节省时间，所以当时间不充足或者计算资源较少的情况下可以使用这种方法，但是得到的效果不会太好。 理解完了DAgger算法，下面自然就要理解一下这个算法的缺陷。 事实上，除去一些所有监督学习方法的共同缺陷外，这个算法的缺陷并不多，其中值得我们认证考虑的缺陷只有一个，就是我们如何划算的给所有数据打标签？ 将DAgger算法迭代越多，则得到的数据量越大，得到的模型效果越好，但是面对增加的数据量，打标签的花费也在上升，有些时候打标签花费比较廉价，但有时可以很昂贵，尤其是面对巨大的数据量，一般最后都不会太廉价。我们还没有考虑其他方面带来的花费，比如采集数据。此外，打标签有时也不是一件简单的事情。仍然以自动驾驶为例，让司机通过看录像打标签，很可能比直接开车做出正确选择的概率会小一点，虽然我们无法直接得到这个概率差别是多少。 谈到这里，我们自然会诞生一个疑问，就是我们怎么克服数据量这个缺陷？显然数据量的需求来源于模型的要求，要克服这个缺陷，我们就要问一个问题，就是能不能找到一种不需要大量数据的模型来完成IL这件事？ 针对这个问题，我们可以提出一种思路，但是真正的解决还是要用到RL模型。下面是这种思路的想法： DAgger addresses the problem of distributional “drift” What if our model is so good that it doesn’t drift? Need to mimic expert behavior very accurately. But don’t overfit! [4] Why might we fail to fit the expert? 下面我们抛开具体算法来讨论一个问题：为什么我们拟合专家行为可能会失败？ 其中一个原因是专家行为可能是非马尔科夫行为(Non-Markovian behavior)。我们之前已经提到过，sequential decision的基本假设是markov property，这一特性在我们的模型中体现为我们使用的监督学习模型是无记忆性的，事实上，大多数监督学习方法都是无记忆性的。但是现实生活中，习惯、心情等事物都可能使一个人在某时刻更加偏爱某种决策，甚至于这种影响有时候其本人都无法察觉。如当我选择一条上班的路线时，我一般会选择最常走的那条，但是某天心情好，我就很想走一条之前未曾走过的路线。这两种选择都不是markovian behavior， 因为正确判断我要选择的路线不仅仅需要知道我当时的状态（心情），还需要知道我之前哪些路走得多，哪些路走得少。 IL面对的另一个重要问题是多方式行为(Multimodal behavior)。意思是说，有些情况下，我有多种方法达到同种效果，但是不能综合这些方法去达到这种效果，只能选择其一。例如，当我想绕过面前的一棵树时，我可以从左边绕过，也可以从右边绕过，但是我不能综合两种方法从中间绕过。 下面我们针对两种问题分别给出一些对应的解决思路，由于笔者能力有限，暂时对这些问题不能给出更深入的理解，其中大部分是对cs294课程的重述，有感兴趣的读者可以进一步研究，同时，笔者会在对此部分有进一步理解时在博文中更新本部分。 [a] Non-Markovian behavior 前面说过，这个问题导致的结果就是：我们在做出下一步决策时，不仅仅要考虑到现在的状况，还要考虑到之前的状况。也就是说，我们要从计算 πθ(at∣ot)\\pi_{\\theta}(a_{t}|o_{t})πθ​(at​∣ot​) 转变为计算 πθ(at∣o1,…,ot).\\pi_{\\theta}(a_{t}|o_{1},\\ldots,o_{t}).πθ​(at​∣o1​,…,ot​). 要达到这种效果也很简单，就是采用递归神经网络，这类神经网络已经有了很大发展，相信大家并不陌生，比如iphone的语音助手siri，它可以联系用户的前几句话来理解用户的意图，其根本原因就是采用了递归神经网络。 ![RNN](Imitation_Learing&Dagger_Algorithm/RNN~1.png) [b] Muitimodal behavior [I] 离散模型 对于离散模型的multimodal behavior行为，我们要做的是从一些有限的方案中选出一种方案，只需要在神经网络最后加上一层softmax层。 [II]连续结构 对于能够采取连续行为的问题，模型最后的输出应当能够对应连续行为的决策，这类问题一般采用高斯分布实现（有时也用均方误差，等价于使用高斯分布，因为均方误差即高斯分布的对数概率），实现方式一般有三种，各有优劣： [i] Output mixture of Gaussians π(a∣o)=ΣiωiN(μi,Σi)\\pi(a|o)=\\Sigma_{i}\\omega_{i}N(\\mu_{i},\\Sigma_{i})π(a∣o)=Σi​ωi​N(μi​,Σi​) 特点：对低维决策效果较好。 [ii] Latent varible models step1. 不改变输出结构，仍以单高斯分布模型的简单形式存在； step2. 在神经网络底部输入额外的随机数（分布不唯一）。 难点：如何让神经网络有效利用噪声。 [iii] Autoregressive discretization step1. 从一个决策维度开始，一个神经网络增加一个决策维度； step2. 每个网络都结合前一个网络的输出和新的条件得到新的输出，并且决策的维度增加一。 特点：简单，但对网络结构改变较大，需要重新设计。 [5] Other topics in imitation learning Structured prediction 这一领域对输出的结构往往有一定要求，应用比较广泛的如机器翻译领域等。 Inverse reinforcement learning 通过模仿，反向理解行为的目的，之后寻找更好的方法来达到该目的。 [6] Imitation learning: What’s the problem? Data is typical finite; Humans are not good at providing some kinds of actions; Humans can learn autonomously; can our mechines do the same? 参考资料 cs294-112, lec-2","permalink":"http://yangtf983.github.io/2020/01/02/Imitation_Learing&Dagger_Algorithm/","photos":[]},{"tags":[{"name":"Reinforcement Learning","slug":"Reinforcement-Learning","permalink":"http://yangtf983.github.io/tags/Reinforcement-Learning/"}],"title":"A simple introduce of reinforcement learning","date":"2019/01/30","text":"[0] 说明 这是本博客RL系列的第一篇文章，旨在对RL进行一个简单的介绍，不涉及高深的理论。作为一个RL初学者，本博客的一系列RL文章都将是我在学习过相应的课程后写的，我会在学习已有资料的基础上加上自己的理解，但是一般不会注明哪些是自己的理解，哪些是材料中的观点。当然我的理解难免会有差错，因此我会在每一篇博文后注明我的学习材料，有需要的读者可以自行寻找阅读，若发现我的差错，欢迎批评指正。 [1] How do we build intelligent machines? 首先我们尝试理解一个大问题，就是：How do we build intelligent machines?理解这个问题是为了解决本节一个更为核心的问题：What is reinforcement learning, and why should we care? 机器是能够根据设定的指令行事的一种无生命的形式，比如手表等物品，但是我们不说手表是智能机器。我们在智能手机名称前加了个智能，但是我们仍然不认为它是智能机器。甚至更复杂一点的，如飞机、火箭、宇宙飞船，集结了人类无数智慧结晶的东西，我们仍然不说它们是智能机器。但是，我们会说microsoft的情感机器人是智能机器、击败人类围棋冠军的alphago是智能机器，哪怕它们目前技术含量并不一定比得上火箭之类的“非智能机器”，但是它们能够对人类的行为做出反应，并且这些反应并不是人类提前用判断语句判断好写入的，而是它们自己学习并适应的，只要给他们足够的时间，他们可以适应越来越多的情况。所以我们可以说：Intelligent machines must be able to adapt. 要想建立这种智能机器，我们就必须要有一种能够让它们学习适应各种环境的方法，这种方法几十年前就有人提出了，但是受限于算力和数据的问题，没有得到很好的发展，直到近年这些问题被解决后才又一次受到大众的观众，这种方法就是**“deep learning”(简称DL)** DL是一种可以处理非结构化环境(unstructures environment)的方法，你也可以说它是一种黑箱算法，它改变了我们传统的自己定义特征的方法，转而由数据进行训练。在看不出过拟合现象（训练集效果很好，预测集效果很差）的前提下，你可以尽可能地增加层数，即便你也不知道每一层分别得到了什么特征。这种方法简化了解决问题的难度，同时具有很高的通用性（不是说迁移能力强），前提是你要有足够的数据。 但是在reinforcement learning(简称RL)诞生并与DL结合前，DL的应用仅仅是在处理感知信息上，比如对数据进行分类，不管数据是有标签的还是无标签的都可以做到，我们分别称之为supervised learning和unsupervised learning，直到RL诞生，才提供了一种让机器学习行为的方式，后来将RL于DL结合，等于是将“是什么”(DL)和“怎么做”(RL)结合到了一起，诞生了一种说法叫做deep reinforcement learning(简称DRL)，人类得以开始建造intelligient mechines.RL领域现在备受关注的主要原因也是由于其与DL方法的结合产生了很多新的成果，甚至于我们可以说RL取得的最大成功就是与神经网络和深度网络的结合。在后续系列博客中，我们将不再区分RL与DRL，一般均用RL代替。 Two ways to build intelligent learning 标准方法：解析→分块生成→组合 学习方法：建立学习算法→自动学习功能 [2] What is deep RL, and why should we care? 在上一部分中我们已经介绍了什么是DL以及什么是DRL，并且向大家简单说明了在使用DL之前的方法（我们称之为“标准方法”）是怎样的。简而言之，标准方法中，特征是人为定义和提取的，机器的行为策略也是人为定义的。下面我们将总结一下引入RL后的改变，帮助大家理解Why should we care about RL? 计算机视觉领域 标准方法：人为提取每一层特征，很复杂，甚至可以作为一个人整个博士期间的研究工作； DL：end-to-end training (端到端) 优点： 减少人工量；2. 找到的方法往往比用标准方法更好。 游戏领域 标准方法：人为建立许多特征与策略； DL：end-to-end training (端到端) 优点： 减少人工量；2. 找到的方法往往比用标准方法更好。 [3] What does end-to-end learning mean for sequential decision making? 我们先来解释一下什么是end-to-end learning. 这个问题的解释其实和你如何定义一个完整的过程有关，假如我们现在定义一个人的一个完整的反射过程是从他接收到环境信息到他对环境做出反应这一整个过程，那么一个end-to-end learning的意思就是我的学习系统只需要这两个端口的信息（也就是环境信息和人做出的反应）就可以进行学习，而不必对中间的过程再进行拆解。 举个例子，假如你现在在野外，你接受到的环境信息是看到了一只凶猛的野生老虎，一个end-to-end learning系统会直接告诉你快点逃跑（当然如果你有武器你可以选择一搏，我们这里只用一般情况做例子），如果你想问这个系统为什么发出这样的指令，它会告诉你不跑你很可能会死掉，但不会告诉你任何判断的中间分析过程。而如果是其他方法，可能会将这个反射过程分解，分解为信息处理系统与决策系统，在信息处理系统中，它专注于得到信息的精准描述，如判断是不是老虎，如果是，再判断是不是动物园的老虎，判断发现这是一只野生老虎，再判断你是否有武器，发现你没有，这个系统的工作就算结束了，接着把这些信息发送给决策系统，决策系统受到信息：你正面对一只野生老虎，没有武器。接着，给你下达了逃跑的指令。想一想如果你问这个系统你为什么需要逃跑它会怎么回答你？它很可能把信息处理系统的结果告诉你，然后说根据这个信息，判断结果你需要逃跑。这就是二者的区别。 再考虑一下它们的实现机理，发现什么不同了吗？在后面的决策过程中，这个系统根本不需要知道如果你不逃跑会有什么后果，我们只需要把指令写进去，它进行简单的逻辑判断就可以发出决策，但是在前者中，你必须知道哪一种行为会得到什么后果，并且知道你需要什么样的后果，当然这里的后果可以通过不断的尝试得到，因为我们可以通过计算机模拟，不像前面举的例子，计算机模拟中尝试后出现不满意后果是完全可以接受的。 实际上我们完全可以这样总结，end-to-end learning在sequential decision中最大的特点就是需要知道不同的结果是好是坏。 Conclusion1: Deep mdoels are what allow reinforcement learning algorithms to solve complex problems end-to-end. [4] 从机器学习到RL 前面的部分中我们已经讲了很多对RL的理解，下面我们引用RL领域一本经典书籍中对RL的一个说明作为其定义，为后面的叙述做铺垫。这本书是:Reinforcement learning: An introduce，关于该定义的详细解释可以自己从书中去找。 Beyond the agent and the environment, one can identify four main subelements of a reinforcement learning system: a policy, a reward signal, a value function, and, optionally, a model of the environment. 结合该定义与之前的例子，我们很容易知道，实现一个RL方法，我们至少需要三个信息：action(行为),observation(观测值),reward(奖励)。下面我们通过重构分类问题以及NPL问题来说明一种观点：RL问题是大多数其他机器学习的一种更一般的表现形式。 分类问题→RL action:输出的标签 observation:图像的像素 reward:分类正确率 NPL(以翻译为例)→RL action:翻译 observation:原语言 reward:翻译质量 [5] Why should we study this now? 这里，我们不加解释的给出三条理由，相信大家能够理解： Advances in deep learning Advances in reinforcement learning Advances in computational capability 一些成功的例子可能更能够让你感受到这个领域目前的发展前景： 用Q-learning学习玩游戏 用policy training控制机器人 alphago, alphastar [6] What other problems do we need to solve to enable real-world sequential decision making? Beyond learning from reward 基础RL方法处理最大化奖励问题，但是sequential decision还涉及其他方面，这对RL方法提出了新的要求。 Where do rewards come from 现实世界中的奖励函数很难得到，甚至在某些事情上，感知做完了和完成这件事一样困难。 Are there other forms of supervision Learning from demonstrations（模仿、推断） Learning from observing the world Prediction [7] Why deep RL? deep = can process complex sensory input and also compute really complex functions RL = can choose complex actions [8] What can deep learning &amp; RL do well now? 有明确的已知简单规则的事物，如围棋； 有原始感觉的输入以及足够经验的事物，如机器人； 通过专家行为的模仿学习. [9] What has proven challenging so far? 学习速度：DRL学习速度比人类慢很多； 迁移能力弱； 难以找到合适的奖励函数，奖励函数对于学习行为与学习速度都至关重要； 应该大力发展基于模型的or无模型的RL. 参考资料 cs294-112, lec-1 Reinforcement Learning: An Introduction","permalink":"http://yangtf983.github.io/2019/01/30/A_simple_introduce_of_RL/","photos":[]},{"tags":[{"name":"Reinforcement Learning","slug":"Reinforcement-Learning","permalink":"http://yangtf983.github.io/tags/Reinforcement-Learning/"}],"title":"Reinforcement Learning学习计划","date":"2019/01/28","text":"A simple introduce of reinforcement learning 建立对RL的认识和理解，帮助进一步学习。 Imitation Learning &amp; Dagger Algorithm 从imitation learning开始，学习相对简单的Dagger算法，并理解为什么要发展RL. 3.","permalink":"http://yangtf983.github.io/2019/01/28/Reinforcement-Learning%E5%AD%A6%E4%B9%A0%E8%AE%A1%E5%88%92/","photos":[]}]}