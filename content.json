{"meta":{"title":"Young's Blog","subtitle":"","description":"keep foolish, keep hungry","author":"Alexis Young","url":"http://yangtf983.github.io","root":"/"},"posts":[{"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"title":"机器学习技法笔记12：Neural Network","date":"2020/07/24","text":"0 说明 上一讲讲了 Gradient Boosted Decision Tree，其核心是用最速下降法求一个聚合模型的最小误差函数值这一过程，这个问题的解法针对不同的损失函数有不同的形式，例如对于平方损失函数而言，基本算法就等于是在解一个从 xnx_nxn​ 到残差的最优算法，假设空间是基本算法的假设空间，而最佳步长则是求一个从当前 gtg_tgt​ 函数值到残差的无截距的线性回归的系数。 截止到上一讲我们就讲完了这门课程中的聚合模型，这一讲开始到第 15 讲结束，我们要讲的算法可以统称为 extraction model，其特点是可以自动提取隐藏的特征，而不是人为设计特征再去提取（比如人决定做几次回归）。 1 Motivation 首先看一个简单的例子：PLA 的线性聚合模型。 这个模型可以用图示表示如下： 这个图中 gtg_tgt​ 是基本模型 PLA，GGG 是把它们线性聚合起来的模型，由于符号函数的图形的阶梯状的，所以我们给相应的表示算法的圆圈里画一个阶梯状图形表示这个意思。 采用图上标注的符号容易得到最后模型的表达式是： G(x)=sign(Σt=1Tαtsign(wtTx))G(x)=sign(\\Sigma_{t=1}^T\\alpha_tsign(w^T_tx))G(x)=sign(Σt=1T​αt​sign(wtT​x)) 我们之前说过，uniform 的聚合追求的是稳定性（避免过拟合，提高模型的推广能力），linear 和 conditional 的模型追求的主要是 powerful，避免模型欠拟合。那么接下来我们就来看一下上面这个把 PLA 线性聚合起来的模型怎么提高了 PLA 的力量。 想象我们有基本的 PLA g1,g2g_1,g_2g1​,g2​，见上图，通过把 g1,g2g_1,g_2g1​,g2​ 线性聚合起来可以表示图三这个相对复杂的区域，并且这个区域可以理解为 g1,g2g_1,g_2g1​,g2​ 的与逻辑运算，通过公式表达就是 G(x)=sign(−1+g1(x)+g2(x))G(x)=sign(-1+g_1(x)+g_2(x))G(x)=sign(−1+g1​(x)+g2​(x))，这个区域显然不是通过一个 PLA 可以表达出来的，所以 PLA 的线性聚合模型扩大了模型的假设空间。同样这样聚合也可以做到或和非逻辑运算，分别可以表达为 G(x)=sign(1+g1(x)+g2(x))G(x)=sign(1+g_1(x)+g_2(x))G(x)=sign(1+g1​(x)+g2​(x)) 和 G(x)=sign(−g1(x))G(x)=sign(-g_1(x))G(x)=sign(−g1​(x)) . 注意这个时候的逻辑运算的表达往往要借助常数项，也就是说增加一个常数项可以增大模型的假设空间，所以我们的神经网络模型中一般每一层都会增加一个常数项。 以上免这个聚合模型为基础，增加基本模型的数量，可以进一步大大增大模型的假设空间，例如可以让模型逼近任意一个凸区域，下面是一个用 PLA 基本模型的与逻辑逼近一个圆形区域的例子： 但是这个时候仍然有一些做不到的区域，比如非凸区域。下面就是一个这样的例子，这个对角区域从逻辑上讲是前两个区域的异或逻辑（XOR），这个区域使用线性聚合的 PLA 做不到，但是我们可以通过增加层数做到： 做到这一点的方法是：XOR(g1,g2)=OR(AND(−g1,g2),AND(g1,g2))XOR(g_1,g_2)=OR(AND(-g_1,g_2),AND(g_1,g_2))XOR(g1​,g2​)=OR(AND(−g1​,g2​),AND(g1​,g2​))，这个时候的网络图是： 通过这个例子我们想说明的是我们通过聚合 PLA、增加聚合层数这两种方法可以非常有效地扩大模型的假设空间，增加模型的能力。理论证明通过增加层数和神经元数量可以逼近任何函数。这也使得这种模型可以学习到任意复杂的特征，当然这也需要我们付出高昂的计算代价并注意避免模型过拟合。 下面开始正式介绍这种模型。 2 Neural Network Hypothesis 上一小节中的网络图其实就已经是比较简单的神经网络模型了，我们现在更一般地来描述一下神经网络模型。 输出层 单看输出层其实就是一个线性模型，这个线性模型不仅仅指线性回归，只要是估计的参数是一次的都算是线性模型。记 s=wTϕ(2)(ϕ(1)(x))s=w^T\\phi^{(2)}(\\phi^{(1)}(x))s=wTϕ(2)(ϕ(1)(x)) ，其中 ϕ(1)\\phi^{(1)}ϕ(1) 是第一层的函数，ϕ(2)\\phi^{(2)}ϕ(2) 是第二层的函数，输出层的函数可以是线性分类 h(x)=sign(s)h(x)=sign(s)h(x)=sign(s)、线性回归 h(x)=sh(x)=sh(x)=s、逻辑斯谛回归 h(x)=θ(s)h(x)=\\theta(s)h(x)=θ(s) 等，这些都算线性模型，都可以套到神经网络的输出层。 非输出层 前面的层数所做的事情可以统称为转换，这些层数总是把上一层的数据转换后输出给下一层。圆圈内的图形表示的函数我们称之为转换函数（transformation function），它做的事情是把一个线性加权分数 sss 转换成输出。转换函数也可以有很多选择，比如 sign(s)sign(s)sign(s) 或 tanh⁡(s)=exp⁡(s)−exp⁡(−s)exp⁡(s)+exp⁡(−s)=2θ(2s)−1\\tanh(s)=\\frac{\\exp(s)-\\exp(-s)}{\\exp(s)+\\exp(-s)}=2\\theta(2s)-1tanh(s)=exp(s)+exp(−s)exp(s)−exp(−s)​=2θ(2s)−1，通常 tanh⁡(s)\\tanh(s)tanh(s) 更加受欢迎，因为符号函数不可求导，最优化比较困难。而 tanh⁡\\tanhtanh 函数既比较接近符号函数，又容易求导，此外 tanh⁡\\tanhtanh 函数和逻辑斯谛回归函数也有关系，从两者的关系可以看出 tanh⁡\\tanhtanh 的导数在原店附近更大，这也使得 tanh⁡\\tanhtanh 更加接近符号函数，优化效果更好。我们通常也不使用线性回归函数，因为中间层使用线性回归函数的话叠加起来还是线性回归函数，我们的神经网络层数的增加等于没有起到效果。当然还有很多其他的转换函数，不过本讲后面的转换函数都用 tanh⁡\\tanhtanh 函数为例。 整个网络 在神经网络模型中，我们给定一个 xxx 数据，通过多层的处理应该能够得到一个输出，这个过程中进行第一次处理的层数我们成为第一层，输入层不处理数据，我们称之为第 0 层，假设一个模型有 LLL 层，那么最后的输出就是第 LLL 层。这样，再定义了第 lll 层的权重 wij(l)w^{(l)}_{ij}wij(l)​ 后，就可以很容易定义出来分数 sj(l)s_j^{(l)}sj(l)​ 和转换后的结果 xj(l)x_j^{(l)}xj(l)​ ，当然每一层我们还会假如一个常数项 +1. 除了输入层和输出层的所有层又被我们称为隐藏层（hidden laters）。 物理意义 神经网络有没有什么物理解释？答案是有的。 每一层的分数都可以看作是该层的输入向量和权重的内积，内积我们知道与向量的余弦相似度密切相关。这样，如果我们把不同的权重向量看作不同的模型，内积就表示该层输入向量与相应模式的匹配程度，这也就是分数的意义，对分数做一些变换得到新的值某种程度上就可以理解为输入向量在这个特征上的表现。 3 Neural Network Learning 这一节讲神经网络模型参数的训练方法，也就是大名鼎鼎的向后传播算法。 这种方法是在最小化 Ein({wij(l)})E_{in}(\\{w_{ij}^{(l)}\\})Ein​({wij(l)​}) 的基础上建立的，正则化留待下一节考虑。记神经网络在 xnx_nxn​ 上的输出值为 NNet(xn)NNet(x_n)NNet(xn​)，采用二次损失函数 en=(yn−NNet(xn))2e_n=(y_n-NNet(x_n))^2en​=(yn​−NNet(xn​))2，可以通过梯度下降/随即梯度下降方法优化权重，不过这需要计算 ∂en∂wij(l)\\frac{\\partial e_n}{\\partial w_{ij}^{(l)}}∂wij(l)​∂en​​，在多层模型中这并不是很容易的事情。 我们使用下图中的网络进行演示，这个图表示输出层使用线性转换函数、中间层使用 tanh⁡\\tanhtanh 函数： 我们知道神经网络是一个线性模型的嵌套，这个图中输出层是线性函数，所以可以用符号把最后的模型表示为 NNet(xn)=wTϕ(2)(ϕ(1)(x))NNet(x_n)=w^T\\phi^{(2)}(\\phi^{(1)}(x))NNet(xn​)=wTϕ(2)(ϕ(1)(x))，可以看出，对于最外层的权重求导是容易的，但是对于内层的权重求导就总是不可避免要碰到外层的权重，这个时候我们容易想到可以链式求导法则。 对输出层的权重求导： ∂en∂wi1(L)=∂en∂s1(L)⋅∂s1(L)∂wi1(L)=−2(yn−s1(L))⋅(xiL−1)\\begin{aligned} &amp;\\ \\ \\ \\ \\ \\frac{\\partial e_n}{\\partial w_{i1}^{(L)}}\\\\&amp;=\\frac{\\partial e_n}{\\partial s_1^{(L)}}\\cdot\\frac{\\partial s_1^{(L)}}{\\partial w_{i1}^{(L)}}\\\\&amp;=-2(y_n-s_1^{(L)})\\cdot(x_i^{L-1}) \\end{aligned} ​ ∂wi1(L)​∂en​​=∂s1(L)​∂en​​⋅∂wi1(L)​∂s1(L)​​=−2(yn​−s1(L)​)⋅(xiL−1​)​ 对隐藏层的权重求导： ∂en∂wij(l)=∂en∂sj(l)⋅∂sj(l)∂wij(l)=δj(l)⋅(xj(l−1))\\begin{aligned} &amp;\\ \\ \\ \\ \\ \\frac{\\partial e_n}{\\partial w_{ij}^{(l)}}\\\\&amp;=\\frac{\\partial e_n}{\\partial s_j^{(l)}}\\cdot\\frac{\\partial s_j^{(l)}}{\\partial w_{ij}^{(l)}}\\\\&amp;=\\delta_j^{(l)}\\cdot(x_j^{(l-1)}) \\end{aligned} ​ ∂wij(l)​∂en​​=∂sj(l)​∂en​​⋅∂wij(l)​∂sj(l)​​=δj(l)​⋅(xj(l−1)​)​ 当 l=Ll=Ll=L 时，容易得到 δ1(L)=−2(yn−s1(L))\\delta_1^{(L)}=-2(y_n-s_1^{(L)})δ1(L)​=−2(yn​−s1(L)​)，当 l&lt;Ll&lt;Ll&lt;L 时，使用链式求导法则得到： δj(l)=∂en∂sj(l)=Σk=1d(l+1)∂en∂sk(l+1)⋅∂sk(l+1)∂xj(l)⋅∂xj(l)∂sj(l)=Σkδk(l+1)⋅wjk(l+1)⋅tanh⁡′(sj(l))\\begin{aligned} &amp;\\ \\ \\ \\ \\ \\delta_j^{(l)}\\\\&amp;=\\frac{\\partial e_n}{\\partial s_j^{(l)}}\\\\&amp;=\\Sigma_{k=1}^{d^{(l+1)}}\\frac{\\partial e_n}{\\partial s_k^{(l+1)}}\\cdot\\frac{\\partial s_k^{(l+1)}}{\\partial x_j^{(l)}}\\cdot\\frac{\\partial x_j^{(l)}}{\\partial s_j^{(l)}}\\\\&amp;=\\Sigma_{k}\\delta_k^{(l+1)}\\cdot w_{jk}^{(l+1)}\\cdot \\tanh&#x27;(s_j^{(l)}) \\end{aligned} ​ δj(l)​=∂sj(l)​∂en​​=Σk=1d(l+1)​∂sk(l+1)​∂en​​⋅∂xj(l)​∂sk(l+1)​​⋅∂sj(l)​∂xj(l)​​=Σk​δk(l+1)​⋅wjk(l+1)​⋅tanh′(sj(l)​)​ δj(l)\\delta_j^{(l)}δj(l)​ 可以逐步地算出来，最终得到更新公式是 wij(l)←wij(l)−η∂en∂wij(l)=wij(l)−ηxi(l−1)δj(l)w_{ij}^{(l)}\\leftarrow w_{ij}^{(l)}-\\eta\\frac{\\partial e_n}{\\partial w_{ij}^{(l)}}=w_{ij}^{(l)}-\\eta x_i^{(l-1)}\\delta_j^{(l)}wij(l)​←wij(l)​−η∂wij(l)​∂en​​=wij(l)​−ηxi(l−1)​δj(l)​ 最后总结向后传播算法的步骤： 上图中还提到了小批量梯度下降法，就是用不止一个点处的负梯度求平均作为更新方向，这种方法优点是能够更准确地指向极值所在的方向（减少一些迭代次数），找到全局最优解的概率更大，但是会增大每次迭代的计算量（但可以通过并行运算使得运算量不增大太多）。关于批量梯度下降法的内容参见 这个博客 4 Optimization and Regularization 向后传播算法是神经网络中最基本的训练模型的方法，它优化的函数是： Ein(w)=1NΣn=1Nerr((…tanh⁡(Σjwjk(2)⋅tanh⁡(Σiwij(1)xi))),yn)E_{in}(w)=\\frac1N\\Sigma_{n=1}^Nerr\\left(\\left(\\ldots \\tanh\\left(\\Sigma_jw_{jk}^{(2)}\\cdot\\tanh(\\Sigma_iw_{ij}^{(1)}x_i)\\right)\\right),y_n\\right) Ein​(w)=N1​Σn=1N​err((…tanh(Σj​wjk(2)​⋅tanh(Σi​wij(1)​xi​))),yn​) 当神经网络存在隐藏层时通常这是一个非凸函数，向后传播算法容易陷入局部最优解，我们希望能够提高找到最优解的可能性。 注意到 wij(l)w_{ij}^{(l)}wij(l)​ 的初始值的选择对其能否得到最佳解有很大影响，我们希望选择到更好的初始值。我们从（随机/小批量）梯度下降法的角度看这个问题，使用梯度下降法优化时，梯度绝对值越小的时候往往意味着我们离某个极值接近了，并且正在走向这个极值。如果这个极值是局部最优解，那么我们就容易陷入局部最优解。如果我们希望避免这个现象，那我们就需要从梯度绝对值较大的地方找初始点。假如中间层的转换函数是 tanh⁡\\tanhtanh，这个函数的特点是自变量 sss 在原点附近的时候导数绝对值很大，离原点远的时候绝对值很小，所以我们选择初始权重的时候应当使得 s=wTxs=w^Txs=wTx 比较小。 神经网络的 VC 维是多少？是否容易过拟合？ 我们不加证明地给出我们这一讲使用的神经网络模型的大致的 VC 维是 dVC=O(V⋅D)d_{VC}=O(V\\cdot D)dVC​=O(V⋅D)，其中 VVV 是神经元（节点）的数量，DDD 是权重的数量。 这说明我们通过增加节点和权重数量可以拟合非常复杂的模式、学习到非常复杂地特征，但是同时也很容易过拟合。 如何避免过拟合？ 最容易想到的一个方法是正则化，神经网络中我们最常用的正则项是 weight-elimination regularizer，又叫做 scaled L2 regularizer，公式是： Ω(w)=Σ(wij(l))21+(wij(l))2\\Omega(w)=\\Sigma\\frac{(w_{ij}^{(l)})^2}{1+(w_{ij}^{(l)})^2} Ω(w)=Σ1+(wij(l)​)2(wij(l)​)2​ 这个正则项的图像如下： 之所以选择这个正则项而不是之前常用的 L2 正则项，是因为这个正则项在权重绝对值较小时的梯度较大，这样的效果是对于绝对值较小的权重而言，缩小其绝对值对于优化目标仍然有比较大的收益，这样有利于把本身比较小的权重缩小到接近 0 ，从而减少权重数量，降低模型复杂度。而 L2 正则项在系数接近 0 时的导数很小，对绝对值比较大的权重惩罚力度明显更大，导致绝对值小的权重相对缩小的也少，整体基本上产生不了接近 0 的权重值。 除了过拟合以外，还有一个更加常被使用的避免过拟合的方法，那就是控制训练轮数。 这个方法的理论基础是这样的：GD/SGD 方法每次找的前进方向只是在当前位置一个很小的范围内的下降最快的方向，所以随着轮数的增加，探索到的范围不断变大，模型复杂度不断提升，所以控制探索的步数可以控制模型复杂度，降低过拟合的可能性。 上图中右边两个曲线图是理论上的 EinE_{in}Ein​ 与 EoutE_{out}Eout​ 的关系和一个神经网络实例中的 EinE_{in}Ein​ 与 EoutE_{out}Eout​ 的关系。可以看出实例中二者的关系和我们前面的分析基本吻合。 因此我们需要在神经网络的训练过程中选择合适的训练步数，这一点依靠的方法是 validation.","permalink":"http://yangtf983.github.io/2020/07/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%B3%95%E7%AC%94%E8%AE%B012%EF%BC%9ANeural%20Network/","photos":[]},{"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"title":"机器学习技法笔记13：Deep Learning","date":"2020/07/24","text":"0 说明 1 Deep Neural Network 2 Autoencoder 3 Denoising Antoencoder 4 Principal Component Analysis","permalink":"http://yangtf983.github.io/2020/07/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%B3%95%E7%AC%94%E8%AE%B013%EF%BC%9ADeep%20Learning/","photos":[]},{"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"title":"机器学习技法笔记11：Gradient Boosted Decision Tree","date":"2020/07/22","text":"0 说明 上一讲讲了随机森林，这是一种用 Bagging 的方式把决策树聚合起来的算法。 我们之前讲过 AdaBoost，这是一种专门针对二分类提出的算法，里面的数据加权和权重计算方法都是针对二分类的。这一讲首先要讲的是 AdaBoost 和决策树的结合，这是之前讲过的 AdaBoost-Stump 更加一般的情形，接着我们用最优化的角度推导 AdaBoost 的来源，这么做可以帮我们把 AdaBoost 算法延伸到更多问题上而不只是二分类问题。 这一讲讲完，聚合模型的内容也就告一段落，这一讲的最后会对讲过的聚合模型做一个总结。 这一讲是整个系列中最难的一讲。 1 AdaBoost Decision Tree 随机森林算法是把决策树 uniform 地放在一起用，现在考虑能不能用 AdaBoost 的方法把决策树 linear 地套在一起用，这就是这一节要提出的算法 AdaBoost Decision Tree，简称 AdaBoost-DTree . 以上一讲讲过的 C&amp;RT 为例，这里有两个问题需要解决：一个就是带权重数据的决策树如何训练？上一讲讲过没有权重的决策树如何训练，而 AdaBoost 是中的 gtg_tgt​ 是在权重数据上训练的，所以这里要使得决策树能够处理带权重的数据才能得到 gtg_tgt​ ；另一个是不同 gtg_tgt​ 的票数 αt\\alpha_tαt​ 怎么算？ 第一个问题：权重数据的训练 达到这个目的的通常做法有两种思路：一种是把原来算法优化的 EinE_{in}Ein​ 写出来 Ein(h)=1NΣerr(yn,h(xn))E_{in}(h)=\\frac1N \\Sigma err(y_n,h(x_n))Ein​(h)=N1​Σerr(yn​,h(xn​)) ，然后给对应位置乘上权重再优化 Einu(h)=1NΣn−1Nun⋅err(yn,h(xn))E_{in}^u(h)=\\frac1N \\Sigma_{n-1}^N u_n\\cdot err(y_n,h(x_n))Einu​(h)=N1​Σn−1N​un​⋅err(yn​,h(xn​)) 就得到加权数据的算法求解方法；另一种是算法不动，改变输入的数据，从中体现权重，例如使用 SGD 优化的算法可以按照权重的比例设置数据被选中的概率，这样就达到了加权的效果。 由于这里的 gtg_tgt​ 是决策树算法，这个算法的 EinE_{in}Ein​ 一般很复杂，而且这是一类算法，其中有各种各样的 EinE_{in}Ein​，所以第一种方法显然太过于复杂，我们希望用第二种方法达到加权的效果。 这里我们的做法是这样的，按照权重设置每个数据点被抽到的概率，然后从中可放回地抽取给定数量的样本作为训练样本，这个时候就把权重转化成了样本数量，决策树是可以直接用来训练有重复样本的数据的，所以不需要改变决策树算法和优化方法。 之所以不是直接把权重按照同时乘一个数变成正数，我推测原因是这样需要乘的数字可能太大。 这样就有 AdaBoost-DTree = AdaBoost + sampling ∝u(t)+\\propto \\mathbf{u}^{(t)}+∝u(t)+ DTree (D~t)\\left(\\tilde{\\mathcal{D}}_{t}\\right)(D~t​) 第二个问题：票数计算 若我们采用完全生成树和之前说的一般的权重计算公式 αt=ln⁡rt=ln⁡1−ϵtϵt\\alpha_t=\\ln \\mathbf{r}_t=\\ln \\sqrt{\\frac{1-\\epsilon_t}{\\epsilon_t}}αt​=lnrt​=lnϵt​1−ϵt​​​ 将面临一个问题，那就是在使用全部的数据训练时完全生成树的错误率是 0，所以权重 αt\\alpha_tαt​ 是无穷大。也就是说如果我们 sampling ∝u(t)\\propto \\mathbf{u}^{(t)}∝u(t) 得到的样本中原始数据都被包含，则会面临这个问题。 避免出现这个问题有两种方法：一种是控制样本，使得训练树的时候不要用所有的数据，这一点其实常常都是可以达到的，我们 sampling ∝u(t)\\propto \\mathbf{u}^{(t)}∝u(t) 得到的样本中一般都不会包含所有的样本；另一种是控制树的复杂度，比如剪枝，或者直接限制树的高度。 在 AdaBoost-DTree 中使用限制树的高度比剪枝更合适，虽然剪枝是采用验证的方法，容易得到更好的树，但是相对复杂，并且 AdaBoost 算法不需要非常好的 gtg_tgt​，剪枝方法常常用在单独的决策树训练问题中。 这样就得到 AdaBoost-DTree = AdaBoost + sampling ∝u(t)+\\propto \\mathbf{u}^{(t)}+∝u(t)+ pruned DTree (D~t)\\left(\\tilde{\\mathcal{D}}_{t}\\right)(D~t​) 这里的 pruned 不仅包含剪枝，还包含控制树的高度，并且常常都是指后者。 下面看一种极端情况，当决策树使用 C&amp;RT，高度限制小于等于 1 时，这时的决策树是什么？ 很容易看出来，当只在数据的某个特征上切一刀时，相当于 decision stump，这个时候就相当于用 AdaBoost 方法把 decision stump 聚合了起来，也就是上一讲最后一节提到的 AdaBoost-Stump，由于 decision stump 可以方便地把权重整合到 EinE_{in}Ein​ 中，所以这种方法不需要 sampling ∝u(t)\\propto \\mathbf{u}^{(t)}∝u(t) ，而是直接用加权的 EinE_{in}Ein​ 求最优解。 因此 AdaBoost-Stump 是AdaBoost-DTree 的一种特殊情况。 2 Optimization of AdaBoost 下面将换一种角度，从最优化的角度看 AdaBoost 算法，并使用这样的思想引出这一讲的主题——Gradient Boosted Decision Tree. 首先对数据权重的更新公式做一个改写，得到一个更加方便优化的形式： un(t+1)={un(t)⋅rt if incorrect un(t)/rt if correct =un(t)⋅rt−yngt(xn)=un(t)⋅exp⁡(−ynαtgt(xn))\\begin{aligned} u_{n}^{(t+1)} &amp;=\\left\\{\\begin{array}{ll} u_{n}^{(t)} \\cdot \\mathbf{r}_{t} &amp; \\text { if incorrect } \\\\ u_{n}^{(t)} / \\mathbf{r}_{t} &amp; \\text { if correct } \\end{array}\\right.\\\\ &amp;=u_{n}^{(t)} \\cdot \\mathbf{r}_{t}^{-y_{n} g_{t}\\left(x_{n}\\right)}\\\\&amp;=u_{n}^{(t)} \\cdot \\exp \\left(-y_{n} \\alpha_{t} g_{t}\\left(\\mathbf{x}_{n}\\right)\\right) \\end{aligned}un(t+1)​​={un(t)​⋅rt​un(t)​/rt​​ if incorrect if correct ​=un(t)​⋅rt−yn​gt​(xn​)​=un(t)​⋅exp(−yn​αt​gt​(xn​))​ 所以 unT+1=un(1)⋅∏t=1Texp⁡(ynαtgt(x))=1N⋅exp⁡(−ynΣt=1Tαtgt(xn))u_n^{T+1}=u_{n}^{(1)}\\cdot \\prod_{t=1}^T\\exp(y_n\\alpha_tg_t(x))=\\frac1N\\cdot\\exp(-y_n\\Sigma_{t=1}^T\\alpha_tg_t(x_n))unT+1​=un(1)​⋅∏t=1T​exp(yn​αt​gt​(x))=N1​⋅exp(−yn​Σt=1T​αt​gt​(xn​)) 回忆 AdaBoost 算法线性聚合后的模型是 G(x)=sign(Σt=1Tαtgt(xn))G(x)=sign(\\Sigma_{t=1}^T\\alpha_tg_t(x_n))G(x)=sign(Σt=1T​αt​gt​(xn​))，和上式中最后的整合形式中的指数部分有一些相同部分，我们称相同的部分 Σt=1Tαtgt(xn)\\Sigma_{t=1}^T\\alpha_tg_t(x_n)Σt=1T​αt​gt​(xn​) 为 {gt}\\{g_t\\}{gt​} 在 xxx 上的投票分数（voting score）。 所以 AdaBoost 中权重和投票分数有如下关系：un(T+1)∝exp⁡(−yn⋅(voting score on xn))u_n^{(T+1)}\\propto\\exp(-y_n\\cdot(voting\\ score\\ on\\ x_n))un(T+1)​∝exp(−yn​⋅(voting score on xn​)) 由于 G(x)=sign(Σt=1Tαtgt(xn))G(x)=sign(\\Sigma_{t=1}^T\\alpha_tg_t(x_n))G(x)=sign(Σt=1T​αt​gt​(xn​))，我们希望预测的结果和真实的类别 yny_nyn​ 一致，所以我们希望 yn⋅G(x)y_n\\cdot G(x)yn​⋅G(x) 是一个正数，即希望 yn⋅voting scorey_n\\cdot voting\\ scoreyn​⋅voting score 为正数，并且越大越好，越大说明这个结果越稳定，否则当数据稍微改变就可能使得判断的结果反转。 再根据 un(T+1)∝exp⁡(−yn⋅(voting score on xn))u_n^{(T+1)}\\propto\\exp(-y_n\\cdot(voting\\ score\\ on\\ x_n))un(T+1)​∝exp(−yn​⋅(voting score on xn​))，使得 yn⋅voting scorey_n\\cdot voting\\ scoreyn​⋅voting score 越大越好也就是让 un(T+1)u_n^{(T+1)}un(T+1)​ 越小越好。 下面要说明的就是随着 t 的增大，相应轮数数据的权重之和 Σn=1Nun(t)\\Sigma_{n=1}^Nu_n^{(t)}Σn=1N​un(t)​ 会越来越小： 在说明 Σn=1Nun(t)\\Sigma_{n=1}^Nu_n^{(t)}Σn=1N​un(t)​ 会越来越小之前，我们先来说明一下这个结论能说明什么？事实上通过使得 Σn=1Nun(t)\\Sigma_{n=1}^Nu_n^{(t)}Σn=1N​un(t)​ 变小我们可以间接缩小分类问题的 0/1 错误，即减小分类错误的概率。 假定已知 Σn=1Nun(t)\\Sigma_{n=1}^Nu_n^{(t)}Σn=1N​un(t)​ 会随着轮数的增加越来越小这个结论，记线性分数 s=Σt=1Tαtgt(xn)s=\\Sigma_{t=1}^T\\alpha_tg_t(x_n)s=Σt=1T​αt​gt​(xn​)，则 0/1 误差函数 err0/1(s,y)=[[ys&lt;0]]err_{0/1}(s,y)=[[ys&lt;0]]err0/1​(s,y)=[[ys&lt;0]]，由于 AdaBoost 达到的效果是最小化 Σn=1Nun(T+1)=1NΣn=1Nexp⁡(−ynΣt=1Tαtgt(xn))\\Sigma_{n=1}^Nu_n^{(T+1)}=\\frac1N\\Sigma_{n=1}^N\\exp(-y_n\\Sigma_{t=1}^T\\alpha_tg_t(x_n))Σn=1N​un(T+1)​=N1​Σn=1N​exp(−yn​Σt=1T​αt​gt​(xn​))，所以 AdaBoost 的误差函数可以定义为 err^ADA(s,y)=exp⁡(−ys)\\hat{err}_{ADA}(s,y)=\\exp(-ys)err^ADA​(s,y)=exp(−ys)，我们可以把这两个误差函数画在一个坐标系中，得到下面的图形： 从中我们可以看到优化 err^ADA(s,y)\\hat{err}_{ADA}(s,y)err^ADA​(s,y) 能够达到优化 err0/1err_{0/1}err0/1​ 的效果，即能够减小分类错误率。这说明了我们把 AdaBoost 的误差函数定义成 err^ADA(s,y)=exp⁡(−ys)\\hat{err}_{ADA}(s,y)=\\exp(-ys)err^ADA​(s,y)=exp(−ys) 的合理之处，也说明了我们用优化的观点看待 AdaBoost 的合理之处。下面我们只需要证明 AdaBoost 确实能够最小化 Σn=1Nun(t)\\Sigma_{n=1}^Nu_n^{(t)}Σn=1N​un(t)​ 即可。 现在我们回到这个中心问题，证明 E^ADA=Σn=1Nun(t)=1NΣn=1Nexp⁡(−ynΣτ=1t−1ατgτ(xn))\\hat{E}_{ADA}=\\Sigma_{n=1}^Nu_n^{(t)}=\\frac1N\\Sigma_{n=1}^N\\exp(-y_n\\Sigma_{\\tau=1}^{t-1}\\alpha_{\\tau}g_{\\tau}(x_n))E^ADA​=Σn=1N​un(t)​=N1​Σn=1N​exp(−yn​Στ=1t−1​ατ​gτ​(xn​)) 会随着 T 的增大而减小。我们用到的方法是梯度下降法（GD），原理是对目标函数做一次泰勒展开 min⁡∥v∥=1Ein(wt+ηv)≈Ein(wt)+ηvT∇Ein(wt)\\min_{\\|v\\|=1}E_{in}(w_t+\\eta v)\\approx E_{in}(w_t)+\\eta v^T \\nabla E_{in}(w_t)min∥v∥=1​Ein​(wt​+ηv)≈Ein​(wt​)+ηvT∇Ein​(wt​) ，之后找到使得函数值最小的步长 η\\etaη ，之后在新的位置接着一阶泰勒展开。 根据梯度下降算法的思想，现在我们的优化问题可以改写作 min⁡hE^ADA=1NΣn=1Nexp⁡(−yn(Στ−1t−1ατgτ(xn)+ηh(xn)))\\min_{h}\\hat{E}_{ADA}=\\frac1N\\Sigma_{n=1}^N\\exp(-y_n(\\Sigma_{\\tau-1}^{t-1}\\alpha_{\\tau}g_{\\tau}(x_n)+\\eta h(x_n)))minh​E^ADA​=N1​Σn=1N​exp(−yn​(Στ−1t−1​ατ​gτ​(xn​)+ηh(xn​))) ，其中 ατ, gτ\\alpha_{\\tau},\\ g_{\\tau}ατ​, gτ​ 是已经得到的部分，也是要进行泰勒展开的位置，η\\etaη 物理意义是步长，在这里代表下一个 gtg_tgt​ 的权重，h(x)h(x)h(x) 物理意义是移动方向，在这里就代表下一个 gtg_tgt​，之所以能用函数代表方向，是因为给定一些 xnx_nxn​ 后函数值可以拼接成一个列向量代表方向。上面的公式可以理解为：当前的泰勒展开位置是 (Στ−1t−1ατgτ(x1),…,Στ−1t−1ατgτ(xN))\\left(\\Sigma_{\\tau-1}^{t-1}\\alpha_{\\tau}g_{\\tau}(x_1),\\ldots,\\Sigma_{\\tau-1}^{t-1}\\alpha_{\\tau}g_{\\tau}(x_N)\\right)(Στ−1t−1​ατ​gτ​(x1​),…,Στ−1t−1​ατ​gτ​(xN​)) ，下一次的移动方向是 (h(x1),…,h(xN))\\left(h(x_1),\\ldots,h(x_N)\\right)(h(x1​),…,h(xN​)) （未正规化过的方向），步长是 η\\etaη . 根据 un(t)=1Nexp⁡(−ynΣτ=1t−1ατgτ(xn))u_n^{(t)}=\\frac1N\\exp(-y_n\\Sigma_{\\tau=1}^{t-1}\\alpha_{\\tau}g_{\\tau}(x_n))un(t)​=N1​exp(−yn​Στ=1t−1​ατ​gτ​(xn​)) 和指数函数在原点的泰勒展开公式可以对最优化的公式做如下的改写： min⁡hE^ADA=1NΣn=1Nexp⁡(−yn(Στ−1t−1ατgτ(xn)+ηh(xn)))=Σn=1Nun(t)exp⁡(−ynηh(xn))≈taylorΣn=1Nun(t)(1−ynηh(xn))=Σn=1Nun(t)−ηΣn=1Nun(t)ynh(xn)\\begin{aligned}\\min_{h}\\hat{E}_{ADA}&amp;=\\frac1N\\Sigma_{n=1}^N\\exp(-y_n(\\Sigma_{\\tau-1}^{t-1}\\alpha_{\\tau}g_{\\tau}(x_n)+\\eta h(x_n))) \\\\ &amp;=\\Sigma_{n=1}^N u_n^{(t)}\\exp(-y_n\\eta h(x_n))\\\\&amp;\\stackrel{taylor}{\\approx}\\Sigma_{n=1}^Nu_n^{(t)}(1-y_n\\eta h(x_n))\\\\&amp;=\\Sigma_{n=1}^Nu_n^{(t)}-\\eta\\Sigma_{n=1}^Nu_n^{(t)}y_nh(x_n)\\end{aligned} hmin​E^ADA​​=N1​Σn=1N​exp(−yn​(Στ−1t−1​ατ​gτ​(xn​)+ηh(xn​)))=Σn=1N​un(t)​exp(−yn​ηh(xn​))≈taylorΣn=1N​un(t)​(1−yn​ηh(xn​))=Σn=1N​un(t)​−ηΣn=1N​un(t)​yn​h(xn​)​ 去掉已知部分，现在优化目标等价于 min⁡hΣn=1Nun(t)(−ynh(xn))\\min_{h}\\Sigma_{n=1}^Nu_n^{(t)}(-y_nh(x_n))minh​Σn=1N​un(t)​(−yn​h(xn​)) 对于二分类问题，yn, h(xn)y_n,\\ h(x_n)yn​, h(xn​) 的取值范围都是 +1 或 -1，这时的优化目标可以进一步改写： ∑n=1Nun(t)(−ynh(xn))=∑n=1Nun(t){−1 if yn=h(xn)+1 if yn≠h(xn)=−∑n=1Nun(t)+∑n=1Nun(t){0 if yn=h(xn)2 if yn≠h(xn)=−∑n=1Nun(t)+2Ein u(t)(h)\\begin{aligned} \\sum_{n=1}^{N} u_{n}^{(t)}\\left(-y_{n} h\\left(\\mathbf{x}_{n}\\right)\\right) &amp;=\\sum_{n=1}^{N} u_{n}^{(t)}\\left\\{\\begin{aligned} -1 &amp; \\text { if } y_{n}=h\\left(\\mathbf{x}_{n}\\right) \\\\ +1 &amp; \\text { if } y_{n} \\neq h\\left(\\mathbf{x}_{n}\\right) \\end{aligned}\\right.\\\\ &amp;=-\\sum_{n=1}^{N} u_{n}^{(t)}+\\sum_{n=1}^{N} u_{n}^{(t)}\\left\\{\\begin{array}{ll} 0 &amp; \\text { if } y_{n}=h\\left(\\mathbf{x}_{n}\\right) \\\\ 2 &amp; \\text { if } y_{n} \\neq h\\left(\\mathbf{x}_{n}\\right) \\end{array}\\right.\\\\ &amp;=-\\sum_{n=1}^{N} u_{n}^{(t)}+2 E_{\\text {in }}^{\\mathbf{u}^{(t)}}(h) \\end{aligned}n=1∑N​un(t)​(−yn​h(xn​))​=n=1∑N​un(t)​{−1+1​ if yn​=h(xn​) if yn​​=h(xn​)​=−n=1∑N​un(t)​+n=1∑N​un(t)​{02​ if yn​=h(xn​) if yn​​=h(xn​)​=−n=1∑N​un(t)​+2Ein u(t)​(h)​ 这样我们就等于是求解使得 Ein u(t)(h)E_{\\text {in }}^{\\mathbf{u}^{(t)}}(h)Ein u(t)​(h) 最小的 h(x)h(x)h(x)，这恰好是 AdaBoost 算法中的基本算法 gtg_tgt​ 的目标，所以我们基本证明了原来的 AdaBoost 算法（在二分类情形下）是在优化 min⁡hE^ADA\\min_{h}\\hat{E}_{ADA}minh​E^ADA​ 说基本证明是因为我们还想对步长 η\\etaη 做一些说明。 梯度下降法的步长一般取常数，这有时候收敛会很慢，可以试想，这个时候的步长就是我们的 gtg_tgt​ 的权重，我们的数据不是自助法样本（地位不平等），但是要取一样的权重，这样很可能收敛效果很差。所以这里我们希望每一步计算一个最佳步长，也就是给每一步一个最合适的权重。这种方法被称为最速下降法（steepest descent）。 最速下降法与梯度下降法一样都使用一阶泰勒展开，但是不同之处在于梯度下降法使用固定的步长而最速下降法使用该方向上最合适的步长。容易理解梯度下降法降低了每一步求解的复杂度，而最速下降法则降低了迭代次数。究竟一个算法使用哪一种方法最合适需要具体问题具体分析。 我们记第 t 轮的步长为 ηt\\eta_tηt​，第 t 轮的优化方向 h(x)h(x)h(x) 假设已经通过基本算法找到了，这样我们原来的优化目标就改写为：E^ADA=Σn=1Nun(t)exp⁡(−ynηtgt(xn))\\hat{E}_{ADA}=\\Sigma_{n=1}^Nu_n^{(t)}\\exp(-y_n\\eta_tg_t(x_n))E^ADA​=Σn=1N​un(t)​exp(−yn​ηt​gt​(xn​)) ，我们寻找最佳步长也就是在做优化问题：min⁡ηtΣn=1Nun(t)exp⁡(−ynηtgt(xn))\\min_{\\eta_t}\\Sigma_{n=1}^Nu_n^{(t)}\\exp(-y_n\\eta_tg_t(x_n))minηt​​Σn=1N​un(t)​exp(−yn​ηt​gt​(xn​)) 当分类正确时，yn⋅gt(xn)=1y_n\\cdot g_t(x_n)=1yn​⋅gt​(xn​)=1，否则 yn⋅gt(xn)≠−1y_n\\cdot g_t(x_n)\\ne-1yn​⋅gt​(xn​)​=−1，并记错误率为 ϵt\\epsilon_tϵt​ ,可以进一步得到： E^ADA=Σn=1Nun(t)exp⁡(−ynηtgt(xn))=Σcorrect xnexp⁡(−ϵ)+Σincorrect xnexp⁡(+ϵ)=(Σn=1Nun(t))⋅((1−ϵt)⋅exp⁡(−ηt)+ϵt⋅exp⁡(+ηt))\\begin{aligned}\\hat{E}_{ADA}&amp;=\\Sigma_{n=1}^Nu_n^{(t)}\\exp(-y_n\\eta_tg_t(x_n))\\\\&amp;=\\Sigma_{correct\\ x_n}\\exp(-\\epsilon)+\\Sigma_{incorrect\\ x_n}\\exp(+\\epsilon)\\\\&amp;=(\\Sigma_{n=1}^Nu_n^{(t)})\\cdot((1-\\epsilon_t)\\cdot\\exp(-\\eta_t)+\\epsilon_t\\cdot \\exp(+\\eta_t))\\end{aligned} E^ADA​​=Σn=1N​un(t)​exp(−yn​ηt​gt​(xn​))=Σcorrect xn​​exp(−ϵ)+Σincorrect xn​​exp(+ϵ)=(Σn=1N​un(t)​)⋅((1−ϵt​)⋅exp(−ηt​)+ϵt​⋅exp(+ηt​))​ 注意我们这个时候的基本算法就是对加权数据做的分类，所以错误点的权重之和除以总权重才是 ϵt\\epsilon_tϵt​，而不是错误点数量除以总权重。 通过求解 ∂E^ADA∂ηt=0\\frac{\\partial\\hat{E}_{ADA}}{\\partial\\eta_t}=0∂ηt​∂E^ADA​​=0 容易得到 ηt=ln⁡1−ϵtϵt=αt\\eta_t=\\ln\\sqrt{\\frac{1-\\epsilon_t}{\\epsilon_t}}=\\alpha_tηt​=lnϵt​1−ϵt​​​=αt​ ，即上一讲中 AdaBoost 算法使用的权重。 到这里我们就从最优化的角度讲解了 AdaBoost 算法背后的道理，上一讲省略的权重公式的推导过程也讲解了。 3 Gradient Boosting AdaBoost 的架构是专门用来做二分类的，上一节我们从最优化的角度推导了 AdaBoost 算法的由来，我们的优化目标是： min⁡ηmin⁡h1NΣn=1Nexp⁡(−yn(Στ=1t=1ατgτ(xn)+ηh(xn)))\\min_{\\eta}\\min_{h}\\frac1N\\Sigma_{n=1}^N\\exp(-y_n(\\Sigma_{\\tau=1}^{t=1}\\alpha_{\\tau}g_{\\tau}(x_n)+\\eta h(x_n))) ηmin​hmin​N1​Σn=1N​exp(−yn​(Στ=1t=1​ατ​gτ​(xn​)+ηh(xn​))) 从最优化的角度来说，我们可以把 err^ADA\\hat{err}_{ADA}err^ADA​ 换成任何误差函数，也就是说上面的最优化问题不过是下面这个最优化问题的一个特例： min⁡ηmin⁡h1NΣn=1Nerr(Στ=1t−1αtgt(xn)+ηh(xn),yn)\\min_{\\eta}\\min_{h}\\frac1N\\Sigma_{n=1}^Nerr(\\Sigma_{\\tau=1}^{t-1}\\alpha_tg_t(x_n)+\\eta h(x_n),y_n) ηmin​hmin​N1​Σn=1N​err(Στ=1t−1​αt​gt​(xn​)+ηh(xn​),yn​) 这个最优化问题对应的算法我们称之为 GradientBoost，AdaBoost 是它的一种情形，通常情况下我们使用 GradientBoost 时的假设都是实数输出值，例如回归问题、软分类问题等等。下面我们就讲一下在回归问题上的应用。 GradientBoost for Regression 回归问题的误差函数 err(s,y)=(s−y)2err(s,y)=(s-y)^2err(s,y)=(s−y)2 这里记 sn=Στ=1t−1αtgt(xn)s_n=\\Sigma_{\\tau=1}^{t-1}\\alpha_tg_t(x_n)sn​=Στ=1t−1​αt​gt​(xn​)，我们的最优化问题变成 min⁡ηmin⁡h1NΣn=1Nerr(sn+ηh(xn),yn)\\min_{\\eta}\\min_{h}\\frac1N\\Sigma_{n=1}^Nerr(s_n+\\eta h(x_n),y_n)minη​minh​N1​Σn=1N​err(sn​+ηh(xn​),yn​) ，求解这个最优化问题我们需要先对它进行一阶泰勒展开，展开的初始点是 sns_nsn​，这个过程如下： min⁡h…≈taylormin⁡h1NΣn=1Nerr(sn,yn)+1NΣn=1Nηh(xn)∂err(s,yn)∂s∣s=sn=min⁡hconstants+ηNΣn=1Nh(xn)⋅2(sn−yn)\\begin{aligned}\\min_{h}\\ldots&amp;\\stackrel{taylor}{\\approx}\\min_h\\frac1N\\Sigma_{n=1}^Nerr(s_n,y_n)+\\frac1N\\Sigma_{n=1}^N\\eta h(x_n)\\frac{\\partial err(s,y_n)}{\\partial s}|_{s=s_n}\\\\&amp;=\\min_h constants+\\frac{\\eta}{N}\\Sigma_{n=1}^Nh(x_n)\\cdot2(s_n-y_n)\\end{aligned} hmin​…​≈taylorhmin​N1​Σn=1N​err(sn​,yn​)+N1​Σn=1N​ηh(xn​)∂s∂err(s,yn​)​∣s=sn​​=hmin​constants+Nη​Σn=1N​h(xn​)⋅2(sn​−yn​)​ 从这里看首先 h(xn)h(x_n)h(xn​) 和 sn−yns_n-y_nsn​−yn​ 需要异号，因为异号得到的结果（负数）一定比同号得到的结果（正数）小，其次 h(xn)h(x_n)h(xn​) 的绝对值越大越好，取到无穷大似乎是最好的。但是我们应该记得，(h(x1),…,h(xn))(h(x_1),\\ldots,h(x_n))(h(x1​),…,h(xn​)) 仅仅代表方向，还有一个与之相乘的代表步长的变量 η\\etaη 需要被优化，所以这里 (h(x1),…,h(xn))(h(x_1),\\ldots,h(x_n))(h(x1​),…,h(xn​)) 应当控制模长，仅仅表示方向，步长交给 η\\etaη 控制 (h(x1),…,h(xn))(h(x_1),\\ldots,h(x_n))(h(x1​),…,h(xn​)) 的模长，我们首先想到的一种方法是在 ∥h∥=1\\|h\\|=1∥h∥=1 的限制条件下求最优解，但是这样求解非常复杂，所以我们换一个思路，可以使用正则化的方法，把模长作为一个正则项，这样放松了对模长的控制，但是没关系，只要最优解的模长是有限值就都可以用 η\\etaη 调整。 按照这种思路我们加入 L2 正则项后问题发生如下变化： min⁡h constants+ηNΣn=1N(2h(xn)⋅(sn−yn)+h2(xn))=constants+ηNΣn=1N(constant+(h(xn)−(yn−sn))2)\\begin{aligned}\\min_h&amp;\\ \\ \\ constants+ \\frac{\\eta}{N}\\Sigma_{n=1}^N(2h(x_n)\\cdot(s_n-y_n)+h^2(x_n))\\\\&amp;=constants+\\frac{\\eta}{N}\\Sigma_{n=1}^N(constant+(h(x_n)-(y_n-s_n))^2)\\end{aligned} hmin​​ constants+Nη​Σn=1N​(2h(xn​)⋅(sn​−yn​)+h2(xn​))=constants+Nη​Σn=1N​(constant+(h(xn​)−(yn​−sn​))2)​ 这个问题求最优解等价于求 min⁡hΣn=1N(h(xn)−(yn−sn))2\\min_h \\Sigma_{n=1}^N(h(x_n)-(y_n-s_n))^2minh​Σn=1N​(h(xn​)−(yn​−sn​))2，注意这个问题，yn−sny_n-s_nyn​−sn​ 是前 n 次基本模型的聚合模型的残差，这个最优化问题的解恰好是在数据集 {(xn,yn−sn)}\\{(x_n,y_n-s_n)\\}{(xn​,yn​−sn​)} 上做回归问题的优化目标，所以我们从求解最优化问题 min⁡ηmin⁡h1NΣn=1Nerr(sn+ηh(xn),yn),where err(s,y)=(s−y)2\\min_{\\eta}\\min_{h}\\frac1N\\Sigma_{n=1}^Nerr(s_n+\\eta h(x_n),y_n),where\\ err(s,y)=(s-y)^2minη​minh​N1​Σn=1N​err(sn​+ηh(xn​),yn​),where err(s,y)=(s−y)2 一步步得到的结果是一个把回归模型用 linear 方式聚合起来的聚合模型。 可以看出这个模型和 AdaBoost 的不同之处，AdaBoost 不同轮数的基本算法的使用区别在于数据的权重不同，而 GradientBoost for Regression 不同轮数的基本模型的区别在于回归的响应变量值不同。 现在 GradientBoost for Regression 还剩下最后一个问题，那就是权重如何确定？ 此时我们已经通过对上一步模型的残差做回归求出来了第 t 轮的基本模型 gtg_tgt​，现在的优化目标就变成了： min⁡η 1NΣn=1Nerr(sn+ηgt(xn),yn)=1NΣn=1N(sn+ηgt(xn)−yn)2=1NΣn=1N((yn−sn)−ηgt(xn))2\\begin{aligned}\\min_{\\eta}&amp;\\ \\ \\ \\frac1N\\Sigma_{n=1}^N err(s_n+\\eta g_t(x_n),y_n)\\\\&amp;=\\frac1N\\Sigma_{n=1}^N(s_n+\\eta g_t(x_n)-y_n)^2\\\\&amp;=\\frac1N\\Sigma_{n=1}^N((y_n-s_n)-\\eta g_t(x_n))^2\\end{aligned} ηmin​​ N1​Σn=1N​err(sn​+ηgt​(xn​),yn​)=N1​Σn=1N​(sn​+ηgt​(xn​)−yn​)2=N1​Σn=1N​((yn​−sn​)−ηgt​(xn​))2​ 从中可以看出，最优 η\\etaη 其实是在数据集 {(gt(xn),yn−sn)}\\{(g_t(x_n),y_n-s_n)\\}{(gt​(xn​),yn​−sn​)} 上做无截距的线性回归的系数。 把这些内容放到一起就得到一个新的算法——Gradient Boosted Decision Tree (GBDT)： 这个算法基本上就是这一节前面讲的流程，只是把在 {(xn,yn−sn)}\\{(x_n,y_n-s_n)\\}{(xn​,yn​−sn​)} 上做回归这一步改成了 C&amp;RT 决策树，其实决策树也可以看成是回归的一种，尤其是接近完全生成树的时候。 这个算法与 AdaBoost 很像，区别在于两点：第一点是用残差做决策树而不是用加权样本；第二点是权重的计算方法有不同。这个模型的应用非常广泛，尤其是在信息检索和网页排序方面。 4 Summary of Aggregation 最后复习一下学习过的聚合模型。 首先我们说明了模型聚合方法大致分为三种：uniform, non-uniform 和 conditional ，三者的区别在于使用什么样的方式把基本模型整合起来。uniform 的聚合方法的好处在于能够得到一个稳定的模型，后两种模型的好处在于功能更强，但是要小心过拟合。 针对三种聚和方式我们给出了几种具有代表性的方法如上图。 除了聚合模型，还有能够将聚合模型聚合起来的方法，我们一共提到过三种：随机森林、AdaBoost-DTree 和 GBDT，这样做的目的是兼顾聚合模型的能力强和稳定性强两个优点，比如随机森林内层使用功能强的聚合模型，外层使用增强稳定性的聚合模型。 聚合模型即可以增强模型的能力，避免欠拟合，又可以提高模型稳定性，避免过拟合，合理使用聚合模型可以做出非常有用的模型。","permalink":"http://yangtf983.github.io/2020/07/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%B3%95%E7%AC%94%E8%AE%B011%EF%BC%9AGradient%20Boosted%20Decision%20Tree/","photos":[]},{"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"title":"机器学习技法笔记10：Random Forest","date":"2020/07/19","text":"0 说明 我们已经学了 uniform 的聚合算法 bagging 、linear 的聚合算法 AdaBoost 和 conditional 的聚合算法决策树。 这一讲要讲的随机森林可以把聚合算法融合起来，可以称之为聚合的聚合（aggregation of aggregation），更具体地说，是用 bagging 的方式把决策树聚合起来。 1 Random Forest Algorithm 随机森林的目标是把 bagging 模型和决策树模型结合在一起，综合其长处得到更好的模型。首先来复习一下这两种聚合模型： bagging 的主要思想是利用 bootstrap 样本得到不同的 gtg_tgt​，由于自助法样本地位相同，所以这样得到的 gtg_tgt​ 也应当具有相同的票数，所以采用 uniform 的方法聚合模型。bagging 的好处是通过聚合降低了用单个模型预测的不稳定性（方差）。 决策树是一种条件聚合模型，它具有很多优点，我们已经总结在上一讲的结尾处。但是同时决策树也有一个明显的缺点，那就是方差比较大/不稳定。例如当我们改变某个数据点导致了某一步的决策边界发生变化，那么后面的所有子树都可能发生变化，这种不稳定性随着决策树的层数增加而增大。 我们希望能够保留决策树的诸多优点，但是又希望降低决策树的不稳定性，所以诞生了随机森林算法，这种算法的思想是用 bagging 的方式把 C&amp;RT 决策树聚合起来，达到提升稳定性的作用。 首先看一下随机森林的基本流程： 如上图所示，随机森林的做法是从原始数据中生成自助法样本，接着分别用这些样本训练 C&amp;RT 决策树（并且是完全生成树），最后用得到的所有树投票做决定。 随机森林有三个特点： 决策树算法效率很高，所以做很多决策树效率依然可以保证； 每个决策树都仍然保留了 C&amp;RT 决策树的优点； 通过很多不同决策树投票的方式消除了完全生成树不稳定的缺点。 再来看随机森林中得到不同决策树的方式。上面的流程中是用自助法样本得到不同的决策树，实际上还有其他方法得到不同的决策树，例如随机选择特征： 如上图所示，我们可以每次随机选择 d′d&#x27;d′ 个特征训练 C&amp;RT 决策树，这样做的好处是能够得到更加不一样的 gtg_tgt​，事实上，随机森林的提出者建议的做法就是除了使用自助法样本外，每次都随机选择一些特征训练 C&amp;RT 决策树。并且他还提出，我们随机选择一些特征得到的不过是原来的数据在子空间中的投影，并且还是投影到自然基底上。根据这种想法，我们可以把原数据投影到子空间的非自然基底上，这样就又大大扩充了决策树的多样性。 达到这一目的的做法是用投影矩阵 P 对数据做变换，为了保证投影后的数据是在 d′′d&#x27;&#x27;d′′ 维子空间的，投影矩阵应当只有 d′′d&#x27;&#x27;d′′ 行是非零的。 这样，我们终于把随机森林中可以随机的部分说完了，上图中最下面做了一个总结，最终的随机森林就是： RF = bagging + random-combination C&RT 其中，bagging 指的是使用自助法样本和 uniform 聚合，random-combination 指的是随机选择子空间内的特征，C&RT 指的是使用的决策树种类。 最后看一个问题： 这个问题问的是在使用 random-combination 特征做的 C&amp;RT 决策树时，b(x)b(x)b(x) 是在做什么动作？答案是感知机。因为使用了 random-combination 的特征后，我们的 stump decision 就像是在原来特征的线性组合上面寻找划分的阈值。 2 Out-of-bag Estimate bagging 中使用的自助法样本由于是放回式抽样，所以一般每个自助法样本中都存在一些原来的样本没有被选中，由于这些样本是没有被 bagging 算法选中的，所以我们称之为 gtg_tgt​ 的 out-of-bag 的样本，也就是下图中标星号的样本： OOB 样本有什么用？直接的想法是，OOB 样本是没有被 gtg_tgt​ 污染过的样本，所以可以用来做验证，但是实际上我们不需要 gtg_tgt​ 的验证，因为聚合模型并不要求每一个基本模型表现得很好，我们希望的是能够用 OOB 样本对 GGG 验证，关于这一点我们分两步来谈： 首先看一下 OOB 样本的数量能否用来做验证。假设每个自助法样本中有 N 个样本（等于原始数据集中数据的量），那么对于某个自助法样本而言，某个数据是 OOB 样本的概率是：(1−1N)N=1(NN−1)N→1e, when N→∞(1-\\frac1N)^N=\\frac{1}{(\\frac{N}{N-1})^N}\\rightarrow \\frac1e,\\ when\\ N\\rightarrow \\infty(1−N1​)N=(N−1N​)N1​→e1​, when N→∞ ，因此 N 个样本中的 OOB 样本的数量大约是 Ne\\frac{N}{e}eN​ ，也就是大约样本数量的三分之一，足够做验证。 其次，要用 OOB 样本对 GGG 做验证，就要求数据不能被污染。使用所有 gtg_tgt​ 都没选到的样本做验证显然不现实，实际的做法如下：对于数据 xnx_nxn​，把不包含它作为训练集的决策树都找出来构成一个决策树 Gn−G^-_nGn−​，这样对每个样本都可以找到一个 Gn−G^-_nGn−​，用 Gn−G^-_nGn−​ 预测 xn, n=1,...,Nx_n,\\ n=1,...,Nxn​, n=1,...,N，得到的误差的平均作为对 GGG 的验证，见下图： 总结一下 RF 中通过验证进行模型选择的方法：通过调整 RF 中的参数（例如特征选择的数量 d′′d&#x27;&#x27;d′′）得到不同的 RF 模型，通过验证选择表现最好的 RF 模型作为最终的模型。 这个验证和之前所讲的验证有一些不同：不需要划分训练集和验证集、最后不需要合并数据重新训练。 实际问题中，EoobE_{oob}Eoob​ 是一种相当准确的衡量 GGG 表现的方法。 3 Feature Selection 下面讲一下 RF 的另一个重要应用：特征选择。 先从特征选择讲起。有时候我们的数据维度很大，但是有一些维度去掉后对分类精确度没有明显的影响，这些维度大致分为两种：一种是冗余特征，像年龄和生日，只需要保留一个，其他的就是冗余特征；另一种是无关特征，像保险类型和癌症诊断，了解前者对后者没有帮助。这两种特征都是我们希望删除的，我们的目的是从 d 维数据中想办法只保留 d′d&#x27;d′ 维对分类有用的特征。 做特征选择有优点也有缺点。**优点是：**提高后续模型训练的效率；减少数据中的噪声，进而提高模型的推广能力；精简特征有利于提高模型的可解释性。**缺点是：**从大量特征中选择少量有用的特征是一个计算量很大的过程；有可能不小心排除掉了好的特征而选择了不好的特征，造成过拟合；如果特征选择有问题，会对模型的可解释性造成很大的影响，导致出现错误的解释。 下面我们从特征选择的方法讲起，先介绍一个其他的特征选择方法，然后引入这一讲的内容——如何用 RF 做特征选择。 我们要介绍的特征选择方法是根据重要性选择特征。如果我们能计算出每一个特征的重要性 importance(i) for i=1,2,...,dimportance(i)\\ for\\ i=1,2,...,dimportance(i) for i=1,2,...,d，那么只需要选择重要性最高的 d′d&#x27;d′ 个特征留下来即可。这种方法在线性模型中很容易实施，只需要保留线性系数绝对值最大的 d′d&#x27;d′ 个特征即可，但是在非线性模型中很难实施，因为涉及到模型的选择等等问题，系数的大小不能反映特征重要性。 但是，如果是在 RF 上，这种方法也具有很好的实用性。 我们的想法是，如果特征 iii 很重要，那么给这个特征加入噪声后，对最后结果的表现影响应该相应的也会更大。 我们不希望加入噪声改变原来这个特征的数据分布，因为这可能对模型产生一些难以预料的影响，这里我们采取的做法是把相应的自助法样本在这个特征上的数值打乱后进行训练决策树，然后把打乱前后的表现进行对比，表现变差的程度就是相应特征的重要性。这种方法我们称之为 permutation test，这个方法展示在下图： permutation test 是一种常用的统计工具，可以用在任意的非线性模型中测试某个特征的重要性。 由于我们这里的非线性模型是随机森林，所以我们的表现也自然而然地可以用 EoobE_{oob}Eoob​ 衡量，重要性就变成 importance(i)=performance(D)−performance(D(p))=Eoob(G)−Eoob(G(p))importance(i)=performance(\\mathcal{D})-performance(\\mathcal{D}^{(p)})=E_{oob}(G)-E_{oob}(G^{(p)})importance(i)=performance(D)−performance(D(p))=Eoob​(G)−Eoob​(G(p))，这个公式中 Eoob(G(p))E_{oob}(G^{(p)})Eoob​(G(p)) 这里还需要对每一个排列组合后的数据进行重新训练，实际应用中这一点可以避免，我们常常直接用排列组合后的数据在 GGG 上面测试表现，并且我们仅仅在对应的 OOB 数据的相应特征上进行排列组合以避免数据污染。这样我们的重要性度量公式就变成：importance(i)=Eoob(G)−Eoob(G(p))importance(i)=E_{oob}(G)-E_{oob}(G^{(p)})importance(i)=Eoob​(G)−Eoob​(G(p)) 当特征数量比较大时，这种特征选择方法是常常会被用到的。 4 Random Forest in Action 还记得我们在这一讲开始说的随机森林的目的吗？用 bagging 降低决策树的不稳定性。 下面通过三个例子来看一下这个问题有没有被解决。 第一个例子 以下每张图都会按顺序分别展示使用 C&amp;RT 决策的边界、第 t 轮自助法样本下的决策边界、前 t 轮 gtg_tgt​ 综合得到的 GGG 从这个例子中可以看出：C&amp;RT 决策树的边界不够稳定，比如下面的分界线几乎贴着一个红色类别的点却离其他点特别远，gtg_tgt​ 的分界线也很不稳定，每次的差别很大，但是随机森林的分界线却是很接近“最大线宽”分界线的，稳定性可以用和 SVM 一样的理论保证。 第二个例子 把边界进一步复杂化。 以下每张图都会按顺序分别展示第 t 轮自助法样本下的决策边界、前 t 轮 gtg_tgt​ 综合得到的 GGG 从图中可以看到，虽然每一轮 gtg_tgt​ 的变化依然很大，但是 GGG 的变化越来越小，很稳定，并且边界也接近最大线宽。 第三个例子 下面进一步增加难度，在第二个例子本就复杂的数据上再增加一些噪声，情况如下： 从图中可以看出，增加了噪声之后依然可以找到和原来很接近的分类边界，只是需要的迭代轮数有所增加。第二个例子中从第 11 轮开始基本上就看不到什么变化了，第三个例子中的主体部分虽然从第 11 轮开始也看不到什么变化了，但是噪声变量处有很多是在后面才剔除的。 到底需要多少颗树？ 那么随机森林多少棵树比较好？这个问题没有比较标准的答案，只能说越多越好。究竟需要多多一般需要做一些检查，例如像上面三个例子一样画出图形进行检查，或者用 OOB 验证等。","permalink":"http://yangtf983.github.io/2020/07/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%B3%95%E7%AC%94%E8%AE%B010%EF%BC%9ARandom%20Forest/","photos":[]},{"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"title":"机器学习技法笔记9：Decision Tree","date":"2020/07/18","text":"0 说明 已经介绍了一种 uniform 的聚合模型 Bagging 和一种 linear 的聚合模型 AdaBoost，这一讲将介绍一类 conditional 的聚合模型——决策树，并展开一种常用的决策树模型。 1 Decision Tree Hypothesis 决策树是一种比较古老的模型，它的出现早于机器学习这个术语的出现，其特点是高度模仿人类决策的过程。 下图是一个决策树的例子： 这个决策树展示的是一个人决定下班回家是否要学习网课的方法，当下班时间早于六点半并且没有约会，那就看网课，如果有约会就不看网课；当下班时间在六点半和九点半之间，看网课；当下班时间晚于九点半，根据课堂作业的截止期限还有多久决定是否看网课。 图中蓝色的叶结点代表的是 base algorithm gtg_tgt​，橘色的路径代表判断条件。 从递归的角度来讲，由于每个节点（除了叶子结点）下方还是一棵树，所以完全可以把决策树看作是递归问题： 具体展开决策树算法留待下一节，这一节的最后介绍一下决策树算法的优缺点： 优点： 可解释性强，每一步的决策原则都是明确的，因此在医学/商业上应用广泛； 简单，容易编程实现； 训练和预测的效率高。 缺点： 是一种启发性算法，感觉上很有效果，但是缺乏理论保证； 2. 对于初学者来说，规则难以选择，并且这些规则都是直观上的启发式规则的堆叠； 3. 由于规则都是启发式的，所以有各种各样不同的决策树算法，其中有一些流行度较高，但是没有哪一种可以说能够代表决策树算法。这一讲接下来要介绍的只是一种比较流行的决策树算法 2 Decision Tree Algorithm 根据前述递归的思想，可以写出来决策树的基本算法流程，这个流程对所有决策树都是通用的，表示如下： 上图中还表示出了决策树算法中需要人为选择的四个部分，分别是：分支数量 C；分支准则 b(x)b(x)b(x)；终止条件；基本假设 gt(x)g_t(x)gt​(x). 针对这四个部分，不同的决策树算法有不同的设计，我们下面将介绍一种较为流行的决策树——C&amp;RT，全称是 Classification and Rregression Tree. 下面分别介绍 C&amp;RT 算法对这四个部分的设计： 首先看两个简单的部分：C 和 gtg_tgt​. C&amp;RT 着眼于二叉树，C=2，并且 gtg_tgt​ 等于一个使得 EinE_{in}Ein​ 最小的常数，容易算出对于分类问题，gtg_tgt​ 等于该部分中数量最多的类别，对于回归问题，gtg_tgt​ 等于该部分中 yny_nyn​ 的平均。 再看剩余部分。分支准则采用 decision stump，目标是使得分开后两个子树的“纯度”尽量高。当用在分类问题中时，纯度是用来衡量子树中的点的类别有多接近的，当用在回归问题中时，纯度是用来衡量子树中的点的 yyy 值有多接近的。即：b(x)= argmin decision stumps h(x)∑c=12∣Dc with h∣⋅ impurity (Dc with h)b(\\mathbf{x})=\\underset{\\text { decision stumps }h(x)}{\\text { argmin }} \\sum_{c=1}^{2} \\mid \\mathcal{D}_{c}\\ with\\ h \\mid \\cdot\\ impurity \\ \\left(\\mathcal{D}_{c}\\ with\\ h \\right)b(x)= decision stumps h(x) argmin ​∑c=12​∣Dc​ with h∣⋅ impurity (Dc​ with h) 分类问题与回归问题中的 impurityimpurityimpurity 有不同的度量方法，最容易想到的是下面两种： 上图中分别给出了一种回归问题的不纯度和一种分类问题的不纯度衡量方法。回归问题的不纯度就是我们常常使用的公式，但是分类问题的不纯度还需要做一些改变。 这个分类问题的不纯度有一个特点，就是仅仅由子树中类别最多的点的数量决定，但是对于多分类问题，非最多类别的数据点的类别有很多可能性，它们可能同属于一类，也可能均匀分布于其他所有类，人们希望能够把这些点的不纯度也用上，于是分类问题最常用的不纯度度量公式是：1−∑k=1K(∑n=1N[[yn=k]]N)21-\\sum_{k=1}^{K}\\left(\\frac{\\sum_{n=1}^{N}\\left[[y_{n}=k]\\right]}{N}\\right)^{2}1−∑k=1K​(N∑n=1N​[[yn​=k]]​)2，这种度量被称为 Gini index. 这个算法有两个不得不终止的情况：子树中的 yny_nyn​ 都相等（此时不纯度为0）或者 xnx_nxn​ 都相同（此时没有 decision stump）。这种情况下终止的决策树称之为 fully-grown tree. 3 Decision Tree Heuristics in C&amp;RT 根据上一节的内容，我们已经可以描绘出一个基本的 C&amp;RT 算法流程： 理论上讲，如果所有的 xnx_nxn​ 不相同，得到的完全生成树（fully-grown tree）一定有 Ein=0E_{in}=0Ein​=0，这时过拟合的危险非常大。 为了避免过拟合，引入一个表示叶子数量的正则项 Ω(G)\\Omega(G)Ω(G)，目标变成最小化加入正则项后的 EinE_{in}Ein​，但是由于无法遍历所有可能的 GGG 寻找使得 Ein+regularizorE_{in}+regularizorEin​+regularizor 最小的决策树，所以实际问题中我们的做法常常是先在 EinE_{in}Ein​ 最小化的准则下找到一个完全生成树，再对它进行剪枝，看剪到什么程度时能够使得 Ein+regularizorE_{in}+regularizorEin​+regularizor 最小。这一过程见下图： 对于不同的 λ\\lambdaλ 都可以找到一个最好的剪枝后的决策树，可以通过验证选择一个比较合适的 λ\\lambdaλ. 通过简单的修改，C&amp;RT 还可以适用于更多场景：当数据点的某个特征不是数值而是分类变量时，可以尝试遍历这个特征的所有子集找到其 decision stump；当数据点的某个特征可能存在缺失时，可以找一个代理变量，例如用身高作为体重的代理变量，这样当体重未知时就用身高与体重的相关关系确定决策路径。 4 Decision Tree in Action 从两个简单的例子中看一下 C&amp;RT 和 AdaBoost-Stump 得到的分类边界的异同： 总结一下 C&amp;RT 的一些优点： 可解释性强； 容易处理多类别问题； 容易处理特征是类别变量的问题； 容易处理缺失数据的问题（代理变量）； 训练和预测效率都很高，可以处理复杂分类边界的问题（见本节上面两个例子）。 这么多的优点几乎只有决策树算法才可能同时拥有。 除了 C&amp;RT 以外，另一个同样很有名的决策树算法是 C4.5 至此，我们讲完了所有的聚合模型的基本模型，下一讲要讲的是一种能够把这些基本模型混起来用的方法。","permalink":"http://yangtf983.github.io/2020/07/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%B3%95%E7%AC%94%E8%AE%B09%EF%BC%9ADecision%20Tree/","photos":[]},{"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"title":"机器学习技法笔记8：Adaptive Boosting","date":"2020/07/18","text":"0 说明 上一讲讲了 blending 和 bagging 两种聚合模型，后者是一种 uniform 的聚合模型，这一讲将介绍一种威力强大的 linear 的聚合模型，这个模型还可以实现边得到 gtg_tgt​ 边生成线性系数。 1 Motivation of Boosting 以一个小例子开始：孩子是如何学会辨别苹果的？ 假如把许多不同水果的图片收集起来，打上标签，让学生们自己从中学习到底什么是苹果。假设这些学生还不知道什么是苹果，那么他们很可能从图片特征上总结苹果的特征。第一个同学说苹果是圆形的，这时候能分对一部分苹果，但是橘子也是圆形的，这个时候老师就要学生从苹果中把橘子区分出来，于是第二个同学说苹果是圆形和红色的。这个时候区分开了苹果和橘子，但是青苹果被分错了，现在老师又要学生修改规则把青苹果归入其中，于是第三个同学说苹果是圆形的、颜色是红色或绿色。但是这个时候西红柿和桃子也可能被归类为苹果，所以老师要求学生进一步修改规则，这个时候目的是区分苹果和西红柿/桃子，于是第四个同学总结说苹果是圆形的、颜色是红色或绿色、顶端有茎的。这个时候我们可以说得到了一条比较完善的规则。 把这个过程抽象化，我们可以得到一些有趣的东西： 学生可以代表简单的模型假设，例如只能是水平线/竖直线的分类器；老师是一个策略算法，作用是指导学生专注于关键的例子上进而修正规则，例如给分类错误的点提高权重；班级代表把所有学生的假设融合起来最后得到的复杂假设。 2 Diversity by Re-weighting 首先从 bagging 讲起，bagging 的核心是自助法，自助法抽样得到的样本往往会包含重复样本，这个时候如果用变量 unu_nun​ 记录下第 n 个数据点在样本中出现的次数，EinE_{in}Ein​ 就可以表示成一个对原始数据求 EinE_{in}Ein​ 后加了权的的形式。因为用原来的数据 D\\mathcal{D}D 计算的 Ein=1NΣ[[yn≠h(xn)]]E_{in}=\\frac1N\\Sigma[[y_n\\ne h(x_n)]]Ein​=N1​Σ[[yn​​=h(xn​)]]，而用自助法样本 D~\\tilde{\\mathcal{D}}D~ 计算的 Ein=1NΣ(x,y)∈D~t[[yn≠h(xn)]]=1NΣ(x,y)∈Dun⋅[[yn≠h(xn)]]E_{in}=\\frac1N\\Sigma_{(x,y)\\in \\tilde{D}_t}[[y_n\\ne h(x_n)]]=\\frac1N\\Sigma_{(x,y)\\in \\mathcal{D}} u_n\\cdot[[y_n\\ne h(x_n)]]Ein​=N1​Σ(x,y)∈D~t​​[[yn​​=h(xn​)]]=N1​Σ(x,y)∈D​un​⋅[[yn​​=h(xn​)]]，如下图所示： 这就激发我们的想法，既然自助法已经被证明是可行的，并且自助法可以被看作是加权的形式，那么我们能不能自己控制权重？例如就像上一节最后说的提高分类错误的点的权重。 这样做的前提是我们的算法应当是可以解最优化加权 EinE_{in}Ein​ 的，这一点对于我们前面讲过的大部分算法都是简单的，例如逻辑斯谛回归的解法就是在选择点的时候各个点被选中的概率与权重成正比；SVM 的解法就是将对偶问题中的条件 0≤αn≤C0\\le \\alpha_n \\le C0≤αn​≤C 改成 0≤αn≤Cun0\\le \\alpha_n \\le Cu_n0≤αn​≤Cun​；等等。 现在我们可以用自己赋权来得到不同的模型，但是在聚合模型中，我们希望的是这些模型之间的差别越大越好，那么我们如何赋权可以得到更加不同的假设呢？ 可以想象，如果我们的 gt+1g_{t+1}gt+1​ 与上一次非常不同，那么应当说明上一次的结果代入 Eint+1E_{in}^{t+1}Eint+1​ 应当离最小值很远（或者说在 t+1 次的样本上表现很差，这里说的样本是按照比例展开后的样本，即权重越大的点在其中重复次数越多），否则应该稍作修改就可以，ggg 不应当差别很大。 换句话说，这个时候 gtg_tgt​ 在第 t+1 次的样本上表现应当很差，我们力求达到最差的情况，那就是分类正确的概率是二分之一，与随机估计效果相同。这个时候要求 gtg_tgt​ 在 t+1 次的样本上有一半是估计正确的，另一半是估计错误的。 这个目的很容易达到，假如 gtg_tgt​ 在第 t 次样本上分类错误的点的权重之和是 1126，分类正确的点的权重之和是 6211，这个时候记错误率 ϵt=11267337\\epsilon_t=\\frac{1126}{7337}ϵt​=73371126​，那么只需要给分类正确的点的权重 utu_tut​ 乘上 ϵt\\epsilon_tϵt​，分类错误的点的权重乘上 1−ϵt1-\\epsilon_t1−ϵt​ 即可。 3 Adaptive Boosting Algorithm 接着上一节定义一个量 rt=1−ϵtϵt\\mathbf{r}_{t}=\\sqrt{\\frac{1-\\epsilon_{t}}{\\epsilon_{t}}}rt​=ϵt​1−ϵt​​​，实际应用中通常的做法是给错误点的权重都乘 rt\\mathbf{r}_trt​，同时给正确点的权重都除以 rt\\mathbf{r}_trt​. 由于 ϵ≤12\\epsilon\\le \\frac12ϵ≤21​，因此 rt≥1\\mathbf{r}_t\\ge 1rt​≥1，从物理意义上来看这一步就是放大错误点的影响，与第一节的例子中的老师的做法相同。 这样我们已经可以总结出来算法的初步流程： 选定初始权重 u(1)u^{(1)}u(1)； for t = 1, 2, …, T : 从 A(D,u(t))\\mathcal{A}(\\mathcal{D}, u^{(t)})A(D,u(t)) 中得到 gtg_tgt​，方法是使用算法 A\\mathcal{A}A 优化以 u(t)u^{(t)}u(t) 为权重的 0/1 误差函数； 使用 rt=1−ϵtϵt\\mathbf{r}_{t}=\\sqrt{\\frac{1-\\epsilon_{t}}{\\epsilon_{t}}}rt​=ϵt​1−ϵt​​​ 把权重从 u(t)u^{(t)}u(t) 更新到 u(t+1)u^{(t+1)}u(t+1)； 得到并返回聚合模型 G(x)G(x)G(x) 在这个流程中还有两步是不确定的，那就是如何选择 u(1)u^{(1)}u(1) 和 G(x)G(x)G(x)： u(1)u^{(1)}u(1) 的选择是简单的，取 un(1)=1Nu^{(1)}_n=\\frac1Nun(1)​=N1​ 即可，这样做可以使得我们在乎的 EinE_{in}Ein​ 尽可能小； G(x)G(x)G(x) 的选择有一点要注意，那就是不能用 uniform 形式的聚合，因为除了初始权重，其他权重下的最优解的目标都不是让 EinE_{in}Ein​ 尽可能小，而仅仅是尽可能修正上一次错误的部分，所以给同样的权重不会使得模型表现更好。这里使用线性或者非线性的聚合都可以。 下面将要介绍一种算法，也就是本讲标题中的算法——AdaBoost，这个算法的 gtg_tgt​ 的选取就如同前几节介绍的一样，使用上一次的错误率修正，但是 G(x)G(x)G(x) 的选取不一样，这个算法把 G(x)G(x)G(x) 限制在线性聚合模型的范围，它能做到一边得到 gtg_tgt​ 一边得到对应的线性系数 αt\\alpha_tαt​ 这个算法选系数的方法很简单，不需要做新的尝试或验证，而是直接取 αt=ln⁡(rt)\\alpha_t=\\ln(\\mathbf{r}_t)αt​=ln(rt​). 这样取具有其合理性，首先我们知道 gtg_tgt​ 的正确率越高对应的 rt\\mathbf{r}_trt​ 越大，并且正确率越高得到的权重越大也是合理的，由于 αt=ln⁡(rt)\\alpha_t=\\ln(\\mathbf{r}_t)αt​=ln(rt​) 是一个关于 rt\\mathbf{r}_trt​ 的单调递增函数，所以从这个角度来讲这么做有其合理性。其次，当 gtg_tgt​ 的正确率是二分之一时，αt=0\\alpha_t=0αt​=0，这时的 gtg_tgt​ 恰好对模型提升没什么效果，当 gtg_tgt​ 的正确率是 1 时，αt=∞\\alpha_t=\\inftyαt​=∞，也符合逻辑。 用一种简单的方法来记忆 AdaBoost 算法，那就是学生、老师和班级三部分，对应如下： AdaBoost = 较弱的学习算法 A\\mathcal{A}A （学生）+ 赋权因子 rt\\mathbf{r}_trt​ （老师）+ 线性聚合系数 αt\\alpha_tαt​ （班级） 这样就讲完了 AdaBoost 的所有流程，总结如下： AdaBoost 在理论上也具有很好的性质： 4 Adaptive Boosting in Action 很多简单算法可以作为 AdaBoost 的 base algorithm，其中一种是 decision stump： decision stump 搭配 AdaBoost 这个算法有一个名字叫做 AdaBoost-Stump，下图可以展现 AdaBoost-Stump 的威力： 通过 AdaBoost，从简单的 decision stump 分类器中得到了一个能够对正弦边界的数据进行分类的分类器。 实际问题中，AdaBoost-Stump 最成功的一个应用是实时人脸识别： 至此，我们介绍了 uniform 的聚合算法 Bagging，也介绍了 linear 的聚合算法 AdaBoost，下一讲将介绍的是一种 conditional 的聚合算法——决策树。","permalink":"http://yangtf983.github.io/2020/07/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%B3%95%E7%AC%94%E8%AE%B08%EF%BC%9AAdaptiva%20Boosting/","photos":[]},{"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"title":"机器学习技法笔记7：Blending and Bagging","date":"2020/07/17","text":"0 说明 上一讲把 kernel trick 引入回归问题，大大增加了我们可以使用的模型和特征。 这一讲目的是看看有没有什么方法可以把我们找到的假设/特征合并起来，提升模型的效果。我们称这样的模型为 Aggregation Model，这一讲中涉及的 Aggregation Model 有两个，分别是 blending 和 bagging. 1 Motivation of Aggregation 首先看一下为什么要用 aggregation，也就是说什么样的场景下我们有用 aggregation 的必要？ 举一个例子，假设你有 T 个朋友，每个人对股票涨跌都有不同的看法，也就是说每个人都有一个预测股票涨跌的函数 gt(x)g_t(x)gt​(x)，那么你如何参考他们的看法能够获得尽可能多的利润呢？ 这个问题可以有很多种看法，比如： 听从平时预测最准确的朋友的看法，这也就是 validation； 听从多数人的看法，让他们对同一个股票投票，每人一票，uniformly； 投票，但是给他们不同的票数，non-uniformly； 投票，non-uniformly，同时根据条件决定票数比例，比如科技股的预测给甲、乙的票数多，石油股的预测给丙、丁的票数多，conditionally. 当然还有其他情景，但以上是几种比较通用的情景，也是在机器学习中有对应方法的情景，本讲后几节以及接下来的几讲的任务就是把这些情景对应到机器学习中，给出相应的 aggregation model. 记聚合模型为 G(x)G(x)G(x)，这个模型可以用 gt, t=1,…,Tg_t,\\ t=1,\\ldots,Tgt​, t=1,…,T 表示如下： 容易看出，第三种情形包含前两种情形，第四种情形包含第三种情形，因此第四种情形包含前面所有情形。 这四种方法中，只有第一种方法的本质是验证，而不需要聚合，后三种都需要模型聚合。下面对第一种单独分析： 通过验证选择模型是一种简单且常用的方法，优点是可以保证模型的推广能力，但是 缺点同样很明显，那就是如果这些可选择的模型效果都一般，即便选出来最好的模型效果也还是一般。这种时候也就是聚合模型最有用的时候，用一堆表现一般的模型，得到一个表现较好的模型，是聚合模型的特点和优势。 因此从验证方法的特点中也就得到了聚合模型最重要的一个特点，那就是可以从一些比较弱的假设中得到一个表现较好的模型。 下面从一个例子中直观理解一下为什么聚合模型可能达到这个效果： 上图中第一个例子，单独使用横线或竖线的分类效果都比较差（即图中三条灰线），但是通过合适的聚合方法可以得到分类较好的模型（即图中黑线）。 上图中第二个例子，图中每一条灰线都是一个 PLA 分类器，它们的表现相同，但是 SVM 已经告诉我们，选择中间的线的推广能力较好。现在如果不考虑线宽，使用投票方法，得到中间的黑线，可以发现其依然是线宽比较大的线，也就是说通过聚合提高了模型的推广能力。更直白地说，聚合模型达到了正则化的效果。 可以看到，第一个例子中聚合提高了假设的强度，使得模型达到更好的表现效果，第二个模型中聚合达到了正则化的效果。在以前的模型中，这两件事情是没办法同时做到的（例如用核函数增加模型强度，用正则化保证推广能力），但是聚合模型某种程度上有两方面的效果，这也是聚合模型的重要特点之一。 2 Uniform Blending 下面开始介绍第一种聚合方法，称之为 uniform blending. 2.1 Uniform Blending for Classification 先把这种方法用于分类问题： 这种方法的思想就是让每个分类器投票，每个分类器都有一票，获得票数多的类别为最终的分类。 当只有两个类别时，容易写出来最后的模型表达式为：G(x)=sign(Σt=1T1⋅gt(x))G(x)=sign\\left(\\Sigma_{t=1}^T1\\cdot g_t(x)\\right)G(x)=sign(Σt=1T​1⋅gt​(x))，当有多个类别时，表达式写作：G(x)=arg⁡max⁡1≤k≤KΣt=1T[[gt(x)=k]]G(x)=\\arg\\max_{1\\le k\\le K}\\Sigma_{t=1}^T[[g_t(x)=k]]G(x)=argmax1≤k≤K​Σt=1T​[[gt​(x)=k]] 这样的模型有两个特点： 当 gtg_tgt​ 都相同时，聚合模型的表现与每一个 gtg_tgt​ 的表现相同； 当 gtg_tgt​ 不相同时，多数模型的预测可以修正少数模型的预测，例如上一小节中的第一个例子就是这样，见下图。 2.2 Uniform Blending for Regression 用在回归问题上，由于每个分类器的权重相同，所以使用平均数作为回归值的预测，即 G(x)=1TΣt=1Tgt(x)G(x)=\\frac1T\\Sigma_{t=1}^Tg_t(x)G(x)=T1​Σt=1T​gt​(x) 首先这个公式和分类的公式具有相同的特点，就是当所有投票的回归模型相同时，聚合模型没有效果，当 gtg_tgt​ 有差异时聚合模型才有效果。 这里有一个假设叫做 diverse hypothesis，其内容是：uniform blending 得到的模型表现一定比其中任何一个单独的模型更好。 下面对这个假设进行一个简单说明： 给定一个 xxx 值，真实的响应值应当是确定的，T 个回归函数给出 T 个回归值，这 T 个回归值是不确定的，由于它们具有相同的票数，因此可以理解为均匀的离散分布，那么 G(x)G(x)G(x) 就是这个随机变量的数学期望，记 f(x)f(x)f(x) 为真实数值，根据均方误差的性质，得到 Et(gt(x)−f(x))2=Vart(gt(x))+(G(x)−f(x))2E_t(g_t(x)-f(x))^2=Var_t(g_t(x))+(G(x)-f(x))^2Et​(gt​(x)−f(x))2=Vart​(gt​(x))+(G(x)−f(x))2，由于回归函数我们一般采用平方误差，所以对这个公式关于 xxx 求期望我们能得到 EtEout(gt)=VartEx(gt)+Eout(G)≥Eout(G)E_tE_{out}(g_t)=Var_tE_x(g_t)+E_{out}(G)\\ge E_{out}(G)Et​Eout​(gt​)=Vart​Ex​(gt​)+Eout​(G)≥Eout​(G) 这个结果说明我们使用 uniformly blending 得到的 G(x) 的效果比随表乱选要好，因为随便乱选的期望误差是 EtEout(gt)E_tE_{out}(g_t)Et​Eout​(gt​)，大于等于 Eout(G)E_{out}(G)Eout​(G) 以上是 uniformly blending 的理论上的保证。 3 Linear and Any Blending 3.1 Linear Blending 前面已经讲过分类和回归问题的 ubiformly blending 的形式，容易推广得到二分类问题的 linear blending 的形式是 G(x)=sign(Σt=1Tαt⋅gt(x)) with αt≥0G(x)=sign\\left(\\Sigma_{t=1}^T\\alpha_t\\cdot g_t(x)\\right)\\ with\\ \\alpha_t\\ge0G(x)=sign(Σt=1T​αt​⋅gt​(x)) with αt​≥0，回归问题的 linear blending 的形式是 G(x)=Σt=1Tαt⋅gt(x) with αt≥0G(x)=\\Sigma_{t=1}^T\\alpha_t\\cdot g_t(x)\\ with\\ \\alpha_t\\ge0G(x)=Σt=1T​αt​⋅gt​(x) with αt​≥0 现在假设已知 gtg_tgt​，目的是找到一组权重 αt\\alpha_tαt​，使得得到一个最优的 G(x)G(x)G(x) 若通过 min⁡αt≥0Ein(α)\\min_{\\alpha_t\\ge0}E_{in}(\\alpha)minαt​≥0​Ein​(α) 寻找权重，则 linear blending for regression 的优化目标可以写成 min⁡αt≥01NΣn=1N(yn−Σt=1Tαtgt(xn))\\min_{\\alpha_t\\ge0}\\frac1N\\Sigma_{n=1}^N\\left(y_n-\\Sigma_{t=1}^T\\alpha_tg_t(x_n)\\right)minαt​≥0​N1​Σn=1N​(yn​−Σt=1T​αt​gt​(xn​))，这个形式和特征空间中的线性回归求最优解是相同的，即先通过 (g1,…,gT)(g_1,\\ldots,g_T)(g1​,…,gT​) 把坐标变换到特征空间，再在特征空间中求线性回归函数。 因此，linear blending = 线性回归模型 + 特征变换 + 系数的限制 αt≥0\\alpha_t\\ge0αt​≥0. 现在问题是加上这个限制条件后如何求解？ 先以二分类问题为例，看一下什么时候有 αt&lt;0\\alpha_t&lt;0αt​&lt;0 ？事实上当出现 αt&lt;0\\alpha_t&lt;0αt​&lt;0 的时候，对应的分类器 gtg_tgt​ 分类正确的概率很可能是小于二分之一的，否则很难通过最小化 EinE_{in}Ein​ 选到 αt&lt;0\\alpha_t&lt;0αt​&lt;0 ，而分类器分类正确的概率小于二分之一时，αt&lt;0\\alpha_t&lt;0αt​&lt;0 是一件很合理的事情。 由于这样的理由，linear blending 在寻找最优解的时候是直接忽略掉限制项的，就是在实数空间内考虑 αt\\alpha_tαt​ . 实际问题中，g1,…,gTg_1,\\ldots,g_Tg1​,…,gT​ 通常是在各自的假设空间 H1,…,HT\\mathcal{H}_1,\\ldots,\\mathcal{H}_TH1​,…,HT​ 中最小化 EinE_{in}Ein​ 得到的，所以对于前述四种方法中的第一种 selection ，其复杂度是 dVC(⋃t=1THt)d_{VC}\\left(\\bigcup_{t=1}^T\\mathcal{H}_t\\right)dVC​(⋃t=1T​Ht​)，linear blending 包含 selection，所以 linear blending 的复杂度大于等于 dVC(⋃t=1THt)d_{VC}\\left(\\bigcup_{t=1}^T\\mathcal{H}_t\\right)dVC​(⋃t=1T​Ht​) ，这样的复杂度难以控制，容易过拟合，所以我们最终选择使用的标准是 EvalE_{val}Eval​，为了留出来干净的数据用于验证，训练过程中使用的是 gt−g_t^-gt−​，整个过程可以分为三步： 把数据集分为训练集 Dtrain\\mathcal{D}_{train}Dtrain​ 和验证集 Dval\\mathcal{D}_{val}Dval​ 两部分，使用训练集训练得到 g1−,…,gT−g_1^-,\\ldots,g_T^-g1−​,…,gT−​ 通过函数 Φ−(x)=(g1−(x),…,gT−(x))\\Phi^-(x)=(g_1^-(x),\\ldots,g_T^-(x))Φ−(x)=(g1−​(x),…,gT−​(x)) 把验证集变换到特征空间上，使用这些数据训练得到 αt\\alpha_tαt​ 使用所有数据一起训练出 g1,…,gTg_1,\\ldots,g_Tg1​,…,gT​，得到最终的模型 GLINB(x)=Σt=1Tαt⋅gt(x)G_{LINB}(x)=\\Sigma_{t=1}^T\\alpha_t\\cdot g_t(x)GLINB​(x)=Σt=1T​αt​⋅gt​(x) 3.2 Any Blending any blending 和 linear blending 的区别很明显，就在于 g1,…,gTg_1,\\ldots,g_Tg1​,…,gT​ 的组合不一定是线性的，计算过程与上面 linear blending 的基本一样，区别仅仅在于第二步中训练的不再是线性模型的系数，第三步中最后的表示也不一样。 我们又称 any blending 为 stacking. 4 Bagging (Bootstrap Aggregation) 前面讲的都是怎么用 g1,…,gTg_1,\\ldots,g_Tg1​,…,gT​ 做成聚合模型，现在问题是这些 gtg_tgt​ 是怎么得到的？能不能一边得到 gtg_tgt​ 一边把它们聚合起来？ 总的来说，得到不同的 gtg_tgt​ 一共有四种方法，分别是：不同的模型、不同的参数、不同的初始点、不同的数据。 下面的目标是看看能否用同一份数据得到不同的 ggg 解决这个看似不可能的问题的方法就是统计学上著名的 bootstrap 方法，这个方法的要点是在数据集 D\\mathcal{D}D 上不断进行放回抽样，这样得到的样本可以看做是在数据的样本空间中直接随机抽样得到的。 由于 bagging 算法的每一次的样本的地位是相等的，所以往往直接采用 uniform 的方法把它们整合起来，比如一人一票进行投票。 假设我们通过 T 次抽样得到了 T 个 bootstrap 样本 D~t\\tilde{\\mathcal{D}}_tD~t​，此时的 gtg_tgt​ 就是用数据 D~t\\tilde{\\mathcal{D}}_tD~t​ 训练得到的，得到 gtg_tgt​ 后使用前面介绍过的流程就可以得到此时的聚合模型。 由于 bootstrap 方法得到的样本本身区别可能不是很大，所以方法对于对数据变化比较敏感的算法效果更好，例如 pocket 算法等。 对于这样得到聚合模型的方法有一个术语叫做 BAGging，是 bootstrap aggregation 的所写。 显然 BAGging 是一种基于其他算法之上的算法，这类算法我们称之为 meta algorithm，其中使用的到的算法我们称之为 base algorithm. 通过下面这个把 Bagging 应用到 Pocket 算法上的例子可以看出来 Bagging 算法还是很有威力的： 使用 bootstrap 样本的 gtg_tgt​ 是同等地位，应当有相同的票数，所以可以直接边得到 gtg_tgt​ 边把它们合起来。 bagging 是 uniform 的聚合模型，虽然已经威力很强了，但是有时候我们还是需要 linear 的或 conditional 的聚合模型，接下来的两节将分别介绍两种类型下有代表性的一个算法。","permalink":"http://yangtf983.github.io/2020/07/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%B3%95%E7%AC%94%E8%AE%B07%EF%BC%9ABlending%20and%20Bagging/","photos":[]},{"tags":[{"name":"编程","slug":"编程","permalink":"http://yangtf983.github.io/tags/%E7%BC%96%E7%A8%8B/"}],"title":"机器学习编程 02：用 sklean 模块实现回归","date":"2020/07/08","text":"0 说明 本文希望实现的回归模型有 SVR, linear regression, ridge regression, logistic regression. 1 SVR SVR 在机器学习技法课程第 6 讲中第一次被提出，这种方法是回归方法，但是具有 svm 的优良性质，例如可以用二次规划求解和能够使用核技巧，因此被广泛使用. 关于这个方法sklearn官方文档中的介绍摘录如下： The method of Support Vector Classification can be extended to solve regression problems. This method is called Support Vector Regression. The model produced by support vector classification (as described above) depends only on a subset of the training data, because the cost function for building the model does not care about training points that lie beyond the margin. Analogously, the model produced by Support Vector Regression depends only on a subset of the training data, because the cost function ignores samples whose prediction is close to their target. There are three different implementations of Support Vector Regression: SVR, NuSVR and LinearSVR. LinearSVR provides a faster implementation than SVR but only considers the linear kernel, while NuSVR implements a slightly different formulation than SVR and LinearSVR. See Implementation details for further details. 1.1 第 1 个例子 参见sklearn官方文档 12345from sklearn import svmX = [[0, 0], [2, 2]]y = [0.5, 2.5]regr = svm.SVR(kernel='linear')regr.fit(X, y) SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='scale', kernel='linear', max_iter=-1, shrinking=True, tol=0.001, verbose=False) 1regr.predict([[1, 1], [0,0], [2,2]]) array([1.5, 0.6, 2.4]) 这个例子中只有两个训练的数据点，因此使用线性回归模型时回归函数值一定等于数据点的值，因为必然存在直线过这两个数据点。但是使用 SVR 的默认参数时却不是这样，这是因为 SVR 权衡了 L2 正则项。可以通过减小 ϵ\\epsilonϵ 或增大 C 达到这样的效果。展示如下： 12345from sklearn import svmX = [[0, 0], [2, 2]]y = [0.5, 2.5]regr = svm.SVR(C=1000, epsilon=0, kernel='linear')regr.fit(X, y) SVR(C=1000, cache_size=200, coef0=0.0, degree=3, epsilon=0, gamma='scale', kernel='linear', max_iter=-1, shrinking=True, tol=0.001, verbose=False) 1regr.predict([[1, 1], [0,0], [2,2]]) array([1.5, 0.5, 2.5]) 1.2 第 2 个例子：SVR using linear and non-linear kernels 参见sklearn官方文档 Toy example of 1D regression using linear, polynomial and RBF kernels. 12345678910111213141516171819import numpy as npfrom sklearn.svm import SVRimport matplotlib.pyplot as plt# ############################################################################## Generate sample dataX = np.sort(5 * np.random.rand(40, 1), axis=0)y = np.sin(X).ravel() # ravel()可以把一个高维数组整理成一个一维数组# ############################################################################## Add noise to targetsy[::5] += 3 * (0.5 - np.random.rand(8))# ############################################################################## Fit regression modelsvr_rbf = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=.1)svr_lin = SVR(kernel='linear', C=100, gamma='auto')svr_poly = SVR(kernel='poly', C=100, gamma='auto', degree=3, epsilon=.1, coef0=1) 1svr_rbf SVR(C=100, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.1, kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False) 1234567891011121314151617181920212223242526# ############################################################################## Look at the resultslw = 2svrs = [svr_rbf, svr_lin, svr_poly]kernel_label = ['RBF', 'Linear', 'Polynomial']model_color = ['m', 'c', 'g']fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 10), sharey=True)for ix, svr in enumerate(svrs): axes[ix].plot(X, svr.fit(X, y).predict(X), color=model_color[ix], lw=lw, label='&#123;&#125; model'.format(kernel_label[ix])) axes[ix].scatter(X[svr.support_], y[svr.support_], facecolor=\"none\", edgecolor=model_color[ix], s=50, label='&#123;&#125; support vectors'.format(kernel_label[ix])) axes[ix].scatter(X[np.setdiff1d(np.arange(len(X)), svr.support_)], y[np.setdiff1d(np.arange(len(X)), svr.support_)], facecolor=\"none\", edgecolor=\"k\", s=50, label='other training data') axes[ix].legend(loc='upper center', bbox_to_anchor=(0.5, 1.1), ncol=1, fancybox=True, shadow=True)fig.text(0.5, 0.04, 'data', ha='center', va='center')fig.text(0.06, 0.5, 'target', ha='center', va='center', rotation='vertical')fig.suptitle(\"Support Vector Regression\", fontsize=14)plt.show() E:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). y = column_or_1d(y, warn=True) E:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). y = column_or_1d(y, warn=True) E:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). y = column_or_1d(y, warn=True) 2 linear regression, ridge regression, logistic regression 这三种回归方法的使用都包含在 sklearn.linear_model 中，分别为 linear_model.LinearRegression(), linear_model.Ridge(), linear_model.LogisticRegression() 2.1 linear_model.LinearRegression() class sklearn.linear_model.LinearRegression(*, fit_intercept=True, normalize=False, copy_X=True, n_jobs=None) 参数的含义见官方文档 2.1.1 第 1 个例子：简单线性回归 1234567import numpy as npfrom sklearn.linear_model import LinearRegressionX = np.array([[1, 13], [1, 2], [2, 2], [2, 30]])# y = 1 * x_0 + 2 * x_1 + 3y = np.dot(X, np.array([1, 2])) + 3reg = LinearRegression().fit(X, y)reg.score(X, y) # 返回决定系数 1.0 1X array([[ 1, 13], [ 1, 2], [ 2, 2], [ 2, 30]]) 1reg.coef_ array([1., 2.]) 1reg.intercept_ 3.0000000000000107 2.1.2 第 2 个例子：数据标准化后再线性回归 1234567891011import numpy as npfrom sklearn.pipeline import make_pipelinefrom sklearn.preprocessing import StandardScalerfrom sklearn.linear_model import LinearRegressionX = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])# y = 1 * x_0 + 2 * x_1 + 3y = np.dot(X, np.array([1, 2])) + 3reg = make_pipeline(StandardScaler(), LinearRegression())reg.fit(X, y) Pipeline(memory=None, steps=[('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('linearregression', LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False))], verbose=False) 1reg.predict(X) array([ 6., 8., 9., 11.]) 2.1.3 第 3 个例子：简单线性回归与作图（使用内置数据集） 参见官方文档 12345678910111213141516171819import matplotlib.pyplot as pltimport numpy as npfrom sklearn import datasets, linear_modelfrom sklearn.metrics import mean_squared_error, r2_score# Load the diabetes datasetdiabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)# Use only one feature# 提取 diabetes_y 的第 3 列数据并转为列向量diabetes_X = diabetes_X[:, np.newaxis, 2] # Split the data into training/testing setsdiabetes_X_train = diabetes_X[:-20]diabetes_X_test = diabetes_X[-20:]# Split the targets into training/testing setsdiabetes_y_train = diabetes_y[:-20]diabetes_y_test = diabetes_y[-20:] 1234567891011121314151617# Create linear regression objectregr = linear_model.LinearRegression()# Train the model using the training setsregr.fit(diabetes_X_train, diabetes_y_train)# Make predictions using the testing setdiabetes_y_pred = regr.predict(diabetes_X_test)# The coefficientsprint('Coefficients: \\n', regr.coef_)# The mean squared errorprint('Mean squared error: %.2f' % mean_squared_error(diabetes_y_test, diabetes_y_pred))# The coefficient of determination: 1 is perfect predictionprint('Coefficient of determination: %.2f' % r2_score(diabetes_y_test, diabetes_y_pred)) Coefficients: [938.23786125] Mean squared error: 2548.07 Coefficient of determination: 0.47 123456789# Plot outputsplt.scatter(diabetes_X_test, diabetes_y_test, color='black')plt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)# 隐藏坐标轴数值#plt.xticks(())#plt.yticks(())plt.show() 2.2 sklearn.linear_model.Ridge() class sklearn.linear_model.Ridge(alpha=1.0, *, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver=‘auto’, random_state=None) 参见官方文档和基石笔记第 14 讲 这里使用的优化函数的形式是：||y - Xw||^2_2 + alpha * ||w||^2_2 因此这里的 alpha 即我们在基石课程中用到的 λ\\lambdaλ，越大则说明正则项影响越强。 2.2.1 第 1 个例子：直接使用岭回归 1234567from sklearn.linear_model import Ridgeimport numpy as npn_samples, n_features = 10, 5rng = np.random.RandomState(0) # 设定随机数种子y = rng.randn(n_samples)X = rng.randn(n_samples, n_features)clf = Ridge(alpha=1.0) 1clf.fit(X, y) Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None, normalize=False, random_state=None, solver='auto', tol=0.001) 1clf.predict(X) array([ 0.95424223, 0.48712089, 1.10388121, 1.79105759, 1.05769745, -0.07915715, 1.12314109, -0.22941441, 0.64906337, 0.52259943]) 2.2.2 第 2 个例子：画出岭迹 岭迹是用来选择合适的 alpha 的重要方法。在岭回归中，当 alpha 越大，得到模型的平方误差函数也就越大，但是系数绝对值越小，系数的方差也越小。当选定 alpha 时，我们的目标是使得具有 L2 正则项的平方误差项最小。但是 alpha 的选择之前的基石课程中却没有讲。实际上 alpha 的选择是为了使得岭回归的系数与真实的线性模型系数的均方误差达到最小（因为使用岭回归的情景就是原问题的系数方差太大，这可能是因为需要求逆的矩阵不可逆或难以求逆等原因导致的）。为了达到这个目的，一般我们使用岭迹确定合适的 alpha，这个方法就是把岭回归中的各种系数随着 alpha 变化的值画到一张图上，我们称这样连出来的线为岭迹，我们选择曲线趋于平稳时的较小的 alpha 作为合适的 alpha。这样做的合理性为：当曲线趋于平稳时，说明系数的方差变化趋于平稳，这样当继续增大 alpha 时，系数方差的减小量很有可能就小于系数与真实系数偏差的增大量了，而均方误差就等于方差加上偏差，所以这样选择比较有可能得到使得均方误差较小的 alpha。 下面看一个画岭迹的例子，参见官方文档 12345678910111213141516171819import numpy as npimport matplotlib.pyplot as pltfrom sklearn import linear_model# X is the 10x10 Hilbert matrixX = 1. / (np.arange(1, 11) + np.arange(0, 10)[:, np.newaxis])y = np.ones(10)# ############################################################################## Compute pathsn_alphas = 200alphas = np.logspace(-10, -2, n_alphas)coefs = []for a in alphas: ridge = linear_model.Ridge(alpha=a, fit_intercept=False) ridge.fit(X, y) coefs.append(ridge.coef_) 12345678910111213# ############################################################################## Display resultsax = plt.gca()ax.plot(alphas, coefs)ax.set_xscale('log')ax.set_xlim(ax.get_xlim()[::-1]) # reverse axisplt.xlabel('alpha')plt.ylabel('weights')plt.title('Ridge coefficients as a function of the regularization')plt.axis('tight')plt.show() 从上图中可以看出，这组数据使用 10e-5 或 10e-6 比较合适 2.3 sklearn.linear_model.LogisticRegression() 这里除了介绍 LogisticRegression()，再介绍一下 sklearn.linear_model.LogisticRegressionCV()，后者是 Logistic regression with built-in cross validation. 首先看 LogisticRegression() class sklearn.linear_model.LogisticRegression(penalty=‘l2’, *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver=‘lbfgs’, max_iter=100, multi_class=‘auto’, verbose=0, warm_start=False, n_jobs=None, l1_ratio=None) 参见官方文档和基石笔记第 10 讲 2.3.1 一个 LogisticRegression() 的小例子 12345from sklearn.datasets import load_irisfrom sklearn.linear_model import LogisticRegressionX, y = load_iris(return_X_y=True)clf = LogisticRegression(random_state=0).fit(X, y)clf.predict(X[:2, :]) E:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG) array([0, 0]) 1y[:2] array([0, 0]) 1clf.predict_proba(X[:2, :]) array([[9.81797141e-01, 1.82028445e-02, 1.44269293e-08], [9.71725476e-01, 2.82744937e-02, 3.01659208e-08]]) 1clf.score(X, y) # Return the mean accuracy on the given test data and labels. 0.9733333333333334 2.3.2 三类别逻辑斯谛回归 LogisticRegression() 与上一个例子一样使用 iris 数据集，这个数据集包含三种类别的鸢尾花，y 值是它们的种类，x 值有四个维度，分别代表：花萼长度、花萼宽度、花瓣长度、花瓣宽度。 参见官方文档 1234567891011121314import numpy as npimport matplotlib.pyplot as pltfrom sklearn.linear_model import LogisticRegressionfrom sklearn import datasets# import some data to play withiris = datasets.load_iris()X = iris.data[:, :2] # we only take the first two features.Y = iris.targetlogreg = LogisticRegression(C=1e5)# Create an instance of Logistic Regression Classifier and fit the data.logreg.fit(X, Y) LogisticRegression(C=100000.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=100, multi_class='auto', n_jobs=None, penalty='l2', random_state=None, solver='lbfgs', tol=0.0001, verbose=0, warm_start=False) 123456789101112131415161718192021222324# Plot the decision boundary. For that, we will assign a color to each# point in the mesh [x_min, x_max]x[y_min, y_max].x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5h = .02 # step size in the meshxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()])# Put the result into a color plotZ = Z.reshape(xx.shape)plt.figure(1, figsize=(4, 3))plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)# Plot also the training pointsplt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors='k', cmap=plt.cm.Paired)plt.xlabel('Sepal length')plt.ylabel('Sepal width')plt.xlim(xx.min(), xx.max())plt.ylim(yy.min(), yy.max())plt.xticks(())plt.yticks(())plt.show() 接着看一下 LogisticRegressionCV() class sklearn.linear_model.LogisticRegressionCV(*, Cs=10, fit_intercept=True, cv=None, dual=False, penalty=‘l2’, scoring=None, solver=‘lbfgs’, tol=0.0001, max_iter=100, class_weight=None, n_jobs=None, verbose=0, refit=True, intercept_scaling=1.0, multi_class=‘auto’, random_state=None, l1_ratios=None) 参见官方文档 与交叉验证有关的重要的参数有两个： 1. Cs: int or list of floats, default=10 Each of the values in Cs describes the inverse of regularization strength. If Cs is as an int, then a grid of Cs values are chosen in a logarithmic scale between 1e-4 and 1e4. Like in support vector machines, smaller values specify stronger regularization. 2. cv: int or cross-validation generator, default=None The default cross-validation generator used is Stratified K-Folds. If an integer is provided, then it is the number of folds used. See the module sklearn.model_selection module for the list of possible cross-validation objects. 2.3.3 一个 LogisticRegressionCV() 的例子 12345from sklearn.datasets import load_irisfrom sklearn.linear_model import LogisticRegressionCVX, y = load_iris(return_X_y=True)clf = LogisticRegressionCV(cv=5, random_state=0)clf.fit(X, y) E:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG) E:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG) E:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG) E:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG) E:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT. Increase the number of iterations (max_iter) or scale the data as shown in: https://scikit-learn.org/stable/modules/preprocessing.html Please also refer to the documentation for alternative solver options: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG) LogisticRegressionCV(Cs=10, class_weight=None, cv=5, dual=False, fit_intercept=True, intercept_scaling=1.0, l1_ratios=None, max_iter=100, multi_class='auto', n_jobs=None, penalty='l2', random_state=0, refit=True, scoring=None, solver='lbfgs', tol=0.0001, verbose=0) 1clf.predict(X[:2, :]) array([0, 0]) 1clf.predict_proba(X[:2, :]).shape (2, 3) 1clf.score(X, y) 0.98 1clf.get_params() {'Cs': 10, 'class_weight': None, 'cv': 5, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1.0, 'l1_ratios': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': 0, 'refit': True, 'scoring': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0}","permalink":"http://yangtf983.github.io/2020/07/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BC%96%E7%A8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BC%96%E7%A8%8B02%EF%BC%9A%E7%94%A8%20sklean%20%E6%A8%A1%E5%9D%97%E5%AE%9E%E7%8E%B0%E5%9B%9E%E5%BD%92/","photos":[]},{"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"title":"机器学习技法笔记6：Support Vector Regression","date":"2020/07/08","text":"0 说明 上一讲讲到要把 SVM 用到逻辑斯谛回归中的两种方法：一种是两阶段法；另一种是用 L2 正则化逻辑斯蒂回归模型和 representer thorem. 这一讲的目的是在一般的回归形式中引入 kernel，这种思想产生了我们常用的 SVR 方法。 1 Kernel Ridge Regression 之前在 representer throrem 中已经讲过，任何线性模型引入核技巧的万能方法是 L2 正则化。 线性回归问题的误差函数是 err(y,wTz)=(y−wTz)2err(y,w^Tz)=(y-w^Tz)^2err(y,wTz)=(y−wTz)2，基石课程中已经讲过，给其引入 L2 正则化后的结果是岭回归，这个模型具有解析解。接下来看一下给其引入核函数后的解的形式： 已知 w∗=Σn=1Nβnznw_*=\\Sigma_{n=1}^N\\beta_nz_nw∗​=Σn=1N​βn​zn​，求解 min⁡w λN+1N(yn−wTzn)2\\min_w\\ \\ \\frac{\\lambda}{N}+\\frac1N(y_n-w^Tz_n)^2minw​ Nλ​+N1​(yn​−wTzn​)2，代入 w∗=Σn=1Nβnznw_*=\\Sigma_{n=1}^N\\beta_nz_nw∗​=Σn=1N​βn​zn​ 得到求解的优化问题如下： 求解的目标函数记作 Eaug(β)E_{aug}(\\beta)Eaug​(β)，显然这是一个关于 β\\betaβ 的二次式，根据原始形式可以知道一定有最小值，因此这个最小值点就是关于 β\\betaβ 的导数为 0 的点，因此：∇Eaug(β)=2N(λKTIβ+KTKβ−KTy)=2NKT((λI+K)β−y)=0\\nabla E_{aug}(\\beta)=\\frac2N\\left(\\lambda K^TI\\beta+K^TK\\beta-K^Ty\\right)=\\frac2NK^T\\left((\\lambda I+K)\\beta-y\\right)=0∇Eaug​(β)=N2​(λKTIβ+KTKβ−KTy)=N2​KT((λI+K)β−y)=0，得到 β=(λI+K)−1y\\beta=(\\lambda I+K)^{-1}yβ=(λI+K)−1y 由于 KKK 是一个半正定矩阵（核函数必须满足的要求），λ&gt;0\\lambda&gt;0λ&gt;0，所以 λI+K\\lambda I+KλI+K 是一个正定矩阵，所以 λI+K\\lambda I+KλI+K 一定可逆，所以 β\\betaβ 的解析解总是存在。 对于密集矩阵求反矩阵，一般时间复杂度是 O(N3)O(N^3)O(N3)，所以求解 kernel ridge regression 的时间复杂度是 O(N3)O(N^3)O(N3) kernel ridge regression 与线性岭回归相比，优点在于引入了 kernel，这样模型就更加灵活，但是缺点在于训练和预测的时间复杂度都和数据量 N 有关系，当数据量比较大时花费时间较长。因此在选择使用线性岭回归还是核岭回归时就是在效率和灵活性之间做权衡。 当得到最优的 β\\betaβ 后，就可以写出核岭回归的回归函数：g(x)=wTz=(Σn=1Nβnzn)z=Σn=1NβnK(xn,x)g(x)=w^Tz=\\left(\\Sigma_{n=1}^N\\beta_nz_n\\right)z=\\Sigma_{n=1}^N\\beta_nK(x_n,x)g(x)=wTz=(Σn=1N​βn​zn​)z=Σn=1N​βn​K(xn​,x) 2 Support Vector Regression Primal 基石课程中说过线性模型可以用来分类，上一节讲的核岭回归方法也可以用来做分类，当把它用来做分类问题时给它一个特殊的名字，叫做 least-squares SVM (简称 LSSVM)，下面是使用高斯核 LSSVM 和软边界高斯核 SVM 在同一个数据集上寻找分类器的例子： 上图中用方框标出的是支持向量，从这个例子中可以看到，LSSVM 多很多支持向量（上一讲讲的核逻辑斯蒂回归模型也是这样），这样的结果是在预测的时候需要多花很多的时间，因此看起来使用 LSSVM（或核逻辑斯谛回归） 并不比 SVM 有优势。 我们寄希望于找到一种系数向量 β\\betaβ 稀疏的回归模型，这种方法要从 tube regression 讲起： tube regression 顾名思义就是把拟合区域从一条线夸大到一个管道，只要是落在这个管道内的点的误差函数值都是 0，此时的误差函数可以写作 err(y,s)=max⁡(0,∣s−y∣−ϵ)err(y,s)=\\max(0,|s-y|-\\epsilon)err(y,s)=max(0,∣s−y∣−ϵ)，其中 ϵ\\epsilonϵ 被称作 ϵ−insensitive error\\epsilon-insensitive\\ errorϵ−insensitive error，图示如下： 接下来的目的就显而易见了，使用 L2 正则化下的 tube regression 得到稀疏的 β\\betaβ 在此之前我们先比较一下 tube regression 的误差函数和普通回归的误差函数： 从二者的误差函数的图像可以看出来，当误差比较小的时候，二者的误差函数很接近，当误差比较大的时候，tube regression 的误差函数明显小于平方误差函数，这样的性质也使得 tube regression 对异常点没有那么敏感，这是 tube regression 的一个优点。 现在的问题变成求解 min⁡w λNwTw+1NΣn=1Nmax⁡(0,∣wTzn−yn∣−ϵ)\\min_w\\ \\ \\frac{\\lambda}{N}w^Tw+\\frac1N\\Sigma_{n=1}^{N}\\max\\left(0,|w^Tz_n-y_n|-\\epsilon\\right)minw​ Nλ​wTw+N1​Σn=1N​max(0,∣wTzn​−yn​∣−ϵ) 的最优解，我们将采用 SVM 的解法来求解这个问题，先把它写成一个二次规划问题，再利用对偶问题求解。这么做有两个原因：一是直接求解比较困难，因为目标函数存在不可微分的点；二是使用对偶问题容易说明系数的系数性。 模拟标准 SVM 问题，我们的优化目标可以改写成 min⁡w 12wTw+C Σn=1Nmax⁡(0,∣wTzn+b−yn∣−ϵ)\\min_w\\ \\ \\frac{1}{2}w^Tw+C\\ \\Sigma_{n=1}^{N}\\max\\left(0,|w^Tz_n+b-y_n|-\\epsilon\\right)minw​ 21​wTw+C Σn=1N​max(0,∣wTzn​+b−yn​∣−ϵ)，通过引入 ξn\\xi_nξn​ 可以把其改写为二次规划问题，改写方法如下： 这样我们就得到了需要求的二次规划问题的原问题： 从下面的例子中可以加深对这个问题的理解： 3 Support Vector Regression Dual 基本步骤与 SVM 问题的对偶问题推导一样，采用拉格朗日乘子法。通过对 w,bw,bw,b 分别求导令为 0 得到两个条件，再加上一组互补松弛条件（complementary slackness），这个问题的 KKT 条件总结如下： 可以推导，也可以直接按照对偶问题的改写方法得到 SVR 的对偶问题如下： 得到上面的对偶问题后就可以用二次规划求解器求解，再用 KKT 条件得到系数，求解的问题解决了，这是我们在上一节最后说的选择用这种方法的第一个原因，第二个原因是这样容易说明 β\\betaβ 向量的稀疏性，下面我们就用 KKT 条件来说明这一点： 对于在管道内部的点，即满足 ∣wTzn+b−yn∣&lt;ϵ|w^Tz_n+b-y_n|&lt;\\epsilon∣wTzn​+b−yn​∣&lt;ϵ 的点而言，显然 ξn∧=0\\xi_{n}^{\\wedge}=0ξn∧​=0 和 ξn∨=0\\xi_{n}^{\\vee}=0ξn∨​=0，因此 (ϵ+ξn∧−yn+wTzn+b)≠0\\left(\\epsilon+\\xi_{n}^{\\wedge}-y_{n}+\\mathbf{w}^{T} \\mathbf{z}_{n}+b\\right) \\neq 0(ϵ+ξn∧​−yn​+wTzn​+b)​=0 和 (ϵ+ξn∨+yn−wTzn−b)≠0\\left(\\epsilon+\\xi_{n}^{\\vee}+y_{n}-\\mathbf{w}^{T} \\mathbf{z}_{n}-b\\right) \\neq 0(ϵ+ξn∨​+yn​−wTzn​−b)​=0，因此 an∧=0a_{n}^{\\wedge}=0an∧​=0 和 an∨=0a_{n}^{\\vee}=0an∨​=0，所以 βn=0\\beta_n=0βn​=0 对于管道外部的点，同样的方法可以推出 βn≠0\\beta_n\\ne0βn​​=0，对于管道边界上的点，可以得到可能为 0 也可能不为 0. 因此，管道内部的点都不是支持向量，这样就得到了 β\\betaβ 的稀疏性。 最后的回归函数形式：y=b+wTz=b+Σ(αn∧−αn∨)znTz=b+Σ(αn∧−αn∨)K(xn,x)y=b+w^Tz=b+\\Sigma(\\alpha_n^{\\wedge}-\\alpha_n^{\\vee})z_n^Tz=b+\\Sigma(\\alpha_n^{\\wedge}-\\alpha_n^{\\vee})K(x_n,x)y=b+wTz=b+Σ(αn∧​−αn∨​)znT​z=b+Σ(αn∧​−αn∨​)K(xn​,x) 4 Summary of Kernel Models 最后回顾总结一下 kernel 模型。 基石课程中学习了三类算法，第一类是 PLA/pocket，做的是硬分类的行为，误差函数是 0/1 误差；第二类是逻辑斯蒂回归模型，做的是软分类，误差函数是交叉熵（cross-entropy）；第三类是线性回归/岭回归，做的是回归问题，误差函数是平方误差（squre erroe）。 技法课程中目前讲过的线性模型有线性软边界 SVM 和线性 SVR，两者的误差函数都是在 0 和一个非负量之间取最大值。 解决线性问题时，上图中第二行使用较多，第一行使用则很少，因为第二行的算法效果更好。 加入核技巧后，产生了一些新的方法。 在前几讲中我们将线性 SVM 扩充到了一般的特征空间中，产生了一般的 SVM 算法，上一讲中在逻辑斯谛回归中引入了核技巧，产生了 probabilistic SVM 和 kernel logistic regression 两种算法，这一讲我们先把核技巧引入了岭回归中，由通过 tube regression 得到了线性 SVR，接着通过其对偶问题引入了核技巧。因此我们一共讲了四种有核技巧的算法，总结在下图中的后两行： 上图中最后一行核方法的使用比倒数第二行要广泛得多，因为它们的支持向量过多，预测的计算量大，想比如最后一行的算法没有其他优势。","permalink":"http://yangtf983.github.io/2020/07/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%B3%95%E7%AC%94%E8%AE%B06%EF%BC%9ASupport%20Vector%20Regression/","photos":[]},{"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"title":"机器学习技法笔记5：Kernel Logistic Regression","date":"2020/07/08","text":"0 说明 这一讲的目的是把 logistic regression 与 kernel trick 结合，看一下能得到什么样的方法。 1 Soft-Margin SVM as Regularized 前四讲介绍了软硬边界情形下的支持向量机原问题与对偶问题，并引入了 kernel function，整合在一张图上表示如下： 实际问题中，一般都是用软边界，硬边界很少用。 对软边界支持向量机而言，ξn\\xi_nξn​ 的物理意义是破坏边界的多少，当边界被破坏时：ξn=1−yn(wTzn+b)\\xi_n=1-y_n(w^Tz_n+b)ξn​=1−yn​(wTzn​+b)，当边界未被破坏时：ξn=0\\xi_n=0ξn​=0 这样软边界支持向量机求解的优化问题可以写作： min⁡b,w 12wTw+C Σn=1Nmax⁡(1−yn(wTzn+b),0)\\min_{b,w}\\ \\ \\frac12 w^Tw+C\\ \\Sigma_{n=1}^N\\max\\left(1-y_n(w^Tz_n+b),0\\right) b,wmin​ 21​wTw+C Σn=1N​max(1−yn​(wTzn​+b),0) 这种形式与基石课程中正则化中讲过的最优化形式很相似：min⁡ 12wTw+C Σerr^\\min\\ \\ \\frac12w^Tw+C\\ \\Sigma\\hat{err}min 21​wTw+C Σerr^ 软边界支持向量机的优化问题可以表示成一个 L2 正则化问题，L2 正则化问题的标准形式为：min⁡ λNwTw+1NΣerr\\min\\ \\ \\frac{\\lambda}{N}w^Tw+\\frac1N\\Sigma errmin Nλ​wTw+N1​Σerr 对比标准形式 L2 正则化问题和软边界支持向量机，二者的区别仅仅在于：SVM 的 www 中不包括所有的未知量，所以比标准形式中的短一点；SVM 使用的参数是 CCC 而不是 λ\\lambdaλ；SVM 中使用的 errerrerr 比较特殊。 这种形式看起来很简单，但是我们之前却不把它作为 SVM 的标准形式，之前也没有讲过这种形式，这是出于两点原因： 这种形式不是二次规划，相比于二次规划问题，它没有对偶问题，因此无法消除特征空间维度的影响，无法使用 kernel trick； max⁡(⋅,0)\\max(\\cdot,0)max(⋅,0) 不可微分，难以求最优解。 现在提出这种形式是为了把软边界支持向量机延伸到其他方法上，使用这种方法容易看出这种延伸的合理性所在。 下图是几种形式的对比： 2 SVM versus Logistic Regression 我们将 0/1，SVM 和 logistic regression 的 error 函数放在一起，写出表达式并画出图形如下： 首先，根据基石课程中的内容，0/1 error 的合理性是建立在使得正确率尽可能高这个基础上的，无需赘述，至于其他误差函数，若其最小化能够导致 0/1 误差最小，则合理，否则，这个误差函数的合理性值得怀疑。 从这个图中可以看出，SVM 的误差函数最小化可以导致 0/1 误差最小，因此这个误差函数是合理的。逻辑斯谛误差函数（这里的与基石中的函数的对数底不同，相当于是倍数关系）的最小化也导致 0/1 误差最小化。 对三种二分类线性模型的对比： 从误差函数可以看出来，二分类逻辑斯谛回归几乎就是在做软边界 SVM，这就是二者的联系。现在反过来想，若我们解出来一个 SVM 的解，能不能把这个解放在逻辑斯蒂回归模型中？这就是 kernel logistic regression 的思想。 3 SVM for Soft Binary 上一节的最后提出了把 SVM 的解应用在逻辑斯蒂回归模型中的想法，那么究竟怎么应用比较好值得继续探讨一下。首先看两个直接的想法： 第一个想法是解 SVM 得到 (bSVM,wSVM)(b_{SVM}, w_{SVM})(bSVM​,wSVM​)，之后用 g(x)=θ(wSVMTx+bSVM)g(x)=\\theta(w^T_{SVM}x+b_{SVM})g(x)=θ(wSVMT​x+bSVM​) 作为最终的模型。这种方法的优点是简单直接，缺点是丧失了原来的逻辑斯谛回归模型解的最大似然的性质。（个人理解：由于 g(x)g(x)g(x) 是一个单调函数，所以这样做在分类上与 SVM 的结果没有区别，只是可以给出一个概率；但是这个函数由于失去了最大似然这个性质，所以给出的概率的合理性也没有之前的逻辑斯谛回归函数那么高。） 第二个想法是解 SVM 得到 (bSVM,wSVM)(b_{SVM}, w_{SVM})(bSVM​,wSVM​) 作为逻辑斯蒂回归模型的初始点迭代求解。这种想法的优点是有最大似然估计的性质，但是缺点是丧失了 SVM 的优点，比如可以整合入 kernel，这样的结果和原来的逻辑斯谛回顾模型一般非常接近，而计算上又不一定会更简单。 这两种方法都有缺点，我们希望融合他们的优点，避免他们的缺点。 这种融合方法如下： 这种方法被称作 Probabilistic SVM，可以分为两步，第一步是得到 SVM 的最优解，第二步是把数据转换成 zn′=wSVMTΦ(xn)+bSVMz_n&#x27;=w^T_{SVM}\\Phi(x_n)+b_{SVM}zn′​=wSVMT​Φ(xn​)+bSVM​ 后对数据集 {(zn′,yn)}n=1N\\{(z_n&#x27;,y_n)\\}_{n=1}^N{(zn′​,yn​)}n=1N​ 做逻辑斯谛回归。 这样做有两个好处： 由于 B 的存在，最终解与 SVM 的分类可能有一些不同，因为当 B 不等于 0 的时候，分界点 g=1g=1g=1 对应的就不再是 zn′=0z_n&#x27;=0zn′​=0 了。这时我们的结果更加接近最大似然估计； 由于第二步只有一个自变量，因此求解方法更多更简单。 4 Kernel Logistic Regression 之前讲的是近似方法，现在想看一下如何真的在 zzz 空间中做 logistic regression，并且还可以使用 kernel trick? 第一个问题似乎很好回答，先把坐标变换到 zzz 空间，再做 logistic regression 的方法做应该就可以，但是这样的形式怎么用核技巧是一个问题。因为前面我们是通过求解二次规划的对偶问题才引入的核技巧，但是这里逻辑斯谛回归的形式不是一个二次规划问题，所以没办法找到对偶问题。 这里需要对之前的核技巧的使用条件做一个更加精确的解释。实际上使用核技巧的条件并不是二次规划，而是问题中要出现 zzz 空间中坐标的内积，然后把这个内积换成核函数，这个问题进一步转化成 www 要能够表示成 zzz 空间中坐标的线性组合，这样 wTzw^TzwTz 就能够表示成内积的线性组合的形式，例如 SVM 的最优解 wSVM=Σn=1N(αnyn)znw_{SVM}=\\Sigma_{n=1}^N(\\alpha_ny_n)z_nwSVM​=Σn=1N​(αn​yn​)zn​，类似于这样的形式我们称之为 www 可以被 znz_nzn​ 表示。 下面给出一个重要的数学上的结果： 定理（representer theorem）：对于任何 L2 正则化线性模型：min⁡w λNwTw+1NΣerr(yn,wTzn)\\min_w\\ \\ \\frac{\\lambda}{N}w^Tw+\\frac1N\\Sigma err(y_n,w^Tz_n)minw​ Nλ​wTw+N1​Σerr(yn​,wTzn​)，其最优解一定可以被 znz_nzn​ 表示，即：w∗=Σn=1Nβnznw_*=\\Sigma_{n=1}^N\\beta_nz_nw∗​=Σn=1N​βn​zn​ 简单证明这个问题： 假设最优解 w∗w_*w∗​ 可以被 znz_nzn​ 空间中的向量 w∥w_{\\parallel}w∥​ 和垂直于 znz_nzn​ 空间的向量 w⊥w_{\\perp}w⊥​ 表示成 w∗=w∥+w⊥w_*=w_{\\parallel}+w_{\\perp}w∗​=w∥​+w⊥​，我们证明 w∗=Σn=1Nβnznw_*=\\Sigma_{n=1}^N\\beta_nz_nw∗​=Σn=1N​βn​zn​ 也就等价于证明 w⊥=0w_{\\perp}=0w⊥​=0. 由于 w⊥⊥span(zn)w_{\\perp}\\perp span(z_n)w⊥​⊥span(zn​)，因此 w⊥Tzn=0w_{\\perp}^Tz_n=0w⊥T​zn​=0，因此 err(yn,w∗Tzn)=err(yn,(w∥+w⊥)Tzn)=err(yn,w∥Tzn)err(y_n,w_*^Tz_n)=err(y_n,(w_{\\parallel}+w_{\\perp})^Tz_n)=err(y_n,w_{\\parallel}^Tz_n)err(yn​,w∗T​zn​)=err(yn​,(w∥​+w⊥​)Tzn​)=err(yn​,w∥T​zn​) 正则项 w∗Tw∗=w∥Tw∥+2w∥Tw⊥+w⊥Tw⊥=w∥Tw∥+w⊥Tw⊥≥w∥Tw∥w_*^Tw_*=w_{\\parallel}^Tw_{\\parallel}+2w_{\\parallel}^Tw_{\\perp}+w_{\\perp}^Tw_{\\perp}=w_{\\parallel}^Tw_{\\parallel}+w_{\\perp}^Tw_{\\perp}\\ge w_{\\parallel}^Tw_{\\parallel}w∗T​w∗​=w∥T​w∥​+2w∥T​w⊥​+w⊥T​w⊥​=w∥T​w∥​+w⊥T​w⊥​≥w∥T​w∥​ 因此当 w⊥≠0w_{\\perp}\\ne 0w⊥​​=0 时，w∥w_{\\parallel}w∥​ 一定比 w∗=w∥+w⊥w_*=w_{\\parallel}+w_{\\perp}w∗​=w∥​+w⊥​ 更优，所以最优解一定有 w⊥=0w_{\\perp}=0w⊥​=0，此时 w∗=w∥=Σn=1Nβnznw_*=w_{\\parallel}=\\Sigma_{n=1}^N\\beta_nz_nw∗​=w∥​=Σn=1N​βn​zn​ 从这个定理中我们得到一个很强的结果：任何 L2 正则化的线性模型都可以使用核方法。 所以，我们虽然没有办法在原始的逻辑斯蒂回归模型上使用核方法，但是可以在加上了 L2 正则化项的逻辑斯蒂回归模型上使用核方法 这是一个没有限制条件的最优化问题，可以使用梯度下降/随机梯度下降等方法求解。 这种方法全称是 kernel logistic regression，简称 KLR，与 SVM 不同的是，KLR 中的系数 βn\\beta_nβn​ 一般不为 0，是密集的，而 SVM 中的系数是稀疏的。这样看来，这种方法需要付出比较大的计算代价。","permalink":"http://yangtf983.github.io/2020/07/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%B3%95%E7%AC%94%E8%AE%B05%EF%BC%9AKernel%20Logistic%20Regression/","photos":[]},{"tags":[{"name":"编程","slug":"编程","permalink":"http://yangtf983.github.io/tags/%E7%BC%96%E7%A8%8B/"}],"title":"机器学习编程 01：SVM","date":"2020/07/02","text":"0 说明 这一篇文章介绍支持向量机的编程问题，涉及到的算法在机器学习技法笔记1-4讲中，参见第 1 讲、第 2 讲、第 3 讲、第 4 讲 1 使用二次规划求解 hard-margin linear SVM 的原问题 使用cvxopt模块求解二次规划问题的使用参考见：Quadratic Programming in Python 和官方文档 12import numpy as npimport cvxopt 1234567891011def cvxopt_solve_qp(P, q, G=None, h=None, A=None, b=None): P = .5 * (P + P.T) # make sure P is symmetric args = [cvxopt.matrix(P), cvxopt.matrix(q)] if G is not None: args.extend([cvxopt.matrix(G), cvxopt.matrix(h)]) if A is not None: args.extend([cvxopt.matrix(A), cvxopt.matrix(b)]) sol = cvxopt.solvers.qp(*args) if 'optimal' not in sol['status']: return None return np.array(sol['x']).reshape((P.shape[1],)) 下面用1.4讲中提到的第一个例子来做演示 X=[00222030], y=[−1−111]X=\\left[\\begin{array}{ll} 0 &amp; 0 \\\\ 2 &amp; 2 \\\\ 2 &amp; 0 \\\\ 3 &amp; 0 \\end{array}\\right],\\ \\ y=\\left[\\begin{array}{l} -1 \\\\ -1 \\\\ 1 \\\\ 1 \\end{array}\\right]X=⎣⎢⎢⎡​0223​0200​⎦⎥⎥⎤​, y=⎣⎢⎢⎡​−1−111​⎦⎥⎥⎤​ 其手算的最优解 gSVM=sign(x1−x2−1)g_{SVM}=sign(x_1-x_2-1)gSVM​=sign(x1​−x2​−1) 1234P = np.array([[0.,0.,0.], [0.,1.,0.], [0.,0.,1.]])q = np.array([0.,0.,0.])G = -np.array([[-1.,0.,0.], [-1.,-2.,-2.], [1.,2.,0.], [1.,3.,0.]])h = np.array([-1.,-1.,-1.,-1.]).reshape((4,)) 1cvxopt_solve_qp(P=P, q=q, G=G, h=h) pcost dcost gap pres dres 0: 3.2653e-01 1.9592e+00 6e+00 2e+00 4e+00 1: 1.5796e+00 8.5663e-01 7e-01 0e+00 2e-15 2: 1.0195e+00 9.9227e-01 3e-02 1e-16 1e-15 3: 1.0002e+00 9.9992e-01 3e-04 2e-16 2e-15 4: 1.0000e+00 1.0000e+00 3e-06 3e-16 7e-16 5: 1.0000e+00 1.0000e+00 3e-08 4e-16 7e-16 Optimal solution found. array([-1.00000001, 1.00000001, -1.00000001]) 得到的结果是b=−1, wt=[1,−1]b=-1,\\ w^t=[1,-1]b=−1, wt=[1,−1] 因此用上面的程序得到的分类器是 g=sign(x1−x2−1)g=sign(x_1 - x_2 - 1)g=sign(x1​−x2​−1)，与课程中手算出来的一样。 2 求解 hard-margin linear SVM 的对偶问题 方法： 使用二次规划求出拉格朗日乘子 使用 KKT 条件求出 (b,w)(b,w)(b,w) 参考网址：我的博客和 Quadratic Programming in Python 和官方文档 12import numpy as npimport cvxopt 123456789101112131415def cvxopt_solve_qp(P, q, G=None, h=None, A=None, b=None): P = .5 * (P + P.T) # make sure P is symmetric args = [cvxopt.matrix(P), cvxopt.matrix(q)] if G is not None: args.extend([cvxopt.matrix(G), cvxopt.matrix(h)]) if A is not None: try: if A.shape[1]: args.extend([cvxopt.matrix(A), cvxopt.matrix(b)]) except: args.extend([cvxopt.matrix(A).trans(), cvxopt.matrix(b)]) sol = cvxopt.solvers.qp(*args) if 'optimal' not in sol['status']: return None return np.array(sol['x']).reshape((P.shape[1],)) 上面的函数与之前的有所不同，对于一维数组，使用cvxopt会自动转成单列矩阵，因此需要trans()一次 保险起见，其实最好的办法是所有输入参数都采用np.matrix数据类型而不是np.array，上面的参考网页中使用的np.array，但这不是最好的选择。 还有一点需要注意的是，之所以该网页上的 q 用的行向量，就是利用了在数据类型变换中自动转置这个特点，当使用np.matrix数据类型时，q 应当使用列向量。 同样用1.4讲中提到的第一个例子来做演示 X=[00222030], y=[−1−111]X=\\left[\\begin{array}{ll} 0 &amp; 0 \\\\ 2 &amp; 2 \\\\ 2 &amp; 0 \\\\ 3 &amp; 0 \\end{array}\\right],\\ \\ y=\\left[\\begin{array}{l} -1 \\\\ -1 \\\\ 1 \\\\ 1 \\end{array}\\right]X=⎣⎢⎢⎡​0223​0200​⎦⎥⎥⎤​, y=⎣⎢⎢⎡​−1−111​⎦⎥⎥⎤​ 其手算的最优解 gSVM=sign(x1−x2−1)g_{SVM}=sign(x_1-x_2-1)gSVM​=sign(x1​−x2​−1) 1234567891011P = np.array([0. for i in range(16)]).reshape([4,4])z = np.array([[0,0], [2,2], [2,0], [3,0]])y = np.array([-1.,-1.,1.,1.])for i in range(4): for j in range(4): P[i, j] = np.dot(z[i], z[j])*y[i]*y[j]G = -np.eye(4)A = yb = np.array([0.])h = np.zeros((4,1))q = np.array([-1. for i in range(4)]) 1cvxopt_solve_qp(P=P, q=q, G=G, h=h, A=A, b=b) pcost dcost gap pres dres 0: -8.1633e-01 -2.1224e+00 6e+00 2e+00 2e+00 1: -8.5663e-01 -1.5796e+00 7e-01 5e-16 2e-16 2: -9.9227e-01 -1.0195e+00 3e-02 2e-16 5e-16 3: -9.9992e-01 -1.0002e+00 3e-04 1e-16 3e-16 4: -1.0000e+00 -1.0000e+00 3e-06 3e-16 2e-16 5: -1.0000e+00 -1.0000e+00 3e-08 2e-16 8e-16 Optimal solution found. array([5.00000000e-01, 5.00000006e-01, 9.99999998e-01, 7.80987988e-09]) 1234567891011121314P = np.matrix([0. for i in range(16)]).reshape([4,4])z = np.array([[0,0], [2,2], [2,0], [3,0]])y = np.array([-1.,-1.,1.,1.])for i in range(4): for j in range(4): P[i, j] = np.dot(z[i], z[j])*y[i]*y[j]G = -np.eye(4)G = np.matrix(G)A = np.array(y)b = np.matrix([0.])h = np.zeros((4,1))h = np.matrix(h)q = np.array([-1. for i in range(4)])q = np.matrix(q).T 12alpha = cvxopt_solve_qp(P=P, q=q, G=G, h=h, A=A, b=b)alpha pcost dcost gap pres dres 0: -8.1633e-01 -2.1224e+00 6e+00 2e+00 2e+00 1: -8.5663e-01 -1.5796e+00 7e-01 5e-16 2e-16 2: -9.9227e-01 -1.0195e+00 3e-02 2e-16 5e-16 3: -9.9992e-01 -1.0002e+00 3e-04 1e-16 3e-16 4: -1.0000e+00 -1.0000e+00 3e-06 3e-16 2e-16 5: -1.0000e+00 -1.0000e+00 3e-08 2e-16 8e-16 Optimal solution found. array([5.00000000e-01, 5.00000006e-01, 9.99999998e-01, 7.80987988e-09]) 结果分析：α1=0.5,α2=0.5,α3=1,α4=0\\alpha_1 = 0.5, \\alpha_2 = 0.5, \\alpha_3 = 1, \\alpha_4 = 0α1​=0.5,α2​=0.5,α3​=1,α4​=0 12w = -0.5*z[0]-0.5*z[1]+z[2]w array([ 1., -1.]) 12b = -1-np.dot(w,z[1])b -1.0 **结论：**得到的结果恰好等于手算得到的最优解 下面把整个过程封装成一个函数 12345678910111213141516def hard_mar_dual(n, P, q, G=None, h=None, A=None, b=None): ''' n: w 中元素个数 其他参数：二次规划的参数 ''' w = np.array([0. for i in range(n)]) alpha = cvxopt_solve_qp(P, q, G, h, A, b) for i in range(alpha.size): alpha[i] = round(alpha[i], 4) w += alpha[i]*y[i]*z[i] for i in range(alpha.size): num = alpha[i] if num != 0: b = y[i]-np.dot(w, z[i]) break return w,b 12result = hard_mar_dual(n=2, P=P, q=q, G=G, h=h, A=A, b=b)print('------------------------ \\n w = &#123;&#125;, b = &#123;&#125;'.format(result[0], result[1])) pcost dcost gap pres dres 0: -1.6020e+00 -4.0542e+00 2e+00 1e-16 2e+00 1: -1.8298e+00 -2.1516e+00 3e-01 2e-16 4e-01 2: -1.9897e+00 -2.0317e+00 4e-02 5e-16 4e-16 3: -1.9999e+00 -2.0003e+00 4e-04 2e-16 3e-16 4: -2.0000e+00 -2.0000e+00 4e-06 3e-16 8e-16 5: -2.0000e+00 -2.0000e+00 4e-08 4e-16 5e-16 Optimal solution found. ------------------------ w = [ 1. -1.], b = -1.0 **结论：**得到了正确的实验结果 3 用 sklean 模块实现 svm 前面已经用二次规划求解器实现了 hard-margin 情形下的原问题和对偶问题的求解，而 kernel 问题是将其中的内积计算换成核函数，soft-margin 问题仅仅是在 hard-margin 问题中给 αn\\alpha_nαn​ 增加了一个上限条件，因此这两种方法均不再写代码实现。下面练习使用 sklearn 模块实现 svm. 1from sklearn import svm 方法介绍 sklearn.svm一共提供了三种分类方法，分别是 svm.SVC(), svm.LinearSVC(), svm.NuSVC() svm.SVC()就是我们技法课程中讲的 soft-margin svm 方法，其中可以设置核函数形式（包括线性核）和各种参数，若想用硬边界支持向量机，只需要把 C 设置的大一点即可，例如 1000。这个函数还支持多分类，采用的 ono-vs-one 方法。 svm.LinearSVC()是专门用来计算线性核的方法，比用 svm.SVC() 中的线性核效率高一些，更适合大样本处理，另外在损失函数的选择和惩罚项的选择上更灵活（参见svm.LinearSVC?）。 svm.NuSVC() 其实是一种技法课中没有讲过的 SVM 方法，称为 Nu SVM，其特点在于限制支持向量的数量来控制错分的比例，这比 C SVM （就是我们学习的方法）中的用 C 衡量错分和线宽的比重从而间接控制错分比例要直接得多。但是一般前两种方法就可以解决大部分问题了，因此不对这个方法过多介绍。 首先可以通过下面的方法看一下各个方法中的参数： 1svm.SVC? 1svm.LinearSVC? 1svm.NuSVC? 下面是几个小例子 第 1 个例子 参见文档 1234567X = [[0, 0], [1, 1]]y = [0, 1]clf = svm.SVC()# 训练clf.fit(X, y)# 预测clf.predict([[2., 2.]]) array([1]) 12clf.score?# 给出分类精确度的方法 1clf.score([[0, 0], [1, 1]], [0, 1]) 1.0 12# 查支持向量clf.support_vectors_ array([[0., 0.], [1., 1.]]) 12# 查支持向量的下标clf.support_ array([0, 1]) 12# 查每一类中支持向量的数量clf.n_support_ array([1, 1]) 第 2 个例子：线性可分硬边界 svm 参见文档 1234import numpy as npimport matplotlib.pyplot as pltfrom sklearn import svmfrom sklearn.datasets import make_blobs make_blobs 是一种生成聚类随机数据的方法，生成的数据是正太分布的，这里的意思是生成 n_samples 个 n_features 个特征的 centers 个中心的随机种子为 random_state 的数据（所有类数据一共 n_sample 个） 1X, y = make_blobs(n_samples=400000, n_features=50, centers=2, random_state=6) 1234%%time# 用较大的 C 达到硬边界的效果clf = svm.SVC(kernel='linear', C=1000)clf.fit(X, y) Wall time: 833 ms SVC(C=1000, break_ties=False, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) 123%%timeclf = svm.LinearSVC(C=1000)clf.fit(X, y) Wall time: 2 s LinearSVC(C=1000, class_weight=None, dual=True, fit_intercept=True, intercept_scaling=1, loss='squared_hinge', max_iter=1000, multi_class='ovr', penalty='l2', random_state=None, tol=0.0001, verbose=0) 完整的例子如下： 12345678910111213141516171819202122232425262728# we create 40 separable pointsX, y = make_blobs(n_samples=40, centers=2, random_state=6)# fit the model, don't regularize for illustration purposesclf = svm.SVC(kernel='linear', C=1000)clf.fit(X, y)plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)# plot the decision functionax = plt.gca()xlim = ax.get_xlim()ylim = ax.get_ylim()# create grid to evaluate modelxx = np.linspace(xlim[0], xlim[1], 30)yy = np.linspace(ylim[0], ylim[1], 30)YY, XX = np.meshgrid(yy, xx)xy = np.vstack([XX.ravel(), YY.ravel()]).TZ = clf.decision_function(xy).reshape(XX.shape)# plot decision boundary and marginsax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])# plot support vectorsax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100, linewidth=1, facecolors='none', edgecolors='k')plt.show() 第 3 个例子：多类别 svm 参见文档 先看两个量/方法的含义： 1clf.decision_function? 1clf.decision_function_shape 'ovr' 根据提示信息可知 clf.decision_function 方法要求输入预测点的坐标，之后给出每个二分类函数的值（因为多分类无论是 one-vs-one 还是 one-vs-rest 都有不止一个二分类函数），而 clf.decision_function_shape 有两个参数可以选择，分别是 ovo 和 ovr 当不指定时，SVC 和 NuSVC 默认都是使用 ovo 方法，而 LinearSVC 默认使用 ovr 方法。 123456789101112X = [[0], [1], [2], [3]]Y = [0, 1, 2, 3]clf = svm.SVC(decision_function_shape='ovo')clf.fit(X, Y)dec = clf.decision_function([[1]])print(dec)print(dec.shape[1]) # 4 classes: 4*3/2 = 6clf.decision_function_shape = \"ovr\"dec = clf.decision_function([[1]])print(dec)print(dec.shape[1]) # 4 classes [[-0.55067104 0. 0.40856676 0.55067104 0.9592378 0.40856676]] 6 [[ 1.9585256 3.22442151 0.9585256 -0.21327254]] 4 12345lin_clf = svm.LinearSVC()lin_clf.fit(X, Y)dec = lin_clf.decision_function([[0],[1],[2],[3]])print(dec)print(dec.shape[1]) # 4 classes [[ 0.36363856 -0.17093573 -0.5811868 -0.99145989] [-0.54544576 -0.37603944 -0.47859789 -0.58122471] [-1.45453009 -0.58114314 -0.37600898 -0.17098952] [-2.36361441 -0.78624685 -0.27342007 0.23924566]] 4 第 4 个例子：归一化处理与 svm 参见文档 在 svm 的使用中，归一化有时可以提供更好的分类器，因为当不同特征的方差相差太大时往往导致难以选出合适的分类器。 下面这一段程序中，make_pipeline 的作用是把多个 estimator 级联成一个 estimator，这么做的原因是考虑了数据处理过程中一系列前后相继的固定流程，例如：feature selection-&gt;normalization-&gt;classification，使用 pipeline 的好处是只需要调用一次 fit 或 predict 就可以在数据集上训练一组 estimators，并且可以把网格搜索一次性用在 pipeline 中所有的 estimators 参数的参数组合上面。 注意：Pipleline中最后一个之外的所有estimators都必须是变换器（transformers），最后一个estimator可以是任意类型（transformer，classifier，regresser） 如果最后一个estimator是个分类器，则整个pipeline就可以作为分类器使用，如果最后一个estimator是个聚类器，则整个pipeline就可以作为聚类器使用。 这段对 pipeline 的介绍参见博客园 StandardScaler 是一个提供对数据标准化的 estimator，进行的处理包括将均值转移到 0 和将标准差化为 1 两项。 12345678import numpy as npfrom sklearn.pipeline import make_pipelinefrom sklearn.preprocessing import StandardScalerX = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])y = np.array([1, 1, 2, 2])from sklearn.svm import SVCclf = make_pipeline(StandardScaler(), SVC(gamma='auto'))clf.fit(X, y) Pipeline(memory=None, steps=[('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('svc', SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False))], verbose=False) 1print(clf.predict([[-0.8, -1]])) [1] 第 5 个例子：使用 svm 做手写数据识别 参见文档 数据集的名称是 digits，详细介绍和相关操作见tutorial 123456from sklearn import datasetsdigits = datasets.load_digits()print(digits.data)print(digits.data.shape)print(digits.images[0])print(digits.images.shape) [[ 0. 0. 5. ... 0. 0. 0.] [ 0. 0. 0. ... 10. 0. 0.] [ 0. 0. 0. ... 16. 9. 0.] ... [ 0. 0. 1. ... 6. 0. 0.] [ 0. 0. 2. ... 12. 0. 0.] [ 0. 0. 10. ... 12. 1. 0.]] (1797, 64) [[ 0. 0. 5. 13. 9. 1. 0. 0.] [ 0. 0. 13. 15. 10. 15. 5. 0.] [ 0. 3. 15. 2. 0. 11. 8. 0.] [ 0. 4. 12. 0. 0. 8. 8. 0.] [ 0. 5. 8. 0. 0. 9. 8. 0.] [ 0. 4. 11. 0. 1. 12. 7. 0.] [ 0. 2. 14. 5. 10. 12. 0. 0.] [ 0. 0. 6. 13. 10. 0. 0. 0.]] (1797, 8, 8) 1digits.target array([0, 1, 2, ..., 8, 9, 8]) 先对这个数据集的分类进行尝试： 1234from sklearn import svmclf = svm.SVC(gamma=0.001, C=100.)clf.fit(digits.data[:-1], digits.target[:-1])# 保留最后一个数据用来检验 SVC(C=100.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) 1clf.predict(digits.data[-1:]) array([8]) 12# 展示一下最后一张图的数字标签，发现确实是8，说明上面的分类是正确的。digits.target[-1] 1from PIL import Image 12345# 展示图像有两种方法，一种是用 PIL 库中的 Image.show，另一种是 matplotlib.pyplot.imshow，下面分别展示：# 第一种方法：# 展示一下最后一张图im = Image.fromarray(digits.images[-1])im.show() 123# 第二种方法# cmap 表示涂色风格，interpolation 表示插值方法plt.imshow(digits.images[-1], cmap=plt.cm.gray_r, interpolation='nearest') &lt;matplotlib.image.AxesImage at 0x206560ce848&gt; 下面增加绘图功能并展示结果的好坏，形成一段完整的代码： 123456789101112# Author: Gael Varoquaux &lt;gael dot varoquaux at normalesup dot org&gt;# License: BSD 3 clause# Standard scientific Python importsimport matplotlib.pyplot as plt# Import datasets, classifiers and performance metricsfrom sklearn import datasets, svm, metricsfrom sklearn.model_selection import train_test_split# The digits datasetdigits = datasets.load_digits() 12345678910111213141516171819202122232425262728293031323334353637383940# The data that we are interested in is made of 8x8 images of digits, let's# have a look at the first 4 images, stored in the `images` attribute of the# dataset. If we were working from image files, we could load them using# matplotlib.pyplot.imread. Note that each image must have the same size. For these# images, we know which digit they represent: it is given in the 'target' of# the dataset._, axes = plt.subplots(2, 4)# zip 函数把其中的可迭代类型的参数按照一对一的形成分别组成元组，得到一个新的可迭代类型images_and_labels = list(zip(digits.images, digits.target))for ax, (image, label) in zip(axes[0, :], images_and_labels[:4]): ax.set_axis_off() # 不显示坐标轴 ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest') ax.set_title('Training: %i' % label) # To apply a classifier on this data, we need to flatten the image, to# turn the data in a (samples, feature) matrix:n_samples = len(digits.images)data = digits.images.reshape((n_samples, -1))# 这一步也可以直接用 data = digits.data 代替，因为这个数据集是内置数据集，本身有这个展开后的形式# 对于一般的读入的图像数据，就需要用上面的步骤将其展开成一维数据了# Create a classifier: a support vector classifierclassifier = svm.SVC(gamma=0.001)# 将数据分成训练集和测试集X_train, X_test, y_train, y_test = train_test_split( data, digits.target, test_size=0.5, shuffle=False)# We learn the digits on the first half of the digitsclassifier.fit(X_train, y_train)# Now predict the value of the digit on the second half:predicted = classifier.predict(X_test)images_and_predictions = list(zip(digits.images[n_samples // 2:], predicted))for ax, (image, prediction) in zip(axes[1, :], images_and_predictions[:4]): ax.set_axis_off() ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest') ax.set_title('Prediction: %i' % prediction) 1234print(\"Classification report for classifier %s:\\n%s\\n\" % (classifier, metrics.classification_report(y_test, predicted)))disp = metrics.plot_confusion_matrix(classifier, X_test, y_test)disp.figure_.suptitle(\"Confusion Matrix\") Classification report for classifier SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False): precision recall f1-score support 0 1.00 0.99 0.99 88 1 0.99 0.97 0.98 91 2 0.99 0.99 0.99 86 3 0.98 0.87 0.92 91 4 0.99 0.96 0.97 92 5 0.95 0.97 0.96 91 6 0.99 0.99 0.99 91 7 0.96 0.99 0.97 89 8 0.94 1.00 0.97 88 9 0.93 0.98 0.95 92 accuracy 0.97 899 macro avg 0.97 0.97 0.97 899 weighted avg 0.97 0.97 0.97 899 Text(0.5, 0.98, 'Confusion Matrix') 12print(\"Confusion matrix:\\n%s\" % disp.confusion_matrix)plt.show() Confusion matrix: [[87 0 0 0 1 0 0 0 0 0] [ 0 88 1 0 0 0 0 0 1 1] [ 0 0 85 1 0 0 0 0 0 0] [ 0 0 0 79 0 3 0 4 5 0] [ 0 0 0 0 88 0 0 0 0 4] [ 0 0 0 0 0 88 1 0 0 2] [ 0 1 0 0 0 0 90 0 0 0] [ 0 0 0 0 0 1 0 88 0 0] [ 0 0 0 0 0 0 0 0 88 0] [ 0 0 0 1 0 1 0 0 0 90]]","permalink":"http://yangtf983.github.io/2020/07/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BC%96%E7%A8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BC%96%E7%A8%8B01%EF%BC%9ASVM/","photos":[]},{"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"title":"机器学习技法笔记4：Soft-Margin Support Vector Machine","date":"2020/07/02","text":"0 说明 上一讲介绍了 kernel，目的是减少在高维变换时候的计算，达到“偷吃步”的目的。连同之前的所有讲，讲的都是 hard-margin 下的 SVM，意思是找到的分类器不能错分任何一个数据点。这一讲的目的是把 SVM 扩展到 soft-margin 情形，我们希望通过牺牲一些观测集的准确度，达到一些更重要的目的，例如过拟合。 1 Motivation and Primal 先通过一个例子看一下把 SVM 从 hard-margin 推广到 soft-margin 的动机： hard-margin 是可以适用所有数据情形的，只要不断增加变换次数，总能找到一个能够全部分类正确的分类器。一个极端的例子是高斯核，之前已经讲过，当高斯核的参数 γ\\gammaγ 不断增大时，分类器会收缩为围绕着某类点的小圆圈。所以无论数据有多复杂，都可以用高斯核得到 hard-margin 情形下的分类器。这也就带来了另一个问题——过拟合。看上图，为了得到一个 hard-margin 的分类器，分类器变得很复杂，但是也仅仅是比线性情况多分对了两个，显然左边的分类器推广能力更强。我们认为这样的 hard-margin 是得不偿失的，因此我们希望我们的模型中能够允许一些分类错误的点的存在，以此避免过拟合，增加分类器的推广能力。 这种想法在基石课程中的一个算法中已经有所体现，那就是 pocket 算法，这个算法在线性分类器的假设下寻找分类错误最少的分类器。现在考虑把分类错误尽量小这个条件加入 hard-margin SVM 的目标函数中，把限制条件改成在分类正确的点满足对应要求。这一步如下图： 上面修改后的最优化问题中的两个限制条件可以用同一个公式表达，修改如下： min⁡b,w12wTw+C⋅∑n=1N[yn≠sign⁡(wTzn+b)] s.t. yn(wTzn+b)≥1−∞⋅[yn≠sign⁡(wTzn+b)]\\begin{array}{cl} \\min _{b, w} &amp; \\frac{1}{2} \\mathbf{w}^{T} \\mathbf{w}+C \\cdot \\sum_{n=1}^{N}\\left[y_{n} \\neq \\operatorname{sign}\\left(\\mathbf{w}^{T} \\mathbf{z}_{n}+b\\right)\\right] \\\\ \\text { s.t. } &amp; y_{n}\\left(\\mathbf{w}^{T} \\mathbf{z}_{n}+b\\right) \\geq 1-\\infty \\cdot\\left[y_{n} \\neq \\operatorname{sign}\\left(\\mathbf{w}^{T} \\mathbf{z}_{n}+b\\right)\\right] \\end{array}minb,w​ s.t. ​21​wTw+C⋅∑n=1N​[yn​​=sign(wTzn​+b)]yn​(wTzn​+b)≥1−∞⋅[yn​​=sign(wTzn​+b)]​ 这个表达形式看起来不复杂，但是面临两个重要的问题： 其中有 bool 变量，不再是一个二次规划问题，难以求解（这一点在 TSVM 中体现最严重，完全影响到求解，而 soft-margin svm 是消除了这一点的影响）； 这个形式无法区分较小的错误和较大的错误，有的点虽然预测错误但是还是在分类器的附近，有的点却离分类器很远，后者更应当尽可能减少。 这样就要求我们对上面的形式进行一些修改。首先，引入一系列的 ξn\\xi_nξn​ 表示分类错误的多少；其次将它放入目标函数中代替分类错误的数据点个数，减少 ξn\\xi_nξn​ 也就是减少分类错误。按照这种想法修改后的最优化问题如下： min⁡b,w,ξ12wTw+C⋅∑n=1Nξn s.t. yn(wTzn+b)≥1−ξn and ξn≥0 for all n\\begin{aligned} &amp;\\min _{b, w, \\xi} \\frac{1}{2} w^{T} w+C \\cdot \\sum_{n=1}^{N} \\xi_{n}\\\\ &amp;\\text { s.t. } \\quad y_{n}\\left(\\mathbf{w}^{T} \\mathbf{z}_{n}+b\\right) \\geq 1-\\xi_{n} \\text { and } \\xi_{n} \\geq 0 \\text { for all } n \\end{aligned}​b,w,ξmin​21​wTw+C⋅n=1∑N​ξn​ s.t. yn​(wTzn​+b)≥1−ξn​ and ξn​≥0 for all n​ 这就是我们要引入的 soft-margin SVM 的优化问题。 容易看出这种形式依然是一个二次规划问题。 当 ξn&gt;0\\xi_n&gt;0ξn​&gt;0 时，说明对应的点不在 margin 外面或者分类错误。例如下图，图中不在 margin 外面的蓝色点对应的 ξ&gt;0\\xi&gt;0ξ&gt;0，但是分类是正确的。所以称 ξn\\xi_nξn​ 为 margin violation 更加合适。 当参数 CCC 变大时，破坏 margin 带来的影响变大；反之，mragin 的大小的影响变大。 这个优化问题中 www 是 d~\\tilde{d}d~ 维的，再加上 b 和 ξn\\xi_nξn​ 一共有 d~+1+N\\tilde{d}+1+Nd~+1+N 个变量，2N2N2N 个限制条件。因此下一步的目的是寻找对偶问题消除 d~\\tilde{d}d~ 的影响。 2 Dual Problem 原问题是： min⁡b,w,ξ12wTw+C⋅∑n=1Nξn s.t. yn(wTzn+b)≥1−ξn and ξn≥0 for all n\\begin{aligned} &amp;\\min _{b, w, \\xi} \\frac{1}{2} w^{T} w+C \\cdot \\sum_{n=1}^{N} \\xi_{n}\\\\ &amp;\\text { s.t. } \\quad y_{n}\\left(\\mathbf{w}^{T} \\mathbf{z}_{n}+b\\right) \\geq 1-\\xi_{n} \\text { and } \\xi_{n} \\geq 0 \\text { for all } n \\end{aligned}​b,w,ξmin​21​wTw+C⋅n=1∑N​ξn​ s.t. yn​(wTzn​+b)≥1−ξn​ and ξn​≥0 for all n​ 对应的拉格朗日函数： L(b,w,ξ,α,β)=12wTw+C⋅∑n=1Nξn+∑n=1Nαn⋅(1−ξn−yn(wTzn+b))+∑n=1Nβn⋅(−ξn)\\begin{aligned} \\mathcal{L}(b, \\mathbf{w}, \\boldsymbol{\\xi}, \\boldsymbol{\\alpha}, \\boldsymbol{\\beta})=&amp; \\frac{1}{2} \\mathbf{w}^{T} \\mathbf{w}+C \\cdot \\sum_{n=1}^{N} \\xi_{n} \\\\ &amp;+\\sum_{n=1}^{N} \\alpha_{n} \\cdot\\left(1-\\xi_{n}-y_{n}\\left(\\mathbf{w}^{T} \\mathbf{z}_{n}+b\\right)\\right)+\\sum_{n=1}^{N} \\beta_{n} \\cdot\\left(-\\xi_{n}\\right) \\end{aligned}L(b,w,ξ,α,β)=​21​wTw+C⋅n=1∑N​ξn​+n=1∑N​αn​⋅(1−ξn​−yn​(wTzn​+b))+n=1∑N​βn​⋅(−ξn​)​ 现在的目标转化为： max⁡αn≥0,βn≥0 (min⁡b,w,ξ L(b,w,ξ,α,β))\\max_{\\alpha_n\\ge0,\\beta_n\\ge0}\\ \\left(\\min_{b,\\mathbf{w},\\boldsymbol{\\xi}} \\ \\mathcal{L}(b, \\mathbf{w}, \\boldsymbol{\\xi}, \\boldsymbol{\\alpha}, \\boldsymbol{\\beta})\\right) αn​≥0,βn​≥0max​ (b,w,ξmin​ L(b,w,ξ,α,β)) 首先对 ξn\\xi_nξn​ 求偏导令其为 0 可以得到 C−αn−βn=0C-\\alpha_n-\\beta_n=0C−αn​−βn​=0，这样可以把 L\\mathcal{L}L 中 的 βn\\beta_nβn​ 用 C−αnC-\\alpha_{n}C−αn​ 换掉，此时外层的条件 βn≥0\\beta_n\\ge0βn​≥0 需要等价地换成 0≤αn≤C0\\le\\alpha_n\\le C0≤αn​≤C，L\\mathcal{L}L 中的 ξn\\xi_nξn​ 项恰好全部消掉，得到最优化问题为： max⁡0≤αn≤C,βn=C−αn(min⁡b,w12wTw+∑n=1Nαn(1−yn(wTzn+b)))\\max _{0 \\leq \\alpha_{n} \\leq C, \\beta_{n}=C-\\alpha_{n}}\\left(\\min _{b, w} \\frac{1}{2} \\mathbf{w}^{T} \\mathbf{w}+\\sum_{n=1}^{N} \\alpha_{n}\\left(1-y_{n}\\left(\\mathbf{w}^{T} \\mathbf{z}_{n}+b\\right)\\right)\\right) 0≤αn​≤C,βn​=C−αn​max​(b,wmin​21​wTw+n=1∑N​αn​(1−yn​(wTzn​+b))) 这个最优化问题内部的公式和之前 hard-margin 情形下的拉格朗日函数相同，区别在于外部增加了一个条件 αn≤C\\alpha_n\\le Cαn​≤C，这样就可以采用第二讲中相同的方法，分别对 www 和 bbb 求偏导并令其为 0 得到两个关系式，再将其代入上述最大最小化问题中。两个关系式见下图： 使用这种方法得到标准软边界 SVM 的对偶问题与硬边界的很像，只是增加了 αn\\alpha_nαn​ 的上界。这个问题和隐含的条件表示如下： 注意到这个问题中有 N 个变量和 2N+1 个限制条件。 w 和 b 的计算方法从表面看和之前都是一样的，但是 b 的计算的条件和之前有些不一样，这里根据李航的《统计学习方法（第2版）》P128 的定理7.3简单补充一下： 求解公式：w=Σn=1Nαnynznw=\\Sigma_{n=1}^N\\alpha_ny_nz_nw=Σn=1N​αn​yn​zn​， b=yn−wTzn, for αn∈(0,C)b=y_n-w^Tz_n,\\ for\\ \\alpha_n\\in(0,C)b=yn​−wTzn​, for αn​∈(0,C) www 的计算公式不用说，就是之前令偏导数等于 0 得到的。 bbb 的计算公式源于 C−αn−βn=0C-\\alpha_n-\\beta_n=0C−αn​−βn​=0 和两个隐藏条件，那就是 αn⋅(1−ξn−yn(wTzn+b))=0\\alpha_{n} \\cdot\\left(1-\\xi_{n}-y_{n}\\left(\\mathbf{w}^{T} \\mathbf{z}_{n}+b\\right)\\right)=0αn​⋅(1−ξn​−yn​(wTzn​+b))=0 和 βn⋅ξn=0\\beta_{n} \\cdot\\xi_{n}=0βn​⋅ξn​=0，这两点的来源可以参考第二讲第一节的推导。根据这三个条件，当 αn∈(0,C)\\alpha_n\\in(0,C)αn​∈(0,C) 时，βn=C−αn≠0\\beta_n=C-\\alpha_n\\ne0βn​=C−αn​​=0，从第二个隐藏条件出发得出 ξn=0\\xi_n=0ξn​=0，再从第一个隐藏条件出发得出 1−ξn−yn(wTzn+b)=1−yn(wTzn+b)=01-\\xi_{n}-y_{n}\\left(\\mathbf{w}^{T} \\mathbf{z}_{n}+b\\right)=1-y_{n}\\left(\\mathbf{w}^{T} \\mathbf{z}_{n}+b\\right)=01−ξn​−yn​(wTzn​+b)=1−yn​(wTzn​+b)=0，因此 b=yn−wTznb=y_n-w^Tz_nb=yn​−wTzn​ 3 Messages 和上一讲一样，分类器的推广只需要将向量内积换成 kernel function即可，这样 qn,m=ynymK(xn,xm)q_{n,m}=y_ny_mK(x_n,x_m)qn,m​=yn​ym​K(xn​,xm​)，分类器 gSVM(X)=sign(ΣSV indices nαnynK(xn,x)+b)g_{SVM}(X)=sign\\left(\\Sigma_{SV\\ indices\\ n}\\alpha_ny_nK(x_n,x)+b\\right)gSVM​(X)=sign(ΣSV indices n​αn​yn​K(xn​,x)+b) soft-margin SVM 问题中，我们将 αn&gt;0\\alpha_n&gt;0αn​&gt;0 的向量称为支持向量，将αn&lt;C\\alpha_n&lt;Cαn​&lt;C 的向量称为自由向量，这样就可以将上一节最后所说的满足 αn∈(0,C)\\alpha_n\\in(0,C)αn​∈(0,C) 的点称为 free SV，再加上 kernel，计算 b 的公式就变成： b=ys−ΣSV indices nαnynK(xn,xs), for free SV (xs,ys)b=y_s-\\Sigma_{SV\\ indices\\ n}\\alpha_ny_nK(x_n,x_s),\\ for\\ free\\ SV\\ (x_s,y_s) b=ys​−ΣSV indices n​αn​yn​K(xn​,xs​), for free SV (xs​,ys​) 极少数的情况下会遇到没有 free SV 的情形，这时只能得到 b 的一个范围。 下面看一个分类器随着 C 的选择而变化的例子： 从中可以看到，当 C 过大时，由于过于要求分类正确，有可能会过拟合，所以 C 的选取需要小心谨慎。 根据 αn\\alpha_nαn​ 的数值不同，可以把数据点分为三类，分别是：non SV, free SV, bounded SV. 展示如下： 4 Model Selection 通常有一些参数需要通过实际检验来确定，例如 Gaussian SVM 需要选择 (C,γ)(C,\\gamma)(C,γ)，选择方法一般是交叉验证。下图是对 9 种 Gaussian SVM 交叉验证的错误率，横轴代表不同的 CCC，纵轴代表不同的 γ\\gammaγ，采用的验证方法为 V-fold cross validation. 交叉验证是 soft-margin SVM 方法最常用的选参方法。 由于 SVM 本身的一些性质，可以先用另一个数据来判断分类器的好坏，若结果已经足够好，则不需要再花时间来做交叉验证，这个数据就是支持向量的比例，这个数据和 Leave-One-Out CV Error 有关，关系是：Eloocv≤#SVNE_{loocv}\\le \\frac{\\#SV}{N}Eloocv​≤N#SV​，根据这个关系可知，若支持向量的比例已经低到心中的预期值，则无需再进行交叉验证，此时的 Leave-One-Out CV Error 一定低于预期值。 下面证明这个关系： 首先 soft-margin SVM 的非支持向量的分类都是正确的，并且去掉其中任何一个向量后训练的分类器都不变，所以如果 leave 的是他们中的一个，则分类都会是正确的，而支持向量被 leave 后重新训练就不一定了，我们不妨取个极端，假设支持向量的 leave one 的训练结果都被错分，那么我们的 Eloocv=#SVNE_{loocv}= \\frac{\\#SV}{N}Eloocv​=N#SV​，所以我们得到 Eloocv≤#SVNE_{loocv}\\le \\frac{\\#SV}{N}Eloocv​≤N#SV​ 针对上一个例子算出每种情形的支持向量数量如下： 两张图对比可以发现支持向量的数量有一定的参考价值。但是它只是一个上限，不够准确，一般的做法是仅仅用它排除掉一些过于“危险的”模型，其他模型还是采取交叉验证的方法。在上面的例子中就可以把第一行的排除，下面两行用交叉验证。","permalink":"http://yangtf983.github.io/2020/07/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%B3%95%E7%AC%94%E8%AE%B04%EF%BC%9ASoft-Margin%20Support%20Vector%20Machine/","photos":[]},{"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"title":"机器学习技法笔记3：Kernel Support Vector Machine","date":"2020/07/01","text":"0 说明 上一讲导出了 hard-margin 情形下 SVM 的对偶解法，这种方法表面上与特征空间的维度 d~\\tilde{d}d~ 无关，但是这一步隐含在求内积 znTzmz_n^Tz_mznT​zm​ 当中了，当特征空间维度很大时，依然会有非常大的开销。 这一讲的目的是找到一种求解的复杂度与 d~\\tilde{d}d~ 无关的 SVM 解法，由此引入了 kernel 这一重要概念。 1 Kernel Trick 之前的计算方法是 znTzm=Φ(xn)TΦ(xm)z_n^Tz_m = \\Phi(x_n)^T\\Phi(x_m)znT​zm​=Φ(xn​)TΦ(xm​)，显然时间复杂度必然大于 O(d~)O(\\tilde{d})O(d~)，想要使得时间复杂度变小，一种思考方式是能不能把 Φ(xn)TΦ(xm)\\Phi(x_n)^T\\Phi(x_m)Φ(xn​)TΦ(xm​) 合并成一个不需要做 d~\\tilde{d}d~ 维向量内积的函数。 这里我们不妨给 x 增加一个维度 x0=1x_0=1x0​=1，这样既对 SVM 没什么影响，又可以方便之后的推导，下面以二次多项式变换为例，说明如何简化运算： 对 x 空间中的向量二次多项式变换后的向量可以表示为：Φ2(x)=(1,x1,…,xd,x12,x1x2,…,x1xd,x2x1,x22,…,x2xd,…,xd2)\\Phi_2(x)=(1,x_1,\\ldots,x_d,x_1^2,x_1x_2,\\ldots,x_1x_d,x_2x_1,x_2^2,\\ldots,x_2x_d,\\ldots,x_d^2)Φ2​(x)=(1,x1​,…,xd​,x12​,x1​x2​,…,x1​xd​,x2​x1​,x22​,…,x2​xd​,…,xd2​)，为容易理解其中同时包含了 x1x2x_1x_2x1​x2​ 和 x2x1x_2x_1x2​x1​，此时变换后空间中向量的内积可以做如下转化： Φ2(x)TΦ2(x′)=1+∑i=1dxixi′+∑i=1d∑j=1dxixjxi′xj′=1+∑i=1dxixi′+∑i=1dxixi′∑j=1dxjxj′=1+xTx′+(xTx′)(xTx′)\\begin{aligned} \\Phi_{2}(x)^{T} \\Phi_{2}\\left(x^{\\prime}\\right) &amp;=1+\\sum_{i=1}^{d} x_{i} x_{i}^{\\prime}+\\sum_{i=1}^{d} \\sum_{j=1}^{d} x_{i} x_{j} x_{i}^{\\prime} x_{j}^{\\prime} \\\\ &amp;=1+\\sum_{i=1}^{d} x_{i} x_{i}^{\\prime}+\\sum_{i=1}^{d} x_{i} x_{i}^{\\prime} \\sum_{j=1}^{d} x_{j} x_{j}^{\\prime} \\\\ &amp;=1+x^{T} x^{\\prime}+\\left(x^{T} x^{\\prime}\\right)\\left(x^{T} x^{\\prime}\\right) \\end{aligned}Φ2​(x)TΦ2​(x′)​=1+i=1∑d​xi​xi′​+i=1∑d​j=1∑d​xi​xj​xi′​xj′​=1+i=1∑d​xi​xi′​+i=1∑d​xi​xi′​j=1∑d​xj​xj′​=1+xTx′+(xTx′)(xTx′)​ 这样无论变换后的特征空间的维度是多少，计算 Φ2(x)TΦ2(x′)\\Phi_{2}(x)^{T} \\Phi_{2}\\left(x^{\\prime}\\right)Φ2​(x)TΦ2​(x′) 都只有 O(d)O(d)O(d) 程度的复杂度。 这种方式实际上是把转换和内积放在一起做了，虽然无法得到转换后的坐标，但是降低了时间复杂度。我们把这种将转换和内积合在一步进行的操作成为 kernel function，一个转换就对应一个 kernel function，例如 Φ2⟺KΦ2(x,x′)=1+(xTx′)+(xTx′)2\\Phi_{2} \\Longleftrightarrow K_{\\Phi_{2}}\\left(\\mathbf{x}, \\mathbf{x}^{\\prime}\\right)=1+\\left(\\mathbf{x}^{T} \\mathbf{x}^{\\prime}\\right)+\\left(\\mathbf{x}^{T} \\mathbf{x}^{\\prime}\\right)^{2}Φ2​⟺KΦ2​​(x,x′)=1+(xTx′)+(xTx′)2 有了 kernel fuction，接下来就是把它用到 SVM 求解过程中降低运算开销，这个过程中 kernel function 的用处主要在三个地方，分别如下： 二次项系数 qn,m=ynymznTzm=ynymK(xn,xm)q_{n, m}=y_{n} y_{m} z_{n}^{T} z_{m}=y_{n} y_{m} K\\left(\\mathbf{x}_{n}, \\mathbf{x}_{m}\\right)qn,m​=yn​ym​znT​zm​=yn​ym​K(xn​,xm​) 最优解的 b 的计算公式现表示为：对于任意一个支持向量 SV(xs,ys)SV(x_s,y_s)SV(xs​,ys​)， b=ys−wTzs=ys−(∑n=1Nαnynzn)Tzs=ys−∑n=1Nαnyn(K(xn,xs))\\begin{aligned}b&amp;=y_{s}-\\mathbf{w}^{T} \\mathbf{z}_{s}\\\\ &amp;=y_{s}-\\left(\\sum_{n=1}^{N} \\alpha_{n} y_{n} \\mathbf{z}_{n}\\right)^{T} \\mathbf{z}_{s}\\\\ &amp;=y_{s}-\\sum_{n=1}^{N} \\alpha_{n} y_{n}\\left(K\\left(\\mathbf{x}_{n}, \\mathbf{x}_{s}\\right)\\right) \\end{aligned}b​=ys​−wTzs​=ys​−(n=1∑N​αn​yn​zn​)Tzs​=ys​−n=1∑N​αn​yn​(K(xn​,xs​))​ 最优分类器表示为： gsvm⁡(x)=sign⁡(wTΦ(x)+b)=sign⁡(∑n=1NαnynK(xn,x)+b)\\begin{aligned} g_{\\operatorname{svm}}(\\mathrm{x})&amp;=\\operatorname{sign}\\left(\\mathbf{w}^{T} \\Phi(\\mathrm{x})+b\\right)\\\\ &amp;=\\operatorname{sign}\\left(\\sum_{n=1}^{N} \\alpha_{n} y_{n} K\\left(\\mathbf{x}_{n}, \\mathrm{x}\\right)+b\\right) \\end{aligned} gsvm​(x)​=sign(wTΦ(x)+b)=sign(n=1∑N​αn​yn​K(xn​,x)+b)​ 这时使用 kernel 的 SVM 的二次规划求解方法与相应的复杂度就可以表示如下： 2 Polynomial Kernel 之前的二次变换写做 Φ2(x)=(1,x1,…,xd,x12,x1x2,…,x1xd,x2x1,x22,…,x2xd,…,xd2)\\Phi_2(x)=(1,x_1,\\ldots,x_d,x_1^2,x_1x_2,\\ldots,x_1x_d,x_2x_1,x_2^2,\\ldots,x_2x_d,\\ldots,x_d^2)Φ2​(x)=(1,x1​,…,xd​,x12​,x1​x2​,…,x1​xd​,x2​x1​,x22​,…,x2​xd​,…,xd2​)，这样得到的 kenel function 是 KΦ2(x,x′)=1+(xTx′)+(xTx′)2K_{\\Phi_{2}}\\left(\\mathbf{x}, \\mathbf{x}^{\\prime}\\right)=1+\\left(\\mathbf{x}^{T} \\mathbf{x}^{\\prime}\\right)+\\left(\\mathbf{x}^{T} \\mathbf{x}^{\\prime}\\right)^{2}KΦ2​​(x,x′)=1+(xTx′)+(xTx′)2，现在给其中的一次项都乘上根号 2， 即使用变换 Φ2(x)=(1,2x1,…,2xd,x12,…,xd2)\\Phi_2(x)=(1,\\sqrt{2}x_1,\\ldots,\\sqrt{2}x_d,x_1^2,\\ldots,x_d^2)Φ2​(x)=(1,2​x1​,…,2​xd​,x12​,…,xd2​)，易知此时的 kernel function 为 K2(x,x′)=1+2(xTx′)+(xTx′)2K_{2}\\left(\\mathbf{x}, \\mathbf{x}^{\\prime}\\right)=1+2\\left(\\mathbf{x}^{T} \\mathbf{x}^{\\prime}\\right)+\\left(\\mathbf{x}^{T} \\mathbf{x}^{\\prime}\\right)^{2}K2​(x,x′)=1+2(xTx′)+(xTx′)2，类似的可以给二次项也加上放缩，得到变换 Φ2(x)=(1,2γx1,…,2γxd,γx12,…,γxd2)\\Phi_2(x)=(1,\\sqrt{2\\gamma}x_1,\\ldots,\\sqrt{2\\gamma}x_d,\\gamma x_1^2,\\ldots,\\gamma x_d^2)Φ2​(x)=(1,2γ​x1​,…,2γ​xd​,γx12​,…,γxd2​)，此时的 kernel function 为 K2(x,x′)=1+2γ(xTx′)+γ2(xTx′)2=(1+γxTx′)2 with γ&gt;0K_{2}\\left(\\mathbf{x}, \\mathbf{x}^{\\prime}\\right)=1+2\\gamma \\left(\\mathbf{x}^{T} \\mathbf{x}^{\\prime}\\right)+\\gamma^2\\left(\\mathbf{x}^{T} \\mathbf{x}^{\\prime}\\right)^{2}=(1+\\gamma \\mathbf{x}^{T} \\mathbf{x}^{\\prime})^2\\ with \\ \\gamma&gt;0K2​(x,x′)=1+2γ(xTx′)+γ2(xTx′)2=(1+γxTx′)2 with γ&gt;0 上面第三种形式是更常用的二次变换形式，它包含了第二种，与第一种相比，更容易拓展到更高维度的变换当中。第三种相当于是对第一种的空间进行了拉伸/压缩，也可以看作是变换到相同的空间但是定义了不同的内积，用这种理解比较容易理解二者的最优解的区别，因为在内积空间中不同的内积就代表了不同的距离，也就是说计算 margin 的方式不同，所以内积定义方式不同，在相同的空间中找到的最优解也可能不同。虽然有这点不同，但是我们一般不会两种都尝试，一般直接采用第三种，因为最简单。三者的这些区别见下图： 同一组数据下，使用第一种 kernel 和第三种 kernel 在 γ=0.001\\gamma = 0.001γ=0.001 和 γ=1000\\gamma = 1000γ=1000 这三种情形寻找 SVM 分类器，结果如下图： 容易看出这三种情形下的分类器和支持向量均不相同，不同的 kernel 得到不同的结果，那么就需要对 kernel 做选择，就像以前对变换做选择一样。 进一步可以把这样的 kernel 推广到更高次的多项式变换中： K1(x,x′)=(0+1⋅x⊤x′)1K2(x,x′)=(ζ+γx⊤x′)2 with γ&gt;0,ζ≥0K3(x,x′)=(ζ+γxTx′)3 with γ&gt;0,ζ≥0…KQ(x,x′)=(ζ+γxTx′)Q with γ&gt;0,ζ≥0\\begin{aligned}&amp;K_1\\left(\\mathbf{x}, \\mathbf{x}^{\\prime}\\right)=\\left(0+1 \\cdot \\mathbf{x}^{\\top} \\mathbf{x}^{\\prime}\\right)^{1}\\\\ &amp;K_{2}\\left(\\mathbf{x}, \\mathbf{x}^{\\prime}\\right)=\\left(\\zeta+\\gamma \\mathbf{x}^{\\top} \\mathbf{x}^{\\prime}\\right)^{2} \\text { with } \\gamma&gt;0, \\zeta \\geq 0\\\\ &amp;K_{3}\\left(\\mathbf{x}, \\mathbf{x}^{\\prime}\\right)=\\left(\\zeta+\\gamma \\mathbf{x}^{T} \\mathbf{x}^{\\prime}\\right)^{3} \\text { with } \\gamma&gt;0, \\zeta \\geq 0\\\\&amp;\\ldots\\\\ &amp;K_{Q}\\left(\\mathbf{x}, \\mathbf{x}^{\\prime}\\right)=\\left(\\zeta+\\gamma \\mathbf{x}^{T} \\mathbf{x}^{\\prime}\\right)^{Q} \\text { with } \\gamma&gt;0, \\zeta \\geq 0 \\end{aligned}​K1​(x,x′)=(0+1⋅x⊤x′)1K2​(x,x′)=(ζ+γx⊤x′)2 with γ&gt;0,ζ≥0K3​(x,x′)=(ζ+γxTx′)3 with γ&gt;0,ζ≥0…KQ​(x,x′)=(ζ+γxTx′)Q with γ&gt;0,ζ≥0​ 对于这种使用了多项式 kernel 的 SVM，我们称之为多项式支持向量机（Polynomial SVM），其中 K1K_1K1​ 就是原来的空间，对应之前讲过的 linear SVM，我们称之为 Linear Kernel，linear SVM 的求解用原问题一般比用对偶解法更快。 kernel 的选择除了先验信息外，就是从低次到高次进行尝试，这一点和基石课程所讲是一致的。尝试的过程中使用交叉验证判断当前分类器的优越性。 3 Gaussian Kernel 这一节要介绍的 Gaussian Kernel 可以看做一种阶数为无限多维的变换。下图是在 x 是一维情况下的相关表示： 此时的最优分类器可以如下图表示： 支持向量的 αn&gt;0\\alpha_n&gt;0αn​&gt;0，因此这个分类器可以看做是以支持向量为中心的高斯函数的线性组合，由于高斯函数的图像是从中心向四周辐射的形式，所以这个 kernel 又被称为 Radial Basis Function (RBF) kernel Gaussian SVM 可以理解为是寻找一个在无限维空间中线宽最大的形式为以支持向量为中心的高斯函数的线性组合的分类器。 对同一组数据取不同的 γ\\gammaγ 得到的三种分类器表示如下： 随着 γ\\gammaγ 变大，分类器变成了一个个围绕着支持向量的小圈圈，这一点可以从高斯函数的性质来看，随着 γ\\gammaγ 变大高斯函数变尖了也变陡了，在更抖的地方对应的特征空间中占据的位置往往越大，因此分割处偏向更抖的地方可以在特征空间中有更大的线宽。此外，这样的变化也意味着分类器可能变得过拟合，事实上上图第三个分类器显然就是过拟合的，large-margin 的保护不是万能的。 4 Comparison of Kernels 最后比较一下不同的 kernel 的优缺点，供做 kernel 选择之时进行参考。 Linear Kernel 优点: 安全：不用担心过拟合。 快速：直接用二次规划求解原问题速度就很快。 可解释性强：w 和支持向量的几何意义非常明确。 缺点： 限制性强：数据点不总是线性可分的。 Polynomial Kernel 优点： 限制性弱：增加次数可以适应更多数据。 通过设定 Q 可以比较好地限制模型复杂度。 缺点： 当 Q 比较大时，会遇到数值上的困难：此时，若 ∣ζ+γxTx′∣&lt;1\\left|\\zeta+\\gamma \\mathbf{x}^{T} \\mathbf{x}^{\\prime}\\right|&lt;1∣∣​ζ+γxTx′∣∣​&lt;1，则 K→0K\\rightarrow0K→0；若∣ζ+γxTx′∣&gt;1\\left|\\zeta+\\gamma \\mathbf{x}^{T} \\mathbf{x}^{\\prime}\\right|&gt;1∣∣​ζ+γxTx′∣∣​&gt;1，则 K→bigK\\rightarrow bigK→big ，因此 Polynomial Kernel 不太适合 Q 比较大的情形。 需要选择的参数 (γ,ζ,Q)(\\gamma, \\zeta, Q)(γ,ζ,Q) 太多，需要花费很多时间尝试。 因此只有在心中预期 Q 比较小的情形下才会使用 Polynomial Kernel Gaussian Kernel 优点： 可以做出比前两个更加复杂的分类器。 数值上的困难比 Polynomial Kernel 少一些。 可选的参数只有 γ\\gammaγ，容易选择。 缺点： 难以解释边界的形状（因为没有 w）。 比 Linear Kernel 慢。 参数过大时容易过拟合。 Gaussian Kernel 是最常用的 kernel 之一，但也不是万能的。 如何寻找其他 kernel ? 首先我们介绍的 kernel 是从内积得到的 Φ(xn)TΦ(xm)\\Phi(x_n)^T\\Phi(x_m)Φ(xn​)TΦ(xm​)，内积其实是一种相似性，例如两个向量同向时内积最大，反向时内积最小。 从这个角度来讲，任何一种相似性我们都可以作为 kernel 函数的一个备选，相似性的定义方法有很多。 但是不是所有的相似性都可以作为 kernel，需要满足一些特别的条件，kernel 代表的只是一种相似性，反之相似性可以作为 kernel 是不成立的。 需要满足的条件有两个： 对称性； 令 kij=K(xi,xj)k_{ij}=K(x_i,x_j)kij​=K(xi​,xj​)，则矩阵 KKK 是半正定的，KKK 的表示为： K=[Φ(x1)TΦ(x1)Φ(x1)TΦ(x2)…Φ(x1)TΦ(xN)Φ(x2)TΦ(x1)Φ(x2)TΦ(x2)…Φ(x2)TΦ(xN)⋯⋯⋯⋯Φ(xN)TΦ(x1)Φ(xN)TΦ(x2)⋯Φ(xN)TΦ(xN)]=[z1z2…zN]T[z1z2⋯zN]=ZZT\\begin{array}{l} K&amp;=\\left[\\begin{array}{cccc} \\Phi\\left(\\mathbf{x}_{1}\\right)^{T} \\Phi\\left(\\mathbf{x}_{1}\\right) &amp; \\mathbf{\\Phi}\\left(\\mathbf{x}_{1}\\right)^{T} \\mathbf{\\Phi}\\left(\\mathbf{x}_{2}\\right) &amp; \\dots &amp; \\mathbf{\\Phi}\\left(\\mathbf{x}_{1}\\right)^{T} \\mathbf{\\Phi}\\left(\\mathbf{x}_{N}\\right) \\\\ \\mathbf{\\Phi}\\left(\\mathbf{x}_{2}\\right)^{T} \\mathbf{\\Phi}\\left(\\mathbf{x}_{1}\\right) &amp; \\mathbf{\\Phi}\\left(\\mathbf{x}_{2}\\right)^{T} \\mathbf{\\Phi}\\left(\\mathbf{x}_{2}\\right) &amp; \\dots &amp; \\mathbf{\\Phi}\\left(\\mathbf{x}_{2}\\right)^{T} \\mathbf{\\Phi}\\left(\\mathbf{x}_{N}\\right) \\\\ \\cdots &amp; \\cdots &amp; \\cdots &amp; \\cdots \\\\ \\mathbf{\\Phi}\\left(\\mathbf{x}_{N}\\right)^{T} \\mathbf{\\Phi}\\left(\\mathbf{x}_{1}\\right) &amp; \\mathbf{\\Phi}\\left(\\mathbf{x}_{N}\\right)^{T} \\mathbf{\\Phi}\\left(\\mathbf{x}_{2}\\right) &amp; \\cdots &amp; \\mathbf{\\Phi}\\left(\\mathbf{x}_{N}\\right)^{T} \\mathbf{\\Phi}\\left(\\mathbf{x}_{N}\\right) \\end{array}\\right] \\\\ &amp;=\\left[\\begin{array}{cccc} \\mathbf{z}_{1} &amp; \\mathbf{z}_{2} &amp; \\ldots &amp; \\mathbf{z}_{N} \\end{array}\\right]^{T}\\left[\\begin{array}{cccc} \\mathbf{z}_{1} &amp; \\mathbf{z}_{2} &amp; \\cdots &amp; \\mathbf{z}_{N} \\end{array}\\right] \\\\ &amp;= \\mathrm{Z} \\mathrm{Z}^{T} \\end{array}K​=⎣⎢⎢⎡​Φ(x1​)TΦ(x1​)Φ(x2​)TΦ(x1​)⋯Φ(xN​)TΦ(x1​)​Φ(x1​)TΦ(x2​)Φ(x2​)TΦ(x2​)⋯Φ(xN​)TΦ(x2​)​……⋯⋯​Φ(x1​)TΦ(xN​)Φ(x2​)TΦ(xN​)⋯Φ(xN​)TΦ(xN​)​⎦⎥⎥⎤​=[z1​​z2​​…​zN​​]T[z1​​z2​​⋯​zN​​]=ZZT​ 这两个条件是一个函数是 kernel 的充分必要条件，被称为 Mercer’s condition. 定义自己的 kernel 是一个很困难的过程，一般都是使用常用的 kernel，比如前三种。","permalink":"http://yangtf983.github.io/2020/07/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%B3%95%E7%AC%94%E8%AE%B03%EF%BC%9AKernel%20Support%20Vector%20Machine/","photos":[]},{"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"title":"机器学习技法笔记2：Dual Support Vector Machine","date":"2020/07/01","text":"0 说明 这一节介绍 hard-margin SVM 的对偶问题。使用对偶有两个好处，一个是简化计算，另一个是方便引入 kernel 1 Motivation of Dual SVM 上一节课最后提到，如果已经有一个 linear svm，想要在 z 空间中寻找 linear svm，那么可以直接把 x 空间的坐标换成 z 空间的坐标，再用二次规划方法。 这样做的好处是既使用了较复杂的分类器，又使用 large-margin 控制了其复杂度。但是直接使用原来最优化问题的二次规划却容易导致运算量过大（因为 z 空间的维数 d~\\tilde{d}d~ 可能非常大甚至无限大），那么能不能找到一种计算方法计算量不随着 z 空间变复杂而变大太多就成为了一个重要问题。 假设原来的 x 空间有 d 维，则需要解 d+1 个变量，变换后需要解 d~+1\\tilde{d}+1d~+1 个变量，现在的目的就是移除 d~\\tilde{d}d~ 对计算复杂度的影响。达到这个目的的方法就是对偶方法，转换后需要求解的变量始终是 N 个。 这个方法是最优化中最重要的方法之一，一般只需要从形式上推出来对偶问题之后用一定的定理求解即可，这里跟随课程把推演的过程展示一边，但不是每个问题都需要这样从头推演。 min⁡b,w12wTw s.t. yn(wTzn+b)≥1 for n=1,2,…,N\\begin{array}{ll} \\min _{b, \\mathbf{w}} &amp; \\frac{1}{2} \\mathbf{w}^{T} \\mathbf{w} \\\\ \\text { s.t. } &amp; y_{n}\\left(\\mathbf{w}^{T} \\mathbf{z}_{n}+b\\right) \\geq 1 \\\\ &amp; \\text { for } n=1,2, \\ldots, N \\end{array}minb,w​ s.t. ​21​wTwyn​(wTzn​+b)≥1 for n=1,2,…,N​ 用 αn\\alpha_nαn​ 表示拉格朗日乘子，写出这个最优化问题的拉格朗日函数： L(b,w,α)=12wTw+Σn=1Nαn(1−yn(wTzn+b))\\mathcal{L}(b,w,\\alpha)=\\frac12w^Tw+\\Sigma_{n=1}^N\\alpha_n(1-y_n(w^Tz_n+b)) L(b,w,α)=21​wTw+Σn=1N​αn​(1−yn​(wTzn​+b)) 首先需要说明 SVM=min⁡b,w(max⁡all αn≥0L(b,w,α))SVM=\\min_{b,w}(\\max_{all\\ \\alpha_n\\ge0}\\mathcal{L}(b,w,\\alpha))SVM=minb,w​(maxall αn​≥0​L(b,w,α))，这是显然的，因为当约束条件满足时，才有 max⁡all αn≥0L(b,w,α)&lt;∞\\max_{all\\ \\alpha_n\\ge0}\\mathcal{L}(b,w,\\alpha)&lt;\\inftymaxall αn​≥0​L(b,w,α)&lt;∞，否则由于 ∃ n, s.t. 1−yn(wTzn+b)&gt;0\\exist\\ n,\\ s.t.\\ 1-y_n(w^Tz_n+b)&gt;0∃ n, s.t. 1−yn​(wTzn​+b)&gt;0 因此 max⁡all αn≥0L(b,w,α)=∞\\max_{all\\ \\alpha_n\\ge0}\\mathcal{L}(b,w,\\alpha)=\\inftymaxall αn​≥0​L(b,w,α)=∞，因此 min⁡b,w(max⁡all αn≥0L(b,w,α))\\min_{b,w}(\\max_{all\\ \\alpha_n\\ge0}\\mathcal{L}(b,w,\\alpha))minb,w​(maxall αn​≥0​L(b,w,α)) 得到的结果一定满足约束条件，在约束条件满足的情况下拉格朗日乘子为 0 或本身 1−yn(wTzn+b)=01-y_n(w^Tz_n+b)=01−yn​(wTzn​+b)=0 使得 Σn=1Nαn(1−yn(wTzn+b))\\Sigma_{n=1}^N\\alpha_n(1-y_n(w^Tz_n+b))Σn=1N​αn​(1−yn​(wTzn​+b)) 最小，此时 min⁡b,w(max⁡all αn≥0L(b,w,α))=min⁡b,w12wTw\\min_{b,w}(\\max_{all\\ \\alpha_n\\ge0}\\mathcal{L}(b,w,\\alpha))=\\min_{b,w}\\frac12w^Twminb,w​(maxall αn​≥0​L(b,w,α))=minb,w​21​wTw 2 Largange Dual SVM 上一小节证明了 SVM=min⁡b,w(max⁡all αn≥0L(b,w,α))SVM=\\min_{b,w}(\\max_{all\\ \\alpha_n\\ge0}\\mathcal{L}(b,w,\\alpha))SVM=minb,w​(maxall αn​≥0​L(b,w,α))，下面简单说一下它的拉格朗日对偶问题。 当给定 αn=α′\\alpha_n=\\alpha&#x27;αn​=α′ 时，显然有 min⁡b,w(max⁡all αn≥0L(b,w,α))≥min⁡b,wL(b,w,α′)\\min_{b,w}(\\max_{all\\ \\alpha_n\\ge0}\\mathcal{L}(b,w,\\alpha))\\ge \\min_{b,w}\\mathcal{L}(b,w,\\alpha&#x27;)minb,w​(maxall αn​≥0​L(b,w,α))≥minb,w​L(b,w,α′)，因此 min⁡b,w(max⁡all αn≥0L(b,w,α))≥max⁡all αn′≥0min⁡b,wL(b,w,α′)\\min_{b,w}(\\max_{all\\ \\alpha_n\\ge0}\\mathcal{L}(b,w,\\alpha))\\ge \\max_{all\\ \\alpha_n&#x27;\\ge0}\\min_{b,w}\\mathcal{L}(b,w,\\alpha&#x27;)minb,w​(maxall αn​≥0​L(b,w,α))≥maxall αn′​≥0​minb,w​L(b,w,α′)，等式右侧的部分被称为左边最优化问题的拉格朗日对偶问题，这个不等关系被称为弱对偶，数学家已经证明，对于满足下面三个条件的 QP 问题，这个不等关系可以改写为对等关系，称之为强对偶。这三个条件分别是： 凸优化 有可行解 条件为线性限制 显然 SVM 问题符合这三个条件，所以 min⁡b,w(max⁡all αn≥0L(b,w,α))=max⁡all αn′≥0(min⁡b,wL(b,w,α′))\\min_{b,w}(\\max_{all\\ \\alpha_n\\ge0}\\mathcal{L}(b,w,\\alpha))= \\max_{all\\ \\alpha_n&#x27;\\ge0}(\\min_{b,w}\\mathcal{L}(b,w,\\alpha&#x27;))minb,w​(maxall αn​≥0​L(b,w,α))=maxall αn′​≥0​(minb,w​L(b,w,α′))，只需要解右边的问题。 下面讨论如何化简 max⁡all αn≥0(min⁡b,w12wTw+Σn=1Nαn(1−yn(wTzn+b)))\\max_{all\\ \\alpha_n\\ge0}\\left(\\min_{b,w}\\frac12w^Tw+\\Sigma_{n=1}^N\\alpha_n(1-y_n(w^Tz_n+b))\\right)maxall αn​≥0​(minb,w​21​wTw+Σn=1N​αn​(1−yn​(wTzn​+b)))： 第一步：关于 b 求偏导 L\\mathcal{L}L 关于b求偏导得：∂L(b,w,α)∂b=0=−∑n=1Nαnyn\\frac{\\partial \\mathcal{L}(b, \\mathbf{w}, \\alpha)}{\\partial b}=0=-\\sum_{n=1}^{N} \\alpha_{n} y_{n}∂b∂L(b,w,α)​=0=−∑n=1N​αn​yn​，因此可以增加限制条件 Σn=1Nαnyn=0\\Sigma_{n=1}^N\\alpha_ny_n=0Σn=1N​αn​yn​=0，这恰好是 L\\mathcal{L}L 中 b 的系数，因此增加这个限制条件后 b 项可以直接去掉，问题变成max⁡all αn≥0, Σn=1Nαnyn=0(min⁡b,w12wTw+Σn=1Nαn(1−yn(wTzn)))\\max_{all\\ \\alpha_n\\ge0,\\ \\Sigma_{n=1}^N\\alpha_ny_n=0}\\left(\\min_{b,w}\\frac12w^Tw+\\Sigma_{n=1}^N\\alpha_n(1-y_n(w^Tz_n))\\right)maxall αn​≥0, Σn=1N​αn​yn​=0​(minb,w​21​wTw+Σn=1N​αn​(1−yn​(wTzn​))) 第二步：关于w求偏导 使用向量求导公式得：∂L(b,w,α)∂w=0=w−∑n=1Nαnynzn\\frac{\\partial \\mathcal{L}(b, \\mathbf{w}, \\alpha)}{\\partial w}=0=w-\\sum_{n=1}^{N} \\alpha_{n} y_{n}z_n∂w∂L(b,w,α)​=0=w−∑n=1N​αn​yn​zn​，因此可以增加限制条件 w=∑n=1Nαnynznw=\\sum_{n=1}^{N} \\alpha_{n} y_{n}z_nw=∑n=1N​αn​yn​zn​，这时 Σn=1Nαn(1−yn(wTzn))=Σn=1Nαn−wT(Σn=1Nαnynzn)=Σn=1Nαn−wTw\\Sigma_{n=1}^N\\alpha_n(1-y_n(w^Tz_n))=\\Sigma_{n=1}^N\\alpha_n-w^T(\\Sigma_{n=1}^N\\alpha_ny_nz_n)=\\Sigma_{n=1}^N\\alpha_n-w^TwΣn=1N​αn​(1−yn​(wTzn​))=Σn=1N​αn​−wT(Σn=1N​αn​yn​zn​)=Σn=1N​αn​−wTw，优化问题变成 max⁡all αn≥0, Σαnyn=0, w=Σαnynzn(min⁡b,w12wTw+Σn=1Nαn−wTw)\\max_{all\\ \\alpha_n\\ge0,\\ \\Sigma\\alpha_ny_n=0,\\ w=\\Sigma\\alpha_ny_nz_n}\\left(\\min_{b,w}\\frac12w^Tw+\\Sigma_{n=1}^N\\alpha_n-w^Tw\\right)maxall αn​≥0, Σαn​yn​=0, w=Σαn​yn​zn​​(minb,w​21​wTw+Σn=1N​αn​−wTw)，进一步化简得到max⁡all αn≥0, Σαnyn=0, w=Σαnynzn−12∥Σn=1Nαnynzn∥2+Σn=1Nαn\\max_{all\\ \\alpha_n\\ge0,\\ \\Sigma\\alpha_ny_n=0,\\ w=\\Sigma\\alpha_ny_nz_n}-\\frac12\\|\\Sigma_{n=1}^{N}\\alpha_n y_n z_n\\|^2+\\Sigma_{n=1}^N\\alpha_nmaxall αn​≥0, Σαn​yn​=0, w=Σαn​yn​zn​​−21​∥Σn=1N​αn​yn​zn​∥2+Σn=1N​αn​ 简化到这一步就可以求解了，求解这个问题使用的就是以上过程中推导出的隐含的或直接的条件。 整理目前已知的所有条件如下： 在推导 SVM=min⁡b,w(max⁡all αn≥0L(b,w,α))SVM=\\min_{b,w}(\\max_{all\\ \\alpha_n\\ge0}\\mathcal{L}(b,w,\\alpha))SVM=minb,w​(maxall αn​≥0​L(b,w,α)) 时包含了一个隐含条件，最优解处 αn(1−yn(wTzn+b))=0\\alpha_n(1-y_n(w^Tz_n+b))=0αn​(1−yn​(wTzn​+b))=0 对 b 求偏导时得到条件 Σn=1Nαnyn=0\\Sigma_{n=1}^N\\alpha_ny_n=0Σn=1N​αn​yn​=0 对 w 求偏导时得到条件 w=∑n=1Nαnynznw=\\sum_{n=1}^{N} \\alpha_{n} y_{n}z_nw=∑n=1N​αn​yn​zn​ 原始问题可行的条件 yn(wTzn+b)≥1y_n(w^Tz_n+b)\\ge1yn​(wTzn​+b)≥1 对偶问题可行的条件 αn≥0\\alpha_n\\ge0αn​≥0 这些条件叫做 KKT 条件，在最优化问题中这些条件可以用来推导最优解或者简化求解。 对于本节的问题来说，使用KKT条件可以帮助得到最优解。 3 Solving Dual SVM 上一小节最后得到的简化形式是： max⁡all αn≥0, Σαnyn=0, w=Σαnynzn−12∥Σn=1Nαnynzn∥2+Σn=1Nαn\\max_{all\\ \\alpha_n\\ge0,\\ \\Sigma\\alpha_ny_n=0,\\ w=\\Sigma\\alpha_ny_nz_n}-\\frac12\\|\\Sigma_{n=1}^{N}\\alpha_n y_n z_n\\|^2+\\Sigma_{n=1}^N\\alpha_n all αn​≥0, Σαn​yn​=0, w=Σαn​yn​zn​max​−21​∥Σn=1N​αn​yn​zn​∥2+Σn=1N​αn​ 这里首先再做一些形式上的变化，这个最优化问题等价于： min⁡α12∑n=1N∑m=1NαnαmynymznTzm−∑n=1Nαn subject to ∑n=1Nynαn=0αn≥0, for n=1,2,…,N\\begin{array}{ll} \\min _{\\alpha} &amp; \\frac{1}{2} \\sum_{n=1}^{N} \\sum_{m=1}^{N} \\alpha_{n} \\alpha_{m} y_{n} y_{m} \\mathbf{z}_{n}^{T} \\mathbf{z}_{m}-\\sum_{n=1}^{N} \\alpha_{n} \\\\ \\text { subject to } &amp; \\sum_{n=1}^{N} y_{n} \\alpha_{n}=0 \\\\ &amp; \\alpha_{n} \\geq 0, \\text { for } n=1,2, \\ldots, N \\end{array}minα​ subject to ​21​∑n=1N​∑m=1N​αn​αm​yn​ym​znT​zm​−∑n=1N​αn​∑n=1N​yn​αn​=0αn​≥0, for n=1,2,…,N​ 由于有 N 个 αn\\alpha_nαn​，这个问题变成一个有 N 个变量和 N+1 个限制条件的凸二次规划问题。 与上节课相同，这里的思路是采用一些求解二次规划的函数解出 α\\alphaα，进而使用 KKT 条件解出 (b,w)(b,w)(b,w) 用求解器解这个二次规划问题需要会写它的标准型，这一点表示如下图： 由于这时求解的是对偶问题，所以不妨把这个时候的标准形式中的 QQQ 记为 QDQ_DQD​，对于标准形式的求解，可以看一下 qn,mq_{n,m}qn,m​，由于 zn,zmz_n,z_mzn​,zm​ 通常不是正交的，所以 qn,mq_{n,m}qn,m​ 通常不是 0，所以 QDQ_DQD​ 是密集的，上一讲中的二次规划问题的 Q 的非对角线元素全部是 0，从这里来看对偶问题的计算似乎比原问题还困难（当维数较大时，密集矩阵的存储和计算的消耗都非常大），实际上 SVM 问题对应的二次规划是比较特殊的，可以采用一些专门求解SVM 的求解器加速求解。 现在假设已经解得了最优的 α\\alphaα，下面的问题是如何求得 (b,w)(b,w)(b,w)： 根据 w=∑n=1Nαnynznw=\\sum_{n=1}^{N} \\alpha_{n} y_{n}z_nw=∑n=1N​αn​yn​zn​ 容易求得 www 根据 αn(1−yn(wTzn+b))=0\\alpha_n(1-y_n(w^Tz_n+b))=0αn​(1−yn​(wTzn​+b))=0 条件，在 αn&gt;0\\alpha_n&gt;0αn​&gt;0 的位置解 1−yn(wTzn+b)=01-y_n(w^Tz_n+b)= 01−yn​(wTzn​+b)=0 可得 b=yn−wTznb=y_n-w^Tz_nb=yn​−wTzn​ 根据以上两条可以求出 (b,w)(b,w)(b,w) 上面的第二条中，当 1−yn(wTzn+b)=01-y_n(w^Tz_n+b)= 01−yn​(wTzn​+b)=0 时，根据上一讲中的推导，这个限制条件的意义时对应的点在线宽的边界上，我们定义对应于 αn&gt;0\\alpha_n&gt;0αn​&gt;0 的点为支持向量，这就是上一讲中提到的支持向量的标准定义（注意不是所有在线宽边界上的点都是支持向量）。仅仅使用支持向量就可以确定 b 进而确定分类器（从求 w 的公式可以看出 w 也纸盒支持向量有关），这也和我们之前说的“直观上支持向量是决定最优分类器线宽的向量”这一说法相对应。 4 Messages behind Dual SVM 可以从以下三个角度提升对 SVM 对偶解法的理解： 由于 (b,w)(b,w)(b,w) 都只和支持向量有关，所以 SVM 的对偶解法可以看作是一种机制：先找到支持向量，再使用支持向量方便地求出最优解。 wSVM=Σn=1Nαn(ynzn)w_{SVM}=\\Sigma_{n=1}^{N}\\alpha_n(y_nz_n)wSVM​=Σn=1N​αn​(yn​zn​) 这种形式（w 又数据的线性组合表示）被称为 w 是可以用数据代表的，有很多机器学习算法的解都是类似形式，例如使用 GD/SGD 方法求解 wo=0w_o=0wo​=0 的线性回归/逻辑斯蒂回归的 w、PLA 算法的 w 等。但是 SVM 算法的 w 有其特别之处，那就是它可以仅仅用支持向量代表。 对于 hard-margin 情形而言，最好是根据情形选择解原最优化问题还是对偶问题，当 d~\\tilde{d}d~ 不大时，解原最优化问题为佳，当 N 不大时，解对偶问题为佳。二者在物理意义上也有区别，原问题是直接定位 (b,w)(b,w)(b,w)，对偶问题是先定位支持向量，再用 KKT 条件和支持向量定位 (b,w)(b,w)(b,w)，最后得到分类器 gSVM(x)=sign(wTΦ(x)+b)g_{SVM}(x)=sign(w^T\\Phi(x)+b)gSVM​(x)=sign(wTΦ(x)+b)。这两点区别见下图： 最后，看一下目前的对偶问题有没有彻底达到我们的目标：避免 d~\\tilde{d}d~ 太大对求解复杂度的影响。 在最后的对偶问题中，求解变量的数目和 d~\\tilde{d}d~ 无关了，但是 qn,m=ynymznTzmq_{n,m}=y_ny_mz^T_nz_mqn,m​=yn​ym​znT​zm​，其中 znTzmz^T_nz_mznT​zm​ 依然是一个 d~\\tilde{d}d~ 维向量的内积，所以要想达到我们的目标，必须能够避开这个内积的计算，这就是下一讲要讲的问题。","permalink":"http://yangtf983.github.io/2020/07/01/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%B3%95%E7%AC%94%E8%AE%B02%EF%BC%9ADual%20Support%20Vector%20Machine/","photos":[]},{"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"title":"机器学习技法笔记1：Linear SVM","date":"2020/06/30","text":"0 说明 这一节介绍的是线性支持向量机的问题，并且是Hard-Margin情形下的支持向量机。 1 Course Introduction 这门课程比基石更加注重算法的讲解。 这门课程主要围绕三种与特征转换有关的技巧展开，分别是： 如何选择合适的特征转换、控制复杂度？ ​ 这种思想促成了 Support Vector Machine (SVM) 的产生。 如何找到一些合适的特征？能不能将找到的特征混合起来得到更好的表现？ ​ 这种思想促成了 Adaptive Boosting (AdaBoosting，译为：逐步增强法) 的产生。 如何找到/学习到潜在的特征？ ​ 这种思想促成了Deep Learning (DL) 的产生。 2 Large-Margin Seperating Hyperplane 这一节延伸自机器学习基石课程的第2讲，在我的基石系列第2讲的笔记中其实已经包含了这一节讲的大部分内容，下面我们从基石第2讲的基础上再复习一遍。 从PLA算法来看，这三条线都属于能找到的最优分类器，具体是哪个取决于选择训练点的顺序。那么这种情况下（不考虑用validation验证）应该选择哪种作为最终的分类器？ 一种公认合理的选择是第三个分类器，因为这个分类器离点更远，假设点的测量存在误差（高斯分布），则第三个分类器被误差影响到的可能性更小，因为测量点离真实点越远的概率越小，所以分类器离数据越远越好。 从上图中可以看到，我们要寻找的实际上就是最胖的线，线越胖则距离数据点的最短距离越大。 因此我们的最优化问题变成： 这里说的fatness在正式场合上的表达是margin，基石课上也已经讲过对于分类正确的点ynwTxn&gt;0y_nw^Tx_n&gt;0yn​wTxn​&gt;0，因此改写为： 3 Standard Large-Margin Problem 这一节的目的是把最优化问题转化为标准形式。 与PLA的符号表示不同的是，从现在开始，不再把w0w_0w0​并入www中，而是把w0w_0w0​拿出来记为bbb，而w=(w1,…,wd)T ,x=(x1,…,xd)Tw=(w_1,\\ldots,w_d)^T\\ ,x=(x_1,\\ldots,x_d)^Tw=(w1​,…,wd​)T ,x=(x1​,…,xd​)T，此时超平面方程变为wTx′+b=0w^Tx&#x27;+b=0wTx′+b=0，根据高中所学的点到平面的距离公式可知点到超平面的距离公式为distance(x,b,w)=∣wT+b∣∥w∥distance(x,b,w)=\\frac{|w^T+b|}{\\|w\\|}distance(x,b,w)=∥w∥∣wT+b∣​，之前的第一个条件every ynwTxn&gt;0every\\ \\ y_nw^Tx_n&gt;0every yn​wTxn​&gt;0现在就表示成了every yn(wTxn+b)&gt;0every\\ \\ y_n(w^Tx_n+b)&gt;0every yn​(wTxn​+b)&gt;0，由于这个条件存在，因此距离公式中的绝对值可以去掉，改写为distance(x,b,w)=yn(wT+b)∥w∥distance(x,b,w)=\\frac{y_n(w^T+b)}{\\|w\\|}distance(x,b,w)=∥w∥yn​(wT+b)​，问题转化为： 另外，对于同一个超平面，其表达式乘一个常数代表的超平面不变，因此可以通过修改(w,b)(w,b)(w,b)使得min⁡n=1,…,Nyn⁡(wTxn+b)=1\\min _{n=1,\\ldots ,N} \\operatorname{y_n}\\left(\\mathbf{w}^{T} \\mathbf{x}_{n}+b\\right)=1minn=1,…,N​yn​(wTxn​+b)=1，因此可以把优化问题改写成下面这种更简单的形式： 当满足min⁡n=1,…,Nyn⁡(wTxn+b)=1\\min _{n=1,\\ldots ,N} \\operatorname{y_n}\\left(\\mathbf{w}^{T} \\mathbf{x}_{n}+b\\right)=1minn=1,…,N​yn​(wTxn​+b)=1时，显然第一个约束every yn(wTxn+b)&gt;0every\\ \\ y_n(w^Tx_n+b)&gt;0every yn​(wTxn​+b)&gt;0是成立的，因此只需要保留第二个约束条件。另外第二个约束条件可以改写为every yn⁡(wTxn+b)≥1every \\ \\ \\operatorname{y_n}\\left(\\mathbf{w}^{T} \\mathbf{x}_{n}+b\\right)\\ge 1every yn​(wTxn​+b)≥1，很容易证明这两个条件是等价的，因为假如得到的最优解使得every yn⁡(wTxn+b)&gt;1every \\ \\ \\operatorname{y_n}\\left(\\mathbf{w}^{T} \\mathbf{x}_{n}+b\\right)&gt; 1every yn​(wTxn​+b)&gt;1，则可以通过给(w,b)(w,b)(w,b)乘一个小于1的倍数使得min⁡n=1,…,Nyn⁡(wTxn+b)=1\\min _{n=1,\\ldots ,N} \\operatorname{y_n}\\left(\\mathbf{w}^{T} \\mathbf{x}_{n}+b\\right)=1minn=1,…,N​yn​(wTxn​+b)=1，乘上这个倍数后1∥w∥\\frac{1}{\\|w\\|}∥w∥1​变大了，得到了一个更优解，原先的最优解被推翻，假设不成立，也就是说在every yn⁡(wTxn+b)≥1every \\ \\ \\operatorname{y_n}\\left(\\mathbf{w}^{T} \\mathbf{x}_{n}+b\\right)\\ge 1every yn​(wTxn​+b)≥1得到的最优解的min⁡n=1,…,Nyn⁡(wTxn+b)=1\\min _{n=1,\\ldots ,N} \\operatorname{y_n}\\left(\\mathbf{w}^{T} \\mathbf{x}_{n}+b\\right)=1minn=1,…,N​yn​(wTxn​+b)=1一定成立，因此两个条件等价。最优化问题转化为： 再转化为最小化的形式： 注：这一节的笔记内容和课程有较大不一样，主要是把一些课程中过于细致的点忽略了（比如对点到平面距离公式的推导等），因为过于关注这些学过的或很容易理解的点容易让人抓不住主线。 4 Support Vector Machine 先从一个例子中看一下支持向量机名字的由来： 上图中一共有四个点，但是可以看到，决定直线方程的是其中三个点，这三个点一动方程就会变，而右下角的点在一定范围内变化对方程无影响。这样我们称另外三个点是支持向量，这个分类器是由支持向量决定的，所以称为支持向量机。实际上在后面会给出从优化问题的结果得到的支持向量的定义，那个是更加准确的，不是所有的在边缘的点都是支持向量，也不是所有在边缘的点都可以决定或单独决定方程，因此这里的定义不能够包含一些情况，只是一种直观理解。 通过上一节的推导我们得到了一个二次规划问题，这个问题已经有很多方法可以求解了，python和matlab等软件中也有求解专用的模块或函数，所以我们的一个想法是使用这些功能求解SVM，不过这就需要先找到这个最优化问题的标准型。 下面是通过我们的最优化问题和二次规划标准型对比得到的改写方法： 上述是线性hard-margin支持向量机，若想推广到非线性，只需要把 x 换成 z 空间内的坐标，但这样对计算能力的要求增加也很大，后面学了对偶方法后有更简单的变换方法，那就是 kernel 。此外，我们也会把hard-margin 推广到 soft-margin 。 附： 本节一开始讲了一个手算最优解的例子： 本节最后有一个例子： 5 Reasons behind Large-Margin Hyperplane 这一节讲了选择最大线宽分类器的一些理论上的理由。 第一个理由是与正则化做了一个对比，对比如下图，由于我本人对基石中正则化这一部分的讲解存在不赞同之处（见我的基石笔记14讲第2节的注释），所以这个表我不再解释。 第二个理由是从线宽与成长函数的角度来看的。以寻找二维平面中的分类器为例，当我们限制了最大线宽后，假设找到的最优解的线宽是 ρ\\rhoρ，这时我们就相当于是在线宽大于等于 ρ\\rhoρ 的所有所有满足分类正确的分类器中寻找解。这种要求就使得这条线不能 shatter 任意三个点了，这样就降低了成长函数。 基石课程中讲到的 VC Dimension 都是和算法无关的，算法是用来寻找使得Ein+regularizorE_{in}+regularizorEin​+regularizor 最小的分类器的，但是这里这个 ρ\\rhoρ 实际上却是和算法有关系的，这就是我们这种理解方法的特殊之处，我们暂且称之为算法的 VC Dimension，这时候在一些特殊情况下可以求出来 VC Dimension，例如当所有的点分布在一个半径为 R 的圆周上时： 从中可以看出此时的 VC Dimension 确实变得比一般情况下的 d+1 小。 以上是两种解释。 从这种解释出发，我们可以看到SVM算法造成的结果。原先的原始 PLA 算法和加上了特征转换的 PLA 算法都有其不足之处，前者是假设空间小（仅对于线性可分或接近线性可分的可以使用），后者是边界复杂，容易过拟合。目前我们已经做到了把 SVM 用到 PLA 中，发现可以控制复杂度，下一步如果能够将其用在加上了特征变换的 PLA 中，那么就可以得到一个算法既有足够大的假设空间，复杂度又可以被控制（见下图）。当然在这么做之前我们需要先介绍对偶问题。","permalink":"http://yangtf983.github.io/2020/06/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%B3%95%E7%AC%94%E8%AE%B01%EF%BC%9ALinear%20SVM/","photos":[]},{"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"title":"机器学习基石笔记16：Three Learning Principles","date":"2020/03/20","text":"0 说明 这一节介绍三种有用的原则/技巧。 （本节不过多叙述课程中提到的帮助理解的方式，重点在于记住三种技巧本身） 1 Occam’s Razor 奥卡姆剃刀定律：如无必要，勿增实体。 应用在机器学习中就是说适合数据的最简单的模型是最好的模型。 如上图所示左边的模型比右边的模型更好。 简单的解释一下，当模型复杂时，成长函数mH(N)m_H(N)mH​(N)随着N增大更快，以投硬币为例子，模型能够完全正确分类的情况占总情况的mH(N)2N\\frac{m_H(N)}{2^N}2NmH​(N)​，若对于某个复杂模型而言，mH(N)m_H(N)mH​(N)达到了2N2^N2N，那么总是能够把手头的数据完美分开，但是数据本身其实是杂乱无章没有规律的。只有用简单的模型在数据上做的比较好才能说明数据确实存在某种模式，并且我们的算法确实学到了这种模型。 为了得到简单的结果，一般有两种方式：一种是一开始就限制在一个简单的模型中（如线性回归）；另一种是通过regularizer来限制模型复杂度。 2 Sampling Bias 当抽样有偏差时，学习的结果也会有偏差。 当犯了抽样有偏差的错误时，就相当于只学习了数学却去考英语。 这不一定是指抽样要随机，而是说用来验证的数据选取需要符合实际应用的情况。 例如做一个电影推荐系统，用来验证的数据集就不应该是随机选取，而应该是用户最后面看的几部电影，因为训练数据与最终要预测的数据有前后的时间关系；同时，训练的时候应当给时间离现在越近的数据赋上越大的权重，因为近期的喜好对接下来会看的电影影响更大。 还有之前的信用卡发放的例子，对于一个个体而言，只有以前给其发放过信用卡才能知道他有没有遵守信用，若之前的系统就没有给其发放信用卡，则不会有这一部分信息，这样的数据在处理的过程中需要用一些特殊的技巧（课程中没有介绍）。 3 Data Snooping（数据窥视） 如果让数据本身影响到了学习过程，那么学习结果就会变差。 若实验者从数据出发，可能获得一些在目前的数据上统计上显著但是实际上没什么关联的关系。数据窥视常常是不经意间发生的，有时会产生一些具有误导性的结论。 这个问题比较难理解，附上两个讲解的网址辅助理解： COMMON MISTEAKS MISTAKES IN USING STATISTICS: Spotting and Avoiding Them 数据窥视偏差：策略优化陷阱 数据窥视非常难以避免，这里提两种方法： 避免用数据对模型做决定，应该先把知识放进模型，而不是先看数据才思考模型； 保持怀疑，学会思考在自己的建模过程中数据可能被污染得多严重。 真正数据分析过程中，数据窥视难以避免，要学会平衡好data snooping与validation. 4 Power of Three 由于本课程中的很多东西都与3有关，所以老师总结了一个power of three： GangXia （￣︶￣）↗ （￣︶￣）↗ （￣︶￣）↗","permalink":"http://yangtf983.github.io/2020/03/20/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B016%EF%BC%9AThree%20Learning%20Principles/","photos":[]},{"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"title":"机器学习基石笔记15：Validation","date":"2020/03/19","text":"0 说明 上一节课讲到可以通过增加regularizer惩罚模型复杂度从而帮助做出正确的选择。 这一节课讲解如何通过validation选择正确的模型。 1 Model Selection Problem 通过先前的学习，我们已经知道了许多在机器学习中需要做的选择，例如：算法A；迭代步数T；学习率η\\etaη；模型次数；regularizer；惩罚项系数λ\\lambdaλ 如何选择出最合适的组合其实是一个很重要的问题。 说到底，参数的合适与否是根据产生结果的效果来确定的，目前还没有方法可以在仅仅知道问题的时候就能够选择出合适的算法和参数，所以寻找最优算法和参数的思路应当且只能从结果出发。 如上图所示，不同的假设（算法和参数的组合）产生不同的最优解，模型选择就是选出这些最优解中的最优解，选出了最优模型，其对应的参数和算法自然就是整个大集合中最佳的。 下面就用这种思路引出validation 可视化之前已经探讨过，具有两大缺陷，分别是高维问题无法可视化以及人脑的复杂度太大。 之前用的最多的模型选择方法是用EinE_{in}Ein​，在VC理论的保证下，EinE_{in}Ein​与EoutE_{out}Eout​的差别大概率很小（模型复杂度一定，数据充足时），因此只要使得EinE_{in}Ein​足够小就可以。但是在我们将模型从线性扩展到高次后，这种方法就不再适用了，因为高次模型的EinE_{in}Ein​一定比低次模型好，这就会导致过拟合（惩罚项系数λ=0\\lambda=0λ=0总会产生最好的EinE_{in}Ein​，这也会导致过拟合）。从VC理论上来说，这种方法不过是从所有假设的并集中找到一个EinE_{in}Ein​最大的结果，这样做的缺点是假设的并集的VC维太大，导致VC bound的约束太宽松，理论上不能保证generalization的效果，实践上可以发现效果确实很差。所以不能用EinE_{in}Ein​做选择。 有一个最理想但不可行的方法是用需要预测的数据集来做模型选择，如上图所示，理论上可以保证这种方法的可行性，实际上在test数据集上的效果也正是我们的目的，如同用期末考试试卷来检测复习效果一样，最直接，但是缺点也很明显，我们无法提前拿到期末考试卷，同样无法提前知道需要预测的数据集。因此，我们就提出了一个折衷的方法： 这种方法就是上图中红色的部分，每次训练模型不采用完整的数据，保留一部分数据避免被污染，用来做模型选择，这种方法我们就称之为validation. 2 Validation 我们一直假设已有的数据集和之后做预测的集合是来自同一个分布的，因此若验证集的数据是随机选择的，也就和前二者有相同的分布，这是连接三者的纽带。根据之前的结论可以推导出上图中的不等式。 从上图中可以看出验证就是用训练集训练模型选出来在验证机上表现最好的模型，图中有一个不等式，右边二者的关系是确定的，而左边灰色部分二者的关系是不一定的，但是根据经验，当其他条件不变，仅仅在原来数据集的基础上扩充数据，得到的模型的表现应该会更好，这是一个大概率事件但不是绝对事件。但是相信这件事，因此我们常常的做法是在验证出来最好的模型后采用整个数据集重新训练一次作为最终的结果。 上图中 黑线是采用$E_{in}$做选择出模型最终的期望错误，虚线是用真正的数据集选择出模型最终的期望错误，红线是用validation方法做选择且直接作为最终模型的期望错误，蓝线是用validation方法做选择且用全部数据重新训练作为最终模型的期望错误，几者的关系和曲线形状是比较好理解的。 $g_m$是用整个数据集做训练得到的最佳模型，显然当validation用掉的数据越多，训练出来的模型与$g_m$的表现差距就越大，因此左侧的约等号要求K比较小；K越大的时候验证的结果越接近真实的$E_{out}$，因此右侧的约等号要求K比较大。因此如何选择合适的K就成为了一个重要的问题，一个**经验法则是$K=\\frac{N}{5}$**，下面将介绍两种常用的验证方法。 3 Leave-One-Out Cross Validation Leave-One-Out Cross Validation，顾名思义，每次拿出来一个数据做验证，K恒等于1. 前两张图说明了Leave-One-Out Cross Validation方法的验证效果是接近真实的EoutE_{out}Eout​的。因为当N不太小时Eout‾(N−1)\\overline{E_{out}}(N-1)Eout​​(N−1)应当与Eout(N)E_{out}(N)Eout​(N)非常接近，而Eloocv(H,A)E_{loocv}(H,A)Eloocv​(H,A)是Eout‾(N−1)\\overline{E_{out}}(N-1)Eout​​(N−1)的一个无偏估计。 从上图可以看出，Leave-One-Out Cross Validation的验证错误EcvE_{cv}Ecv​确实接近EoutE_{out}Eout​，与之前的理论推导符合，并且从图中我们发现根据EcvE_{cv}Ecv​指标选择模型复杂度中等的模型最好，从上图中第一行不同分类器的表现来看这样的选择确实比用EinE_{in}Ein​更加合理。 4 V-Fold Cross Validation Leave-One-Out Cross Validation的缺点也很明显，那就是计算复杂度和稳定性。 当数据量较大时，模型计算非常复杂，因为对于每一个算法而言都要训练N次。 不稳定行从上图中的黑线的不平滑程度可以体现出来。 由于有这两个原因，Leave-One-Out Cross Validation常常不够实用。 于是就产生了V-fold Cross Validation，特点是将数据分成V份，每次拿出来一份做验证，等于只需要训练V次。 具体的方法表示在下图中： V-fold Cross Validation是一种比Leave-One-Out Cross Validation更实用的方法，一般用5-Fold或10-Fold的效果就会比较好。","permalink":"http://yangtf983.github.io/2020/03/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B015%EF%BC%9AValidation/","photos":[]},{"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"title":"机器学习基石笔记14：Regularization","date":"2020/03/19","text":"0 说明 regularization就是给假设加上限制条件后进行最优化的做法。 这一堂课将引入regularization，介绍weight decay regularization、regularization与VC理论的关系，并讲解正则化的一般方法。 1 Regularized Hypothesis Set 上一节课介绍了过拟合，说明了H2H_2H2​比H10H_{10}H10​有时更适合（虽然后者的假设集包含前者），最后介绍了几种避免过拟合的方法。除了数据清洗、删除和扩充，正则化也是一种重要的方法，这里就从上个例子来说明如何在不变动数据的情况下选择出H2H_2H2​而不是H10H_{10}H10​ H2H_2H2​可以看做是H10H_{10}H10​的后8个参数均为0，不过为了将模型更推广一点（因为如果我们知道后八个参数为0那就不需要再求解带约束的H10H_{10}H10​而是应该直接求解H2H_2H2​，而且一般也很难提前知道几次模型最合适），再将H2H_2H2​放松到H(C)H(C)H(C)，即限制所有参数的总和。 H2H_2H2​与H(C)H(C)H(C)是重叠而不是包含的关系，二者有重叠的部分但不存在从属关系。随着C的变大假设H(C)H(C)H(C)集合是越来越大的。像这种对模型的参数加以限制的假设集被称为正则化假设集(regularized hypothesis set) 2 Weight Decay Regularization 这一节课程中推导了在限制条件下使得EinE_{in}Ein​最小的公式，使用的拉格朗日乘子法，中间的过程见下面几张图，这里不再对其仔细解释，而是聚焦于regularization这个方法。这一小节先介绍课程中的内容，另外我对这种推导方式不太相信，在最后我要提出我对这种推导方法的质疑。 总之，正常的拉格朗日乘子法是确定了C进而可以求出来λ\\lambdaλ进而求得权重的唯一解的，不过在regularization中C不是一个一开始就确定的值，那么就相当于是未知数比方程数多了1个，那么λ\\lambdaλ和C可以互相确定，因此与其先给出C再求解出λ\\lambdaλ再求解 权重，不如直接尝试不同的λ\\lambdaλ，这样也就相当于尝试不同的C寻找最合的那个。 所以问题变成了求解∇Ein(wREG)+2λNwREG=0\\nabla E_{in}(w_{REG})+\\frac{2\\lambda}{N}w_{REG}=0∇Ein​(wREG​)+N2λ​wREG​=0，进而变成了最小化Ein(w)+λNwTwE_{in}(w)+\\frac{\\lambda}{N}w^TwEin​(w)+Nλ​wTw，其中的λ\\lambdaλ是可以人为选择的参数。值得注意的是，上面几张图中的变换都是以上一个例子为基础的，也就是说限制条件是参数的模长，实际上这个限制条件是可以变的，变化后上面图中的regularizer项wTww^TwwTw也要变成新的限制项。 针对上一个例子而言，可以得到最终结果的解析表达： 由于ZTZZ^TZZTZ是半正定矩阵，所以当λ\\lambdaλ是是正数时，括号内部分的逆矩阵一定存在，此时只接求逆是简单的。 而当regularizer不再是对参数的平方和模长的限制时，无法得到上式，应当采用其他优化方法，例如当Eaug(w)E_{aug}(w)Eaug​(w)可以求导时可以采取之前学过的梯度下降法求最优解。 上图是对这个例子的求解，通过尝试不同的λ\\lambdaλ发现当λ=0.0001\\lambda=0.0001λ=0.0001时就可以做到很好。现实问题不知道目标函数f，则应当通过validation进行判断哪一个结果较好，这是下一节课的内容。 提一个注意事项，当x的取值范围在-1到+1时，其高次项的绝对值非常小，这样就导致高次项的参数需要非常大才能体现出作用，这对于regularization的选参和求解不利，这时一种有效的解决方法是采用Legendre多项式，这种多项式的优点是能够求出来一组正交的多项式，这组多项式中都有次数较低的项，这就限制了Legendre多项式中不同项的绝对值差别不会太大。 注：拉格朗日乘子法求解的是刚好在限制条件边缘上的极值，但是这一节林老师用这种方法来求解在限制条件内部的最优解，那么就必须能够解释这个最优解一定在边缘上而不在内部，但是对这一节的这个问题（线性规划）这一点是不成立的，比如我们可以找到一条完美的直线拟合数据时，就不可能再使得 www 的模随着限制条件的放宽再变大，所以这种理解角度我不赞同，正则化方法可以直接用在目标函数增加限制项而不是在限制条件中增加限制项来理解，这也是大部分教材通用的方法。（若有同学发现我这段话中的错误，欢迎指正。） 3 Regularization and VC Theory 下面看一下用VC理论如何解释regularization方法的合理性： 从图中可以看出，regularization的做法实际上是用最小哈EaugE_{aug}Eaug​代替了最小化EinE_{in}Ein​，这种做法的优点是考虑到了模型复杂度（regularization实际上是对模型复杂的惩罚，本章的例子中C越大模型越复杂），这样当EaugE_{aug}Eaug​较小时不仅可以使得EinE_{in}Ein​也较小，而且当对模型复杂度的惩罚程度与Ω(H)\\Omega(H)Ω(H)较接近时，也可以保证EoutE_{out}Eout​较好。相反，若没有regularizer，选到的模型比较复杂时由于Ω(H)\\Omega(H)Ω(H)较大而无法确保EoutE_{out}Eout​较小。之前的线性回归和PLA之所以可以直接最优化EinE_{in}Ein​是因为模型复杂度已经被次数限制在了一个很小的范围内。 4 General Regularization 与err项的选择类似，regularizer的选择也有三种考量，分别是：target-dependent, plausible, friendly. L1和L2是常用的regularizer，对他们做一个比较： 由于L1L1L1作为regularizer产生的最优解常常在假设区域的顶点（顶点的某些参数为0），因此当已知最终解比较稀疏（sparse）时采用L1是比较合适的。 通过上图可以看出，当stochastic/deterministic噪声变大时，最优的λ\\lambdaλ都会随之变大。 不过现实情况是噪声往往是未知的，选择合适的λ\\lambdaλ就变得有技巧性，如何选择合适的λ\\lambdaλ留待下一次课程。 上图中是一种与本节的例子中不同的regularizer，其特征是对高次项的参数惩罚加大，因此倾向于选择低维的多项式，这种regularizer显然属于target-dependent类型的。","permalink":"http://yangtf983.github.io/2020/03/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B014%EF%BC%9ARegularization/","photos":[]},{"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"title":"机器学习基石笔记13：Hazard of Overfitting","date":"2020/03/19","text":"0 说明 这一节介绍overfitting的危害与应对方法 1 What is overfitting 假设dVC⋆d_{VC}^{\\star}dVC⋆​是某个问题的最佳VC Dimension，结合上一节课最后的曲线图看一下dVCd_{VC}dVC​： 图中紫色虚线对应最合适的模型，该处Eout−EinE_{out}-E_{in}Eout​−Ein​最小，EinE_{in}Ein​也足够小。当dVCd_{VC}dVC​变大时，EinE_{in}Ein​下降而EoutE_{out}Eout​增加，这部分属于overfitting；反之，Ein,EoutE_{in},E_{out}Ein​,Eout​同时上升，这一部分属于underfitting underfitting的模型一般不会被选择，因为EinE_{in}Ein​往往达不到要求，而overfitting的模型由于EinE_{in}Ein​很小容易被选中，但是generalization的效果太差，应当避免选择到overfitting的模型。 上图是一个典型的overfitting的例子，从一个二次函数上选择五个点并用四次函数拟合，拟合结果在这五个点上表现完美，但在其余出差别很大。 为了尽量避免overfitting，首先应该知道造成overfitting的原因都有哪些。 概略地说，造成过拟合的原因主要有三点：模型太过复杂；数据存在噪声；数据点较少。 后面将更加相信分析这些原因。 2 The Role of Noise and Data Size 假设现在有两组数据，分别来源于10次模型（且有噪声）和50次模型（无噪声）： 现在分别用2次模型和10次模型拟合这两组数据，发现在两组数据上都是2次模型的结果较好： 从图中可以看到，两组数据在从H2H_2H2​到H10H_{10}H10​的过程中都发生了EinE_{in}Ein​的下降和EoutE_{out}Eout​的上升，根据之前的定义，这是过拟合现象。 这两个过拟合似乎很出乎意料。第一组数据中用与目标函数相同类型的假设过拟合，第二组数据中用与目标函数更接近的模型过拟合。这说明假设与目标函数的接近与否不能作为判断是否过拟合的标志。 造成这一现象的原因可以直接从下面的曲线中得到： 从曲线中可以看出，当数据量较小的时候，H2H_2H2​总是更好。 对于第二个例子，没有噪声，但是H2H_2H2​仍然较好，其实我们可以把模型复杂度当作一种噪声来理解，只是这个噪声是确定的，这个噪声被称为deterministic noise，其定义是假设集HHH中 最好的h∗h^*h∗与目标函数之间的差距。 3 Deterministic Noise 通过实验来说明过拟合程度与噪声水平(σ2\\sigma^2σ2)、模型复杂度(QfQ_fQf​)、数据点数量NNN 之间的关系。 通过设计一个有噪声模型，从中抽取数据，分别采用2次和10次曲线进行拟合，用Eout(g10)−Eout(g2)E_{out}(g_{10})-E_{out}(g_2)Eout​(g10​)−Eout​(g2​)衡量10次模型的过拟合程度，得到下面两张图： 从红色到蓝色表示过拟合程度在减轻甚至g10g_{10}g10​不再过拟合。 称σ2\\sigma^2σ2为stochastic noise，QfQ_fQf​为deterministic noise，从图中可以得到下述结论： 当数据量N减小时，过拟合程度上升； 当stochastic noise上升时，过拟合程度上升； 当deterministic noise上升时，过拟合程度上升。 此外，从上图中的第二张图可以看到，左下角部分有一块红色，与上面部分的红色的交接点应当是Qf=10Q_f=10Qf​=10，从这里可以看出，当模型次数高于目标函数过多时，过拟合程度增加（当Qf=10Q_f=10Qf​=10时，g10g_{10}g10​相对于g2g_2g2​的过拟合程度是最小的）。不过这个结论相对于前三个没有那么重要。 从对过拟合的影响来说，deterministic noise与stochastic noise的影响一样，简单理解来说，假如目标函数没有被包含在假设集HHH中，则无论如何总是有一些部分无法被模型正确描述，这就是由deterministic noise造成的噪声。电脑中的伪随机数就是用一个很复杂的函数生成的，从中就体现了deterministic noise与stochastic noise相同的思想。二者的区别主要在两点：一点是deterministic noise依赖于假设HHH；另一点是当固定x多次取样（由于有随机噪声故多次取样y不同）时deterministic noise不变。 4 Dealing with Overfitting 针对不同的过拟合原因分别提出合适的避免过拟合的方法： data cleaning是修正数据集中的错误标签，data pruning是删除数据集中标签错误的数据，data hinting是通过一些方法利用现有数据集增加更多数据，regularization和validation留待后两节课讲解。","permalink":"http://yangtf983.github.io/2020/03/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B013%EF%BC%9AHazard%20of%20Overfitting/","photos":[]},{"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"title":"机器学习基石笔记12：Nonlinear Transformation","date":"2020/03/18","text":"0 说明 本节课从线性模型拓展到了非线性模型，总结了这种变换的注意事项和代价。 1 Quadratic Hypothesis 我们已经讲解了一些线性模型并说明了其可行性，但是有时会碰到用线性模型无论如何无法将EinE_{in}Ein​变得足够小的情况，此时用线性模型就不能保证学到东西了。为此，需要突破线性假设的限制。 上图是一个例子，数据点非线性可分且线性模型也难以将EinE_{in}Ein​做的足够小。但是通过观察加尝试，可以用一个通过原点的圆将数据完美的分开。 那么此时是不是应该建立一个寻找圆形分类器的PLA，再证明其可行性等等，将之前的部分再做一遍呢？ 其实有一种更好更通用的方法，那就是将数据变换到另一个空间使其成为线性可分的，套用PLA的过程和理论即可。 上图是对这个“圆形可分”的问题做的变换，变换后的数据(zn,yn)(z_n,y_n)(zn​,yn​)是线性可分的。我们称这样的变换为特征变换，z1,z2z_1,z_2z1​,z2​为特征。 如上图，改变变化后的模型的权重，对应于不同的变换前的曲线类型，如果再加上其他一次项和二次项，就可以得到一个能够对应所有二次曲线的分类器，z空间内的线性感知机对应的是x空间内的二次曲线（包括次数更低的曲线）。 2 Nonlinear Transform 容易知道z空间中被感知机分割的部分对应在x空间内也是被二次曲线分割开的，因此两个空间有下述关系： 也就是说这样的特征变换把从x空间内找好的分类器的问题转化为从z空间内找好的分类器。 用特征转换求解分类问题的大致步骤： 这个步骤中需要做两次选择： 特征变换Φ\\PhiΦ 线性模型A\\mathcal{A}A 通过类似的方法可以不断提高多项式次数，做到任意n次PLA，任意n次回归。 下面看一个之前提过的特征转换的例子： 这个例子中就是把难以直接用来分类的像素（raw feature）通过我们的知识变成了密度和对称性（concrete feature），是一个典型的特征变换。 特征变换的效果很强，但是也会付出一些代价，这就是下一小节的主题。 3 Price of Nonlinear Transform 虽然非线性变换可以提高假设的适用情况，但并不一定就是合适的，因为变换常常也需要付出代价。下面将介绍两种代价： 上面两张图介绍了两种代价。第一种是计算/存储代价，d个变量组成Q次多项式，次数为Q的项就相当于是把Q个相同的小球同时随机放到d个盒子中（一个盒子可以放多个），可以用隔板法计算，给Q+d-1个位置放d-1个隔板将空间隔离成d块，相邻两个隔板之间的空位置数就是对应项的次数，可以知道这样的项最多有(Q+d−1d−1)\\left(\\frac{Q+d-1}{d-1}\\right)(d−1Q+d−1​)个。再通过将所有次数不大于Q的项的最大数量相加，得到Q次项最多有： (Q+d−1d−1)+(Q+d−2d−1)+…+(dd−1)+1=(Q+dd)\\left(\\frac{Q+d-1}{d-1}\\right)+\\left(\\frac{Q+d-2}{d-1}\\right)+\\ldots+\\left(\\frac{d}{d-1}\\right)+1=\\left(\\frac{Q+d}{d}\\right) (d−1Q+d−1​)+(d−1Q+d−2​)+…+(d−1d​)+1=(dQ+d​) 当Q增大时，参数的数量增大很快，会导致计算/存储困难。 第二种是模型复杂度快速增加，导致dVCd_{VC}dVC​快速增加，VC bound过于宽松。 第二种代价会导致模型结果难以外推（generalization）。下图的例子就反映了这样的问题： 第二个模型虽然EinE_{in}Ein​更小，但是模型复杂度太高，无论是从理论还是从直观视觉上看与EoutE_{out}Eout​的差距都超过第一张图，generalization的效果较差。 如何决定选择多么复杂的模型？ 从上一个例子来看视觉直观似乎是一个不错的选择，但是当维数较高时无法采用。并且在采取视觉直观方法的时候我们通过看数据加上人脑的理解决定的模型，这样有一个很严重的问题就是其实模型加上了“大脑中的”复杂度（这个地方没听太理解，有点哲学的味道，总之就是不能通过人脑决策代替机器学习决策）。 4 Structured Hypothesis Sets 不同多项式次数假设的关系： 从关系图中可以看出，一味增加多项式模型的复杂度虽然可以降低EinE_{in}Ein​，但是Eout−EinE_{out}-E_{in}Eout​−Ein​难以保证，而实际问题中EoutE_{out}Eout​是未知的，不能每个模型计算出来Eout−EinE_{out}-E_{in}Eout​−Ein​进行判断，所以安全的方法是从一次模型开始，逐渐增大次数，当EinE_{in}Ein​能够满足要求时就终止。 事实上，线性模型已经能够满足很多模型的情况了。","permalink":"http://yangtf983.github.io/2020/03/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B012%EF%BC%9ANonlinear%20Transformation/","photos":[]},{"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"title":"机器学习基石笔记11：Linear Models for Classification","date":"2020/03/18","text":"0 说明 本堂课在整合之前讲的分类问题后将其延伸到更多更复杂的问题上。 1 Binary Classification 这一节比较一下线性分类、线性回归、logistic回归三种方法在二分类问题上的应用。 三者的简单总结： 之前已经证明过PLA的合理性，那么线性回归和logistic回归的合理性怎么说明？ 其实这个问题之前已经涉及到，就是用损失函数的大小来判断，如果后两者损失函数很小的时候可以保证PLA的损失函数也很小，那我们就可以说后两者用来做二分类是合理的。 对三者的损失函数做一点变换，目的是将h和x换成s，再将y和s换成ys，而ys是一个有实际意义的变量，当其为正时，说明估计值与真实值相同（且绝对值越大说明做出这样估计的把握越大）；当其为负时，说明估计值与真实值相反（且绝对值越大说明做出这样估计所犯的错误越大）。因此ys被称为分类正确分数（classification correctness score）。变换方法如下： 这样我们可以画出三者的函数图像： err0/1err_{0/1}err0/1​是否为0与ys是否大于0完全一致，也就是说err0/1err_{0/1}err0/1​恰好在该惩罚的情况下惩罚。errsqrerr_{sqr}errsqr​在s不太大的情况下做的不错，但是当s太大时过度惩罚。errCEerr_{CE}errCE​的单调性与err0/1err_{0/1}err0/1​相同，但是二者没有明确的大小关系，这一点可以通过对errCEerr_{CE}errCE​改变比例得到errsce=errCEln⁡2err_{sce}=\\frac{err_{CE}}{\\ln2}errsce​=ln2errCE​​完全在err0/1err_{0/1}err0/1​上方。 因此，我们可以对几者得关系作出如下判断： 进而得到三者的优缺点和应用方法： PLA算法在数据线性可分时非常有效，但是缺点是线性可分的数据很难保证。线性回归是最容易优化的，但是损失函数和真实情况在ys较大时不契合。logistic回归也比较容易优化，缺点是当ys很小时惩罚可能太大。 总之，PLA/pocket/logistic回归的结果的合理性都是可以保证的，线性回归有时可以保证，因此常用的方式是把线性回归模型的结果 作为其他三者迭代的初始值。并且前三者在实际应用中logistic回归最受欢迎，由于其容易优化的性质和结果的可靠性。 2 Stochastic Grad. Descent（随机梯度下降法） 比较PLA和logistic回归/pocket算法，我们发现前者每次迭代只需要找到一个点，而后者却需要算所有点的梯度（用梯度下降法解logistic回归）/标签，计算量大大增加，因此这一部分将提出一个新的梯度下降方法来降低计算复杂度，使得logistic回归的求解每次也只需要计算一个点的梯度。 先展示一下之前的logistic回归的梯度下降法的迭代公式： wt+1←wt+η1N∑n=1Nθ(−ynwtTxn)(ynxn)⏟−∇Ein(wt)\\mathbf{w}_{t+1} \\leftarrow \\mathbf{w}_{t}+\\eta \\underbrace{\\frac{1}{N} \\sum_{n=1}^{N} \\theta\\left(-y_{n} \\mathbf{w}_{t}^{T} \\mathbf{x}_{n}\\right)\\left(y_{n} \\mathbf{x}_{n}\\right)}_{-\\nabla E_{\\mathrm{in}}\\left(\\mathbf{w}_{t}\\right)} wt+1​←wt​+η−∇Ein​(wt​)N1​n=1∑N​θ(−yn​wtT​xn​)(yn​xn​)​​ Ein=∑errE_{in}=\\sum errEin​=∑err，上式中的1N\\frac1NN1​是我们之前刻意加上的，为的就是此时看起来EinE_{in}Ein​的梯度像errerrerr的梯度的平均值。这样我们只计算任意一个errerrerr的梯度，它就是EinE_{in}Ein​的梯度的一个点估计。随机梯度下降法（SGD）就是用这个点估计代替真实值，从而降低计算复杂度，每次只需要对一个点求∇err\\nabla err∇err 因此，logistic回归的SGD的迭代公式为： wt+1←wt+ηθ(−ynwtTxn)(ynxn)⏟−∇err(wt,xn,yn)\\mathbf{w}_{t+1} \\leftarrow \\mathbf{w}_{t}+\\eta \\underbrace{\\theta\\left(-y_{n} \\mathbf{w}_{t}^{T} \\mathbf{x}_{n}\\right)\\left(y_{n} \\mathbf{x}_{n}\\right)}_{-\\nabla err\\left(\\mathbf{w}_{t},x_n,y_n\\right)} wt+1​←wt​+η−∇err(wt​,xn​,yn​)θ(−yn​wtT​xn​)(yn​xn​)​​ SGD有两个重要的经验法则： 终止条件 SGD的目的就在于减少计算量，因此终止条件不能是计算∇Ein\\nabla E_{in}∇Ein​使其足够小（这样会导致计算量与梯度下降法相同），只能是迭代足够的步数（有效性由经验表明）。 η\\etaη 经验表明，0.1左右是一个合适的值，可以采用你自己的在0.1左右的“幸运数字”（老师的幸运数字是0.1126） 看一个例子： 由于线性回归的err=(yn−wtTxn)2err=(y_n-w_t^Tx_n)^2err=(yn​−wtT​xn​)2，因此∇err=2(wtTxn−yn)xn\\nabla err=2(w_t^Tx_n-y_n)x_n∇err=2(wtT​xn​−yn​)xn​，因此负随机梯度方向是第四项，这也是SGD解线性回归的迭代方向。 3 Multiclass via Logistic Regression 多分类问题是一种常见的问题，也可以称之为识别。 假设现在有一个有四种类别的数据的图像，下面讨论如何合理的分类。 首先容易想到的一种方法是每次将三种类别合为一类，与第四种类别做一个二分类，一共做四次。 上图中第一行第一张图是四种类别的数据点，第二行是四次二分类的结果，第一行第二张图是最后分类的结果。 从图中可以看到，四个对角上的五边形中的数据点通过这种分类方法是确定的，而四个三角形中的数据都同时被分到了两类，中间的四边形则是同时被分到了四类，从这个模型中无法区别三角形和四边形区域中数据点的具体类别。 为了解决这个问题，我们寄希望于找到一个能够类别概率的模型，这样就可以轻易地将数据点分到其最有可能所属的类别。 之前学过的logistic回归可以做到这一点。 新的方法是建立四次logistic回归模型，每次还是以某一类和其他所有数据进行二分类，结果如下： 由于logistic回归函数θ(s)\\theta(s)θ(s)正比于sss，因此g(x)=arg⁡max⁡k∈yθ(w[k]Tx)g(x)=\\arg \\max_{k\\in y}\\theta(w^T_{[k]}x)g(x)=argmaxk∈y​θ(w[k]T​x)等价于g(x)=arg⁡max⁡k∈yw[k]Txg(x)=\\arg \\max_{k\\in y}w^T_{[k]}xg(x)=argmaxk∈y​w[k]T​x 这种算法的名字为One-Versus-All(OVA) Decomposition，具体流程如下： 这种算法的优点是高效，并且其中的logistic回归模型可以换成任何结果是软标签的方法。 这种方法的缺点也很明显，就是当类别过多或者类别间数据不平衡较大时，将数据分成一类对其他所有类可能非常不平衡，从而导致了分类结果的变差。例如如果较少的类别只占总数的1%，那么始终将所有数据分成其他类的错误就很小，对这种不平衡数据一般的二分类都是会失效的。logistic回归有一种专门适用于多分类的修改模型，被称为multinomial (‘coupled’) logistic regression 虽然有缺点，但OVA仍然是一个很实用且很有效的多分类模型。 4 Multiclass via Binary 针对上一节提出的OVA模型出现的数据不平衡问题，有一个简单的解决办法是每次只考虑两个类别的数据，这种方法被称作OVO(one-versus-one)模型。 每次考虑两个类别，这样N个类别就建立N(N−1)2\\frac{N(N-1)}{2}2N(N−1)​个分类器，用这么多分类器对数据进行分类，最终被分得次数最多的类别胜出，成为OVO模型的分类结果。 如上图，4个类别共建立6个二元分类器（图中第2行），当判断某个数据点的类别时，用六个分类器分别判断一次，最终得票最高的类别获胜（就像比赛冠军一样，因此图中写作tournament champion）。 OVO算法过程： 这种算法的优点是：高效、稳定、可以结合任何二元分类方法。 缺点是：由于需要训练的模型数量是O(K2)O(K^2)O(K2)的，因此类别太大时效率较低（占用空间更大，需要更多训练次数，预测更慢）。 最后看一个例子： 从这个例子中可以看出OVO有时可以显著降低OVA的算法复杂度。","permalink":"http://yangtf983.github.io/2020/03/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B011%EF%BC%9ALinear%20Models%20for%20Classification/","photos":[]},{"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"title":"机器学习基石笔记10：Logistic Regression","date":"2020/03/18","text":"0 说明 这一节探讨Logistic回归的问题，这种回归实际上是可以看做是一种soft binary classification问题的解法，这种问题是说数据的标签是硬的（确定是圈或叉），但理想的无噪声数据的标签其实是软的（只有是不同标签的概率值）。Logistic模型的形式给出的就是0到1之间的数字（对概率的估计）。 1 Logistic Regression Problem 现在有一个例子，知道一些人的一些身体指标和有没有患心脏病，要求学习一个函数估计任意一个人患心脏病的概率。该问题的Logistic模型的相关符号如下： s是各项指标的线性加权值，θ(s)\\theta(s)θ(s)是s的sigmoid函数，h(x)h(x)h(x)是最终的假设写法。 至于为什么sigmoid函数，课程中没有讲，实际上这也不是一个能够很清晰解释的问题，主要有三个原因，分别是：值域在0到1之间，适合表示概率；s型曲线从直观和实践上发现都是符合现实问题中的很多情况的；相对于其他满足前两点的s型曲线，sigmoid函数更加简单，实际性能也往往更好。 2 Logistic Regression Error PLA、线性回归、logistic回归三种模型可以简单对比如下： 计算这个模型的参数值用的是最大似然估计，这个是统计学基础内容，就不用解释了。我们需要最大化的概率是产生数据(xi,yi)(x_i,y_i)(xi​,yi​)联合概率∏iP(xi,yi)\\prod_{i}P(x_i,y_i)∏i​P(xi​,yi​)，由乘法法则这个概率等于∏iP(xi)P(yi,xi)\\prod_{i}P(x_i)P(y_i,x_i)∏i​P(xi​)P(yi​,xi​)，具体的表示和变换如图所示： 因此模型的目标可以写作： max⁡h likelihood(logistic h)∝∏n=1Nh(ynxn)\\max _{h} \\text { likelihood(logistic } h ) \\propto \\prod_{n=1}^{N} h\\left(y_{n} \\mathbf{x}_{n}\\right) hmax​ likelihood(logistic h)∝n=1∏N​h(yn​xn​) 由于h(x)=11+exp⁡(−wTx)h(x)=\\frac{1}{1+\\exp(-w^Tx)}h(x)=1+exp(−wTx)1​，而θ(wTx)=11+e−wTx\\theta(w^Tx)=\\frac{1}{1+e^{-w^Tx}}θ(wTx)=1+e−wTx1​，因此模型的目标等价地写作： max⁡w likelihood(logistic h)∝∏n=1Nθ(ynwTxn)\\max _{w} \\text { likelihood(logistic } h ) \\propto \\prod_{n=1}^{N} \\theta\\left(y_{n} w^T\\mathbf{x}_{n}\\right) wmax​ likelihood(logistic h)∝n=1∏N​θ(yn​wTxn​) 取对数： max⁡w ln⁡∏n=1Nθ(ynwTxn)\\max _{w} \\ \\ln \\prod_{n=1}^{N} \\theta\\left(y_{n} w^T\\mathbf{x}_{n}\\right) wmax​ lnn=1∏N​θ(yn​wTxn​) 最后取符号变成最小化： 如图中所示，称这样的error为cross-entropy error. 3 Gradient of Logistic Regression Error EinE_{in}Ein​表达式见上图，我们不加证明的告诉读者它具有连续、二阶可导、凸函数的性质。因此这里需要找的是梯度为零向量的点。 求梯度过程如下图： 梯度可以看作是一个由θ\\thetaθ对−ynxn-y_nx_n−yn​xn​加权平均的结果，梯度为0的一种可能是所有权重都为0，这就要求ynwTxn≫0y_nw^Tx_n\\gg 0yn​wTxn​≫0，从PLA的推导我们就知道这个要求达成的必要条件是数据是线性可分的，显然要求过高，很难达到。只能从加权和为0求解这个公式。 遗憾的是，这个公式的求解没有解析解，只能采用迭代方法，接下来一部分要介绍的是梯度下降法。这里先重写一下PLA算法并引进方向和步长的概念。 原先的算法是： 等价地重写为： 这里的η\\etaη是步长（PLA可以看做默认是1），v是方向，二者是迭代优化算法中必不可少的参数。 4 Gradient Descent 这里我想先写出梯度下降法的最终结果： wt+1←wt−η∇Ein (wt)∥∇Ein (wt)∥\\mathbf{w}_{t+1} \\leftarrow \\mathbf{w}_{t}-\\eta \\frac{\\nabla E_{\\text {in }}\\left(\\mathbf{w}_{t}\\right)}{\\left\\|\\nabla E_{\\text {in }}\\left(\\mathbf{w}_{t}\\right)\\right\\|} wt+1​←wt​−η∥∇Ein ​(wt​)∥∇Ein ​(wt​)​ 下面是推导： 假设此时已经有wtw_twt​，下一步要找出wt+1w_{t+1}wt+1​，我们的目标时找到一个合适的方向和步长使得： min⁡∥v∥=1Ein(wt+ηv)\\min _{\\|\\mathbf{v}\\|=1} E_{\\mathrm{in}}\\left(\\mathbf{w}_{t}+\\eta \\mathbf{v}\\right) ∥v∥=1min​Ein​(wt​+ηv) 对这个公式进行泰勒展开，一阶估计是： Ein(wt+ηv)≈Ein(wt)+ηvT∇Ein (wt)E_{\\mathrm{in}}\\left(\\mathbf{w}_{t}+\\eta \\mathbf{v}\\right)\\approx E_{in}(w_t)+\\eta v^T {\\nabla E_{\\text {in }}\\left(\\mathbf{w}_{t}\\right)} Ein​(wt​+ηv)≈Ein​(wt​)+ηvT∇Ein ​(wt​) 考虑贪婪方法，这一步的目标变成： 其中只有方向和步长是需要确定的参数，由于步长η\\etaη一定是正数，因此要求方向和梯度的内积最小，显然方向应当是梯度的反方向，即v=∇Ein (wt)∥∇Ein (wt)∥v=\\frac{\\nabla E_{\\text {in }}\\left(\\mathbf{w}_{t}\\right)}{\\left\\|\\nabla E_{\\text {in }}\\left(\\mathbf{w}_{t}\\right)\\right\\|}v=∥∇Ein ​(wt​)∥∇Ein ​(wt​)​ 这样就得到了最终的结果wt+1←wt−η∇Ein (wt)∥∇Ein (wt)∥\\mathbf{w}_{t+1} \\leftarrow \\mathbf{w}_{t}-\\eta \\frac{\\nabla E_{\\text {in }}\\left(\\mathbf{w}_{t}\\right)}{\\left\\|\\nabla E_{\\text {in }}\\left(\\mathbf{w}_{t}\\right)\\right\\|}wt+1​←wt​−η∥∇Ein ​(wt​)∥∇Ein ​(wt​)​ 关于η\\etaη的选取还需要探讨一下，选的太小会使得下降太慢，选的太大使得不稳定，良好的η\\etaη应当随着算法的迭代而变化。 从第三条曲线可以看出，梯度大的地方η\\etaη大一些、梯度小的地方η\\etaη小一些是较为合适。所以不妨假设η=η′∥∇Ein (wt)∥\\eta=\\eta&#x27;\\|{\\nabla E_{\\text {in }}\\left(\\mathbf{w}_{t}\\right)}\\|η=η′∥∇Ein ​(wt​)∥，其中η′\\eta&#x27;η′是一个固定的常数比例，这样可以得到： wt+1←wt−η′∇Ein (wt)∥∇Ein (wt)∥\\mathbf{w}_{t+1} \\leftarrow \\mathbf{w}_{t}-\\eta&#x27; \\frac{\\nabla E_{\\text {in }}\\left(\\mathbf{w}_{t}\\right)}{\\left\\|\\nabla E_{\\text {in }}\\left(\\mathbf{w}_{t}\\right)\\right\\|} wt+1​←wt​−η′∥∇Ein ​(wt​)∥∇Ein ​(wt​)​ 这个迭代式被称为固定学习率的梯度下降法（fixed learning rate gradient descent），η′\\eta&#x27;η′被称为学习率。","permalink":"http://yangtf983.github.io/2020/03/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B010%EF%BC%9ALogistic%20Regression/","photos":[]},{"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"title":"机器学习基石笔记9：Linear Regression","date":"2020/03/17","text":"0 说明 从这一节课开始讲几种算法。 这一节将线性回归，中间的计算步骤涉及到很多矩阵运算，由于这些矩阵运算一般是学过线性代数的人应该已经掌握的，所以文中对视频中对矩阵计算所做的一些解释就不再重复（实际上视频中的解释相当有限，直接看最终结果无碍）。未来几篇文章都会更加关注于对具体算法本身的理解，而对计算的解释将会尽可能忽略。 1 Linear Regression Problem 线性回归的假设是h(x)=wTxh(x)=w^Txh(x)=wTx，其中xxx向量的第一个元素是1，这样就将常数项并入了权重向量。 统计上最常用的误差衡量方式是平方误差： 2 Linear Regression Algorithm 我们想对上述EinE_{in}Ein​做一些变换，将其表示成矩阵形式，目的是用写出解的统一表达式，并且很多编程软件也矩阵运算，可以直接用矩阵形式进行编程。 改写方法如下： EinE_{in}Ein​可以证明是一个连续的可微的凸函数，可微函数的极小值点处梯度必然为0，因此可以转化为求梯度为0的点。 为了求出EinE_{in}Ein​梯度的矩阵表达式，先将EinE_{in}Ein​拆开，再用矩阵求导法则得到梯度表达式，中间步骤如下图： 再通过求逆得到最终的权重向量的表达式， 总结来看这个算法只需要下面三步即可完成： 其中的第二步可能存在由于XTXX^TXXTX不可逆而无法直接计算的问题，但是对于有线性回归内置函数的编程语言或者模块而言，都会有处理这种情况的内置方法，只需要将第一步的矩阵输入，无需考虑XTXX^TXXTX是否可逆。线性回归函数比较常见，因此建议采用这种方法，而不赞同自己造轮子。 3 Generalization Issue 从上一部分看来似乎线性回归的解是一个分析解，而不是通过迭代方法不断修正的，似乎和机器学习形式不太相同。 但也有一些理由支持它是一种机器学习方法。首先它可以找到一个比较小的EinE_{in}Ein​；其次，由于权重的参数数量是有限的，而dVCd_{VC}dVC​可以解释为自由度，因此线性回归算法有有限的dVCd_{VC}dVC​；最后，看似解析解的权重表达式中其中具有迭代的部分，例如求逆矩阵就要用到迭代方法，若对中间步骤都算一个权重，则这个权重应当是随着迭代越来越好的。 总的来说，若线性回归的EoutE_{out}Eout​是好的，learning就发生了。 通过一些步骤（课程中给了大致解释，包括一些简单的矩阵运算和线性代数的几何解释，这里省略），得到： Ein‾=σ2⋅(1−d+1N)Eout‾=σ2⋅(1+d+1N)\\begin{aligned} \\overline{E_{\\mathrm{in}}} &amp;=\\sigma^{2} \\cdot\\left(1-\\frac{d+1}{N}\\right) \\\\ \\overline{E_{\\mathrm{out}}} &amp;=\\sigma^{2} \\cdot\\left(1+\\frac{d+1}{N}\\right) \\end{aligned}Ein​​Eout​​​=σ2⋅(1−Nd+1​)=σ2⋅(1+Nd+1​)​ 从图中看二者的关系： 4 for Binary Classification PLA算法是一个NP难问题，只能得到近似解，但是线性回归是一个有解析解公式的算法，看起来结果的可靠性更能够保证，那么能否用线性回归算法解决二分类问题呢？ 可以通过二者的误差衡量函数来看，PLA是0/1误差，线性回归是平方误差，二者的公式和图如下所示： 从图中可以看出，当平方误差很小时，0/1误差也很小，并且平方误差总是大于等于0/1误差。 第7堂课中已经得到： 因此可以写出PLA的EoutE_{out}Eout​与线性回归的EinE_{in}Ein​的不等式关系： 可以看出，当线性回归的EinE_{in}Ein​足够小时，也就约束了PLA的EinE_{in}Ein​和EoutE_{out}Eout​足够小，这也就解释了可以用线性回归做二分类问题。 并且由于线性回归的err更好解（有解析式），因此可以先用线性回归得到一个相对不错的解，再将其作为初始解采用PLA或pocket算法寻找更好的解。","permalink":"http://yangtf983.github.io/2020/03/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B09%EF%BC%9ALinear%20Regression/","photos":[]},{"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"title":"机器学习基石笔记8：Noise and Error","date":"2020/03/16","text":"0 说明 之前的VC bound和VC Dimension是建立在一定的假设上的，这一堂课将学习如何放宽这些假设，使其能够适用于更多不同的问题。 1 Noise and Probabilistic Target （这一部分的内容都没有给出证明，只是给了一些解释，对这些解释可以直接跳过，结论就是加上噪声以后仍然可以learning到东西） 之前的机器学习流程图已经展示过多次了，现在的问题是给数据加上noise会对我们的理论造成什么样的影响？ 首先来看一下noise的三种类型： 图中以信用卡的例子为例，描述了噪声（就是noise）的三种类型： 第一种是关于y的噪声，特点是将标签标反，本来是+1的标签标成了-1； 第二种也是关于y的噪声，其特点是给同一个数据标上了不同的标签，这种情况常常出现在请多个专家标标签的情况，不同专家给同一个数据标的标签可能是不相同的，这样反映在图上就出现了同一个点既是圈又是叉的情况； 第三种是关于x的噪声，即对消费者信息的采集可能是不精确的。 问题是有了这些噪声的情况下VC bound还能不能作用的很好。 回忆一下VC bound证明的核心： VC bound推导的核心在于一个装满了两种颜色弹珠的罐子。其中的弹珠符合一个分布P(x)，当f(x)=h(x)时，对应弹珠的颜色是绿色的，否则是橙色的。 现在考虑一种特别的弹珠，其颜色可能变化，现在从每一颗弹珠来看，其颜色可能是蓝色也可能是绿色，但是从整个罐子来看，大部分情况下还是有一个大概的接近真实情况的颜色比例。现在有一种方式是几率抽出来瞬间变色弹珠的颜色，用这个记录中的比例来估计整个罐子中的颜色比例。既然这个估计是可以的，那么就可以把整个证明过程重写一次，这时原来的x仍然来源于P(x)，但原来的y现在来源于P(y|x)而不是f(x)，P(y|x)就相当于f(x)+noise. 现在的P(y|x)虽然在大部分情况下还是等于正确的标签值（因为噪声一般不会太大），但是也会有一个概率等于错误的标签值，我们用这样的标签值来做机器学习，实际上是我们learning的目标变成了在常见的点（来源于P(x)）上尽量做得好（目标值来源于P(y|x)） 总之，最终结论是VC bound仍然适用。 这样就解释了pocket算法的合理性，因为pocket算法就是将目标值看作来源于P(y|x)而搜索使得Ein(h)E_{in}(h)Ein​(h)最小的h作为g. 噪声情况下的算法流程图如下： 2 Error Measure 整个机器学习可行性证明的核心在于证明g≈fg\\approx fg≈f，之前衡量的方法是采用： Eout (g)=Ex∼P[g(x)≠f(x)]⏟err (g(x),f(x))E_{\\text {out }}(g)=\\underset{\\mathbf{x} \\sim P}{\\mathcal{E}} \\underbrace{[g(\\mathbf{x}) \\neq f(\\mathbf{x})]}_{\\text {err }(g(\\mathbf{x}), f(\\mathbf{x}))} Eout ​(g)=x∼PE​err (g(x),f(x))[g(x)​=f(x)]​​ 其中E\\mathcal{E}E是求期望的符号，中括号表示示性函数，每个点上G(x)与f(x)是否相同记为err(g(x),f(x))err(g(x),f(x))err(g(x),f(x))，样本点上的误差衡量是Ein(g)=1N∑n=1Nerr(g(xn),f(xn))E_{in}(g)=\\frac1N\\sum_{n=1}^Nerr(g(x_n),f(x_n))Ein​(g)=N1​∑n=1N​err(g(xn​),f(xn​))，整个样本空间上的误差衡量是Eout (g)=Ex∼P[g(x)≠f(x)]E_{\\text {out }}(g)=\\underset{\\mathbf{x} \\sim P}{\\mathcal{E}}[g(\\mathbf{x}) \\neq f(\\mathbf{x})]Eout ​(g)=x∼PE​[g(x)​=f(x)]，这种衡量方法属于pointwise的误差衡量方式，这也是这门课将会一直沿用的一种衡量方式。 下面是两种重要的pointwise的衡量方式： 0/1 error： err⁡(y~,y)=[y~≠y]\\operatorname{err}(\\tilde{y}, y)=[\\tilde{y} \\neq y]err(y~​,y)=[y~​​=y]，衡量正/误，常用于分类问题； squared error: err⁡(y~,y)=(y~≠y)2\\operatorname{err}(\\tilde{y}, y)=(\\tilde{y} \\neq y)^2err(y~​,y)=(y~​​=y)2，衡量距离，常用于回归。 上图中展示了两种误差衡量方式对同一个例子衡量误差的结果。若$\\tilde{y}=1$，采用第一种衡量方法的误差是$0.7*[1\\neq 2]+0.1*[1\\neq 3]=0.8$，采用第二种衡量方法的误差是$0.7*(1-2)^2+0.1*(1-3)^2=1.1$ 第一种衡量方法1.9的误差最大，但是第二种衡量方法1.9的误差却最小，可见误差衡量方法影响着衡量结果，进而影响着我们对最理想的结果的判断（因为最理想的结果往往都是衡量的误差最小的）。 图中紫色部分是两种衡量方式下最优（使得误差最小）预测的计算方法，前一个是选择概率最大的y，后一个是选择平均值。 这样算法流程图就变成了上图中的样子。由于不同误差衡量方法选出来的结果不一定相同，所以必须提前给出一个适合当前问题的误差衡量方法，最后还要用这个衡量方法对选出来的g做检验。 VC Dimension通过细微的修改可以适用于绝大多数H和err，后面不再各自推导。 3 Algorithmic Error Measure 误差衡量方法是哪里来的？ 一个指纹识别系统，可能会识别错误，有两种错误的情况，一种是错误的接受（本来不应接受），一种是错误的拒绝（本来不应拒绝）。 先考虑将这个系统用在一个超市识别折扣的系统上，若错误的拒绝给客户折扣，会给客户很不好的印象，若错误的给了用户折扣，则不过是少赚点钱，但是维护了超市的形象，这是更大的利益。这样一来就很清楚，在这个例子中错误的拒绝影响更坏，我们就应当在误差衡量中把错误拒绝的误差权重加大。 而若把这个系统应用于CIA情报系统，则错误的接受的影响一定更加严重（因为会泄露国家机密），因此一定要加大错误接受的误差权重。 因此错误衡量方法是要根据不同的应用和不同的决策者的喜好而变化的，即便都是二元分类也可能差别非常大。 因此在设计算法的时候要想办法用到合适的错误衡量方法。但是要量化这样的误差衡量方法常常是比较困难的，常常要用一些替代方式。 有两种常见的替代方式。一种是从常见的误差衡量方法中找一个看似合理的(plausible)，例如前述PLA采用0/1误差衡量，又如之后会讲的回归采用squared误差衡量，这种方法采用的误差衡量记为err^\\hat{err}err^，它不一定是最好的，但应当是最好的误差衡量方法的一种估计；另一种方法是找一个易于设计算法的(friendly)，例如容易求出解析解的衡量方法或者凸函数（可以采用很多方法得到较优解），这样选出来的误差衡量函数也被记为err^\\hat{err}err^。 这样我们就又能更新一次算法流程图： err是真正意义上最合适的误差衡量函数，但我们在算法中用的是err^\\hat{err}err^ CIA问题的err写法如下： &lt;img src=“https://myfoundationnote-1257754469.cos.ap-nanjing.myqcloud.com/机器学习基石笔记/08/fig07.png” 4 Weighted Classification 不同的错误有不同的重要性（前述超市和CIA的例子），称称这样的分类为weighted classification. 由上图可以知道CIA问题的EoutE_{out}Eout​表示为： Eout (h)=E(x,y)∼P{1 if y=+11000 if y=−1}⋅[y≠h(x)]E_{\\text {out }}(h)=\\underset{(\\mathbf{x}, y) \\sim P}{\\mathcal{E}}\\left\\{\\begin{array}{cl} 1 &amp; \\text { if } y=+1 \\\\ 1000 &amp; \\text { if } y=-1 \\end{array}\\right\\} \\cdot[y \\neq h(\\mathbf{x})]Eout ​(h)=(x,y)∼PE​{11000​ if y=+1 if y=−1​}⋅[y​=h(x)] EinE_{in}Ein​表示为： Ein (h)=1N∑n=1N{1 if y=+11000 if y=−1}⋅[y≠h(x)]E_{\\text {in }}(h)=\\frac1N\\sum_{n=1}^N\\left\\{\\begin{array}{cl} 1 &amp; \\text { if } y=+1 \\\\ 1000 &amp; \\text { if } y=-1 \\end{array}\\right\\} \\cdot[y \\neq h(\\mathbf{x})]Ein ​(h)=N1​n=1∑N​{11000​ if y=+1 if y=−1​}⋅[y​=h(x)] 我们记加权分类问题的EinE_{in}Ein​为EinWE_{in}^WEinW​，下面看一下这种问题的的解法（也就是如何最小化EinWE_{in}^WEinW​）： 若数据是线性可分的，采用PLA算法自不必说，一定可以找到最小的EinW=0E_{in}^W=0EinW​=0，可是若用修正的pocket算法（把Ein0/1E_{in}^{0/1}Ein0/1​换成EinWE_{in}^WEinW​），还能否保证得到好的结果？ 可以通过把数据按照权重复制成相应的份数再采用未修改过的pocket算法来做解释这个问题，总之结果是这个修改的pocket算法是有保证的，我们称之为weighted pocket算法，这个算法相对于原始的pocket算法有两步的更改，分别是： 用最小化EinWE_{in}^WEinW​替代最小化Ein0/1E_{in}^{0/1}Ein0/1​； 选择不同标签点的可能性与其标签的权重成正比（选到的点其实是按照一个点放入EinWE_{in}^WEinW​中计算总误差值的）。 这种 修改算法的方法被称为systematic route或reduction，可以用来将很多算法修改成有权重的情况。","permalink":"http://yangtf983.github.io/2020/03/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B08%EF%BC%9ANoise%20and%20Error/","photos":[]},{"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"title":"机器学习基石笔记7：The VC Dimension","date":"2020/03/16","text":"0 说明 这节课对VC Dimension做出定义和说明。 1 Definition of VC Dimension 首先回忆一下上节课所讲的VC bound： 从中可以看出，若同时具备有一个好的mH(N)m_H(N)mH​(N)和一个好的N这两个条件，则VC bound可以变得很小，也就是使得Eout=EinE_{out}=E_{in}Eout​=Ein​的概率很大。这里说的mH(N)m_H(N)mH​(N)好是指存在break point，N好是指N足够大。 在具备上面两个条件的同时，若再有一个好的算法A（这个好是指能够选出EinE_{in}Ein​很小的g），则可以说这个机器学习算法很可能学到了东西（不是绝对的，是几率问题，需要一点运气）。 下面定义VC Dimension：假设H下最大的non-break point，记作dvc(H)d_{vc}(H)dvc​(H) 可见，dvc(H)d_{vc}(H)dvc​(H)就是假设H下能够shatter的最多的点，dvc(H)=′minimum k′−1d_{vc}(H)=&#x27;minimum\\ k&#x27;-1dvc​(H)=′minimum k′−1；当N≤dvc(H)N\\leq d_{vc}(H)N≤dvc​(H)时，有可能H可以shatter这N个点（并不是一定的，例如PLA可以 shatter三个不共线的点，但没办法shatter三个共线的点）；当k&gt;dvc(H)k&gt;d_{vc}(H)k&gt;dvc​(H)时，k就是H的一个break point，只是不一定是最小的那个。 因此，从本文第一张图中的公式可以轻易得到：当N≥2N\\geq2N≥2且dvc≥2d_{vc}\\geq2dvc​≥2时，可以得到mH(N)≤NdVCm_H(N)\\leq N^{d_{VC}}mH​(N)≤NdVC​ 下面给出几种情况下的VC Dimension: 当VC Dimension有限时，无论算法、数据分布和目标函数f是什么，我们都可以使得Eout(g)≈Ein(g)E_{out}(g)\\approx E_{in}(g)Eout​(g)≈Ein​(g) 2 VC Dimension of Perceptrons 回忆之前的二维PLA，用VC Dimension的理论解释其可行性。 当数据是线性可分的时（服从某个未知的分布并且有一个未知的模型f），存在分类器能够使得Ein(g)=1E_{in}(g)=1Ein​(g)=1，已知此时的dvc=3d_{vc}=3dvc​=3，因此可以写出VC bound约束，当N足够大时，可以保证Eout(g)≈Ein(g)E_{out}(g)\\approx E_{in}(g)Eout​(g)≈Ein​(g)很可能发生，这样就使得Eout(g)≈0E_{out}(g)\\approx 0Eout​(g)≈0也很可能发生。 下面看一下在更高维度下的PLA的情况： 首先说明结论：d维情况下PLA的VC Dimendion为dVC=d+1d_{VC}=d+1dVC​=d+1 我们在先前的学习已经知道了一维情况下dVC=2d_{VC}=2dVC​=2，二维情况下dVC=3d_{VC}=3dVC​=3，都是符合这个结论的，但这并不足以证明这个结论，下面将证明这个结论，证明过程分别两步，依次是dVC≥d+1d_{VC}\\geq d+1dVC​≥d+1和dVC≤d+1d_{VC}\\leq d+1dVC​≤d+1 第一步用到的方法是用一定的方法在d维空间中找到d+1个点，再证明这样找到的d+1个点总是可以被shatter的。 上图中橙色部分表示的就是之前说的d+1个点，只是在最左侧加上了一列1，这等于是将PLA算法的threshold并入权重后的数据点写法，容易知道这个方阵是可逆的，因此无论标签值向量yyy是什么（当然指的是元素都是由+1和-1组成的），都可以令w=X−1yw=X^{-1}yw=X−1y，这样就有Xw=yXw=yXw=y，进而sign(Xw)=ysign(Xw)=ysign(Xw)=y。这样，将yyy取遍所有的可能情况都能得到合适的www，则证明了这样取到的d+1个点是可以被shatter的，从而也就证明了第一步的结论。 下面证明第二步，用的方法是证明任何d+2个点都是不能被shatter的。 首先我们表示出d维PLA情况下由任意d+2个点组成的一个矩阵，表示如下： 由于矩阵XXX是d+2行d+1列的，因此组成矩阵的d+2个行向量必然是线性相关的。 这里不妨先假设前d+1个行向量是线性无关的（课程中没有假设，不严谨），则可以通过前 d+1个行向量的线性组合表示第d+2个行向量（正如图中蓝色部分在乘权重wTw^TwT之前一样），此时系数aia_iai​是唯一确定的，这时定义一个yyy，其特征是前d+1个元素就是a1,...,ad+1a_1,...,a_{d+1}a1​,...,ad+1​的符号，根据第一步的结果前d+1个点是可以被shatter的，这样可以找到一个wTw^TwT使得前d+1个wTxiw^Tx_iwTxi​的符号都和aia_iai​相同，此时得到的等号右边的式子的结果一定是大于0的，也就是说若yyy的d+1个值为-1，则一定没办法在确保前d+1个点的分类都正确的情况下让wTxd+2&lt;0w^Tx_{d+2}&lt;0wTxd+2​&lt;0，因此证明了这d+2个点是无法被shatter **补充证明：**当前d+1个点线性相关时，可以找到其中的一个最大线性无关组，用这个无关组唯一表示出随便一个剩余不在无关组中的向量，然后采用刚刚的方法，可以证明这个无关组加上一个不在无关组中的向量一定是不能够被shatter的，由于更少的点都不能被shatter，所以原矩阵中的d+2个点一定不能被shatter，综合之前的部分，就证明了任意d+2个点都不能够被shatter 这样，我们就证明了d维情况下PLA的VC Dimendion为dVC=d+1d_{VC}=d+1dVC​=d+1，也就是说PLA在高维情况下仍然是可以学习到东西的。 3 Physical Intuition of VC Dimension 从直觉上可以对VC Dimension做出一些解释： d维情况下PLA所求的权重www有d+1个任意参数，称之为自由度，这个例子中自由度就等于dVC(H)d_{VC}(H)dVC​(H)。 dVC(H)d_{VC}(H)dVC​(H)指的是假设H下最多能够shatter的点，因此可以将dVC(H)d_{VC}(H)dVC​(H)理解为假设H的强度（powerfulness）。 通过更多的例子（这里不再列举）可以得到一条经验法则：dVC≈#free parametersd_{VC}\\approx \\#free\\ parametersdVC​≈#free parameters，也就是说VC Dimension大概（但不一定）等于自由参数的数量（自由度）。 这样，第五讲在描述M和两个重要问题的关系的时候的M就可以换成dVCd_{VC}dVC​，关系如下图： 4 Interpreting VC Dimension 现在希望更深入地了解VC Dimension是什么。 如图中所示，将VC bound记作δ\\deltaδ，则可以通过如下变换求出ϵ\\epsilonϵ的表达式： 因此，好事情（∣Ein−Eout∣≤ϵ|E_{in}-E_{out}|\\leq \\epsilon∣Ein​−Eout​∣≤ϵ）以概率1−δ1-\\delta1−δ发生，进而能够得到如下图所示的类似置信区间的EoutE_{out}Eout​的表达式： 这个区间的下界不重要（用灰色表示），上界重要，可以看出在样本量N和概率δ\\deltaδ固定的情况下上界由VC Dimension决定。 引入一个变量：模型复杂度。当VC Dimension变大时，H能够shatter更多的点，模型自然更复杂。 随着样本VC Dimension变大，可以选择的h变多，因此EinE_{in}Ein​变小。 开始的时候EinE_{in}Ein​变小的速度快，上图中的上界受EinE_{in}Ein​变小的影响大，也变小，之后受EinE_{in}Ein​变小的影响小，随着后一项变大的影响大，逐渐变大。 这样就可以得到下图： 其中Ω\\OmegaΩ是模型复杂度，几个变量的关系如图中蓝色区块所示。 从图中可以看出，虽然当模型复杂度增加时，EinE_{in}Ein​一直在减小，但是∣Ein−Eout∣≤ϵ|E_{in}-E_{out}|\\leq \\epsilon∣Ein​−Eout​∣≤ϵ却是在增大，实际问题中我们必须兼顾这两项都比较小，因此合理的模型复杂度不能太大也不能太小。 VC bound给出的上界是比较比较宽的，若我们要求达到上面两项都很小，从VC bound算出来的结果往往要是dVCd_{VC}dVC​的上万倍，但是实际上，经验法则告诉我们只要N≈10dVCN\\approx 10d_{VC}N≈10dVC​就可以得到较好的结果。 VC bound之所以宽松，可以解释为我们获得这个bound过程中的各种条件非常的宽松，进而导致了这样宽松的bound，分别包括下面几项： （1）hoeffding不等式对数据的分布P和目标函数f都没有任何的约束； （2）实际问题中能够选取的只是手头上的有限资料，但是在计算成长函数的时候却是考虑的空间上所有的点； （3）最终计算并不是直接用的成长函数mH(N)m_H(N)mH​(N)，而是其上界NdVCN^{d_{VC}}NdVC​； （4）用的是union bound，估计的是最坏的情况。 以后用的最多的并不是VC bound的公式，而是它带给我们的哲学上的信息，例如刚刚的要选择合适的模型复杂度（不能太大也不能太小），这是后面讲算法常会用到的东西，","permalink":"http://yangtf983.github.io/2020/03/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B07%EF%BC%9AThe%20VC%20Dimension/","photos":[]},{"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"title":"机器学习基石笔记6：Theory of Generalization","date":"2020/03/15","text":"0 说明 之前说明了假设的数量M可能达到无穷，这个时候会给机器学习的解释造成问题。之后我们定义了一个成长函数mH(N)m_H(N)mH​(N)，希望能够用这个成长函数替代M，并且在上一节的最后说明了如果成长函数mH(N)m_H(N)mH​(N)是多项式量级的，那么就可以达到之前用来解释有限M的条件：随着N的增大，好事情发生的概率可以无限趋近于1. 接下来要看一下成长函数的增长能不能不超过多项式量级。 1 Restriction of Break Point 成长函数mH(N)m_H(N)mH​(N)的定义是可以产生的dichotomy的最大数量，break point的定义是成长函数小于2N2^N2N的最小数据量。 下面从一个例子出发，这个例子中有N=3个数据点，break point k=2，也就是说mH(1)=21=2m_H(1)=2^1=2mH​(1)=21=2，而mH(2)&lt;22=4m_H(2)&lt;2^2=4mH​(2)&lt;22=4，接下来我们看一看mH(3)m_H(3)mH​(3)是多少： 由于要shatter两个点需要4个dichotomy，所以任意给出三个dichotomy都不能shatter三个点中的任意两个点（如上图），但是再增加一个dichotomy就可能出现shatter两个点的情况： 通过增加别的dichotomy可以达到4种dichotomy但是不shatter任意两个点的情况： 通过不断尝试，此时最多只能有4种dichotomy，再增加任何一种都会导致有两个点会shatter 当N=2时，最大的mH(2)=3&lt;22=4m_H(2)=3&lt;2^2=4mH​(2)=3&lt;22=4；当N=3时，最大的mH(3)=4≪23=8m_H(3)=4\\ll 2^3=8mH​(3)=4≪23=8，可见对于超过break point的点的数量，成长函数增加非常慢，似乎有可能降到多项式级别。 2 Bounding Function-Basic Cases 定义bounding函数B(N,k)，其意义是当break point=k时成长函数mH(N)m_H(N)mH​(N)的最大值。 注意这个函数是与假设HHH无关的，如果我们能够找到这样的函数，那么就不必管具体的假设，只要有berak point，就可以用同样的方法分析。 下面要说明的问题是bounding函数是不是多项式级别成长的。 根据之前的分析，已经可以得到B(2,2)=3, B(3,2)=4. 此外，若break point k=1，不难发现B(N,1)=1,∀ N=1,2,...B(N,1)=1,\\forall\\ N=1,2,...B(N,1)=1,∀ N=1,2,...，因此可以得到下表 由于当N&lt;k的时候N个点是一定可以shatter的，所以图中的上半部分可以直接用2N2^N2N填充： 当N=k时，由于刚刚不能shatter,因此bounding函数不可能超过2N−12^N-12N−1，因此不严格地可以就把对角线写成2N−12^N-12N−1（事实上这个结果应该是严格的，但是课程中并没有加以证明）： 3 Bounding Function-Inductive 现在填写表格下半部分： 首先我们从一个具体的地方开始，比如B(4,3)，我们通过计算机搜索的方法可以找到此时最多有11种情况，在下图左侧，按照一定的方法排序得到右侧的图： 右图中的排序方法是将x4x_4x4​单独拿出来看，发现1和5、2和8、3和10、4和11这四对在其余三个点上的情况分别是一样的，都是只改变x4x_4x4​ 若将拿出一个数据出来其余能够成对的数据量记作2α2\\alpha2α，不能成对的数据量记作β\\betaβ，则由于B(4,3)的情况下任意4个点不能被shatter，因此α+β≤B(3,3)\\alpha+\\beta\\leq B(3,3)α+β≤B(3,3) 又由于α\\alphaα是在x4x_4x4​成对的情况下(x1,x2,x3)(x_1,x_2,x_3)(x1​,x2​,x3​)的dichotomy数量，所以橙色部分不能够shatter(x1,x2,x3)(x_1,x_2,x_3)(x1​,x2​,x3​)中的任意两个，否则我们加上x4x_4x4​后一定能够shatter(x1,x2,x3,x4)(x_1,x_2,x_3,x_4)(x1​,x2​,x3​,x4​)中的某三个，所以α≤B(3,2)\\alpha\\leq B(3,2)α≤B(3,2)，这样我们就得到了： B(4,3)=α+(α+β)≤B(3,3)+B(3,3)B(4,3)= \\alpha + (\\alpha+\\beta)\\leq B(3,3)+B(3,3) B(4,3)=α+(α+β)≤B(3,3)+B(3,3) 显然这样的推导具有普适性，也就是说当N&gt;k时，有不等式B(N,k)≤B(N−1,k)+B(N−1,k−1)B(N,k)\\leq B(N-1,k)+B(N-1,k-1)B(N,k)≤B(N−1,k)+B(N−1,k−1) 通过数学归纳法容易得到： B(N,k)≤∑i=0k−1(Ni)B(N,k)\\leq \\sum_{i=0}^{k-1}\\left(\\begin{array}{c}N\\\\ i\\end{array}\\right) B(N,k)≤i=0∑k−1​(Ni​) 课程中没有给出这一公式的证明，这里给出补充证明如下： 补充证明： 数学归纳法 （1）当N&gt;1时，B(N,1)=1=(N0)B(N,1)=1=\\left(\\begin{array}{c}N\\\\ 0\\end{array}\\right)B(N,1)=1=(N0​)； 当N=k时，B(N,k)=2N−1=∑i=0N−1(Ni)=∑i=0k−1(Ni)B(N,k)=2^N-1=\\sum_{i=0}^{N-1}\\left(\\begin{array}{c}N\\\\ i\\end{array}\\right)=\\sum_{i=0}^{k-1}\\left(\\begin{array}{c}N\\\\ i\\end{array}\\right)B(N,k)=2N−1=∑i=0N−1​(Ni​)=∑i=0k−1​(Ni​)； （2）当N&gt;k&gt;1时： B(N,k)⩽∑i=0k−1(N−1i)+∑i=0k−2(N−1i)=(N−10)+[(N−10)+(N−11)]+⋯+[(N−1k−2)+(N−1k−1)]=(N0)+⋯+(Nk−1)=∑i=0k−1(Ni)\\begin{aligned} B(N, k) &amp; \\leqslant \\sum_{i=0}^{k-1}\\left(\\begin{array}{c} N-1 \\\\ i \\end{array}\\right)+\\sum_{i=0}^{k-2}\\left(\\begin{array}{c} N-1 \\\\ i \\end{array}\\right) \\\\ &amp;=\\left(\\begin{array}{c} N-1 \\\\ 0 \\end{array}\\right)+\\left[\\left(\\begin{array}{c} N-1 \\\\ 0 \\end{array}\\right)+\\left(\\begin{array}{c} N-1 \\\\ 1 \\end{array}\\right)\\right]+\\cdots+\\left[\\left(\\begin{array}{l} N-1 \\\\ k-2 \\end{array}\\right)+\\left(\\begin{array}{l} N-1 \\\\ k-1 \\end{array}\\right)\\right] \\\\ &amp;=\\left(\\begin{array}{c} N \\\\ 0 \\end{array}\\right)+\\cdots+\\left(\\begin{array}{c} N\\\\ k-1\\end{array}\\right) \\\\ &amp;=\\sum_{i=0}^{k-1}\\left(\\begin{array}{c} N \\\\ i \\end{array}\\right) \\end{aligned}B(N,k)​⩽i=0∑k−1​(N−1i​)+i=0∑k−2​(N−1i​)=(N−10​)+[(N−10​)+(N−11​)]+⋯+[(N−1k−2​)+(N−1k−1​)]=(N0​)+⋯+(Nk−1​)=i=0∑k−1​(Ni​)​ 证明结束 事实上再经过严格的数学证明，上式中的不等号是可以换成等号的。但这里只需要这样的不等式的结论就可以说明问题，∑i=0k−1(Ni)\\sum_{i=0}^{k-1}\\left(\\begin{array}{c}N \\\\ i\\end{array}\\right)∑i=0k−1​(Ni​)的最高次项是Nk−1N^{k-1}Nk−1，因此我们就证明了当break point存在时mH(N)m_H(N)mH​(N)最多是多项式增长的。 4 A Pictorial（形象化的） Proof 经过了这么多推导，那么是不是直接就可以将 成长函数换到hoeffding不等式中的M呢？实际上这么做是不严谨的（个人猜测的理由是这里不仅仅是用成长函数换掉M那么简单，因为我们要解决的是M是无限值的情况，这个时候直接换成一个有限只肯定是不严谨的，所以应当通过更加严谨地推导得到严谨的结果），真正严谨的结果应该是下图中的第二个公式： 注意第二个公式不是始终成立的，而是当N足够大的时候才成立。 这个公式的证明技巧性太强，课程中没有具体讲，只是讲了一下证明的概要，下面我们来说一说（一下说明略去的证明过程都是课程中本就略去的）。 第一步是消掉Eout(h)E_{out}(h)Eout​(h)，用的方式是再找一个样本集D′D&#x27;D′产生一个Ein′E_{in}&#x27;Ein′​，略去中间证明过程可以得到下式： 12P[∃h∈H s.t. ∣Ein (h)−Eout (h)∣&gt;ϵ]≤P[∃h∈H s.t. ∣Ein (h)−Ein ′(h)∣&gt;ϵ2]\\begin{aligned} &amp; \\frac{1}{2} \\mathbb{P}\\left[\\exists h \\in \\mathcal{H} \\text { s.t. }\\left|E_{\\text {in }}(h)-E_{\\text {out }}(h)\\right|&gt;\\epsilon\\right] \\\\ \\leq &amp; \\mathbb{P}\\left[\\exists h \\in \\mathcal{H} \\text { s.t. }\\left|E_{\\text {in }}(h)-E_{\\text {in }}^{\\prime}(h)\\right|&gt;\\frac{\\epsilon}{2}\\right] \\end{aligned}≤​21​P[∃h∈H s.t. ∣Ein ​(h)−Eout ​(h)∣&gt;ϵ]P[∃h∈H s.t. ∣Ein ​(h)−Ein ′​(h)∣&gt;2ϵ​]​ 之所以这么换，是因为EoutE_{out}Eout​可能是无限多种，但Ein,Ein′E_{in},E_{in}&#x27;Ein​,Ein′​有有限多种，其种类数由mH(N)m_H(N)mH​(N)限制。 第二步是用mH(2N)m_H(2N)mH​(2N)换掉M，个人理解这里可以把EoutE_{out}Eout​看作是恒为0的常数，把Ein−Ein′E_{in}-E_{in}&#x27;Ein​−Ein′​看作是新的EinE_{in}Ein​，显然Ein−Ein′E_{in}-E_{in}&#x27;Ein​−Ein′​只有有限多种可能性，由∣H(x1,...,xN,x1′,...xN′)∣|H(x_1,...,x_N,x_1&#x27;,...x_N&#x27;)|∣H(x1​,...,xN​,x1′​,...xN′​)∣限制（也就是mH(2N)m_H(2N)mH​(2N)），由于EoutE_{out}Eout​看作是恒为0的常数，这里就可以把这个新的公式完全看作是之前介绍的有限情况下的问题，采用 与之前的推导类似的方法就可以得到： BAD≤2P[∃h∈H s.t. ∣Ein(h)−Ein′(h)∣&gt;ϵ2]≤2mH(2N)P[ fixed h s.t. ∣Ein(h)−Ein′(h)∣&gt;ϵ2]\\begin{aligned} \\mathrm{BAD} &amp; \\leq 2 \\mathrm{P}\\left[\\exists h \\in \\mathcal{H} \\text { s.t. }\\left|E_{\\mathrm{in}}(h)-E_{\\mathrm{in}}^{\\prime}(h)\\right|&gt;\\frac{\\epsilon}{2}\\right] \\\\ &amp; \\leq 2 m_{H}(2 N) \\mathbb{P}\\left[\\text { fixed } h \\text { s.t. }\\left|E_{\\mathrm{in}}(h)-E_{\\mathrm{in}}^{\\prime}(h)\\right|&gt;\\frac{\\epsilon}{2}\\right] \\end{aligned}BAD​≤2P[∃h∈H s.t. ∣Ein​(h)−Ein′​(h)∣&gt;2ϵ​]≤2mH​(2N)P[ fixed h s.t. ∣Ein​(h)−Ein′​(h)∣&gt;2ϵ​]​ 第三步是得到最终的公式： BAD≤2mH(2N)P[ fixed h s.t.∣Ein(h)−Ein′(h)∣&gt;ϵ2]≤2mH(2N)⋅2exp⁡(−2(ϵ4)2N)\\begin{aligned} \\mathrm{BAD} &amp; \\leq 2 m_{\\mathcal{H}}(2 N) \\mathrm{P}\\left[\\text { fixed } h\\ \\mathrm{s.t.}\\left|E_{\\mathrm{in}}(h)-E_{\\mathrm{in}}^{\\prime}(h)\\right|&gt;\\frac{\\epsilon}{2}\\right] \\\\ &amp; \\leq 2 m_{H}(2 N) \\cdot 2 \\exp \\left(-2\\left(\\frac{\\epsilon}{4}\\right)^{2} N\\right) \\end{aligned}BAD​≤2mH​(2N)P[ fixed h s.t.∣Ein​(h)−Ein′​(h)∣&gt;2ϵ​]≤2mH​(2N)⋅2exp(−2(4ϵ​)2N)​ 关于最后一步并没有给出证明，只是举了一个hoeffding without replacement的例子。这个例子是说假设有一个只有2N个弹珠的小瓶子，每次取出来N个计算EinE_{in}Ein​，另外N个计算Ein′E_{in}&#x27;Ein′​，显然此时Eout=Ein+Ein′2E_{out}=\\frac{E_{in}+E_{in}&#x27;}{2}Eout​=2Ein​+Ein′​​，而∣Ein−Ein′∣&gt;ϵ2|E_{in}-E_{in}&#x27;|&gt;\\frac{\\epsilon}{2}∣Ein​−Ein′​∣&gt;2ϵ​等价于说EinE_{in}Ein​离二者的平均数距离超过ϵ4\\frac{\\epsilon}{4}4ϵ​（否则无法达到∣Ein−Ein′∣&gt;ϵ2|E_{in}-E_{in}&#x27;|&gt;\\frac{\\epsilon}{2}∣Ein​−Ein′​∣&gt;2ϵ​），也就是说∣Ein−Ein′∣&gt;ϵ2|E_{in}-E_{in}&#x27;|&gt;\\frac{\\epsilon}{2}∣Ein​−Ein′​∣&gt;2ϵ​等价于∣Ein−Ein+Ein′2∣&gt;ϵ4|E_{in}-\\frac{E_{in}+E_{in}&#x27;}{2}|&gt;\\frac{\\epsilon}{4}∣Ein​−2Ein​+Ein′​​∣&gt;4ϵ​，此时用有限假设情况下的hoeffding不等式可以轻易地得到第三步的结果，只是将这个结果推广到普适的情况这里不予证明。 最终，我们就得到了Vapnik-Chervonenkis(VC) bound: P[∃h∈H s.t. ∣Ein(h)−Eout′(h)∣&gt;ϵ]≤4mH(2N)exp⁡(−18ϵ2N)\\mathrm{P}\\left[\\exists h \\in \\mathcal{H} \\text { s.t. }\\left|E_{\\mathrm{in}}(h)-E_{\\mathrm{out}}&#x27;(h)\\right|&gt;\\epsilon\\right]\\leq 4m_\\mathcal{H}(2N)\\exp(-\\frac18\\epsilon^2N) P[∃h∈H s.t. ∣Ein​(h)−Eout′​(h)∣&gt;ϵ]≤4mH​(2N)exp(−81​ϵ2N) 最后用一个例子来看一下这个bound的约束效果： 从中可以看出，当我们拥有10000个数据并把误差放大到0.1时，给出的bound仍然算不上小（接近0.3）。 实际上，VC bound是一个约束不严格的上界，那么这个上界究竟有什么重要作用值得这样推导？留待下一节解释。","permalink":"http://yangtf983.github.io/2020/03/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B06%EF%BC%9ATheory%20of%20Generalization/","photos":[]},{"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"title":"机器学习基石笔记5：Training versus Testing","date":"2020/03/15","text":"0 说明 从这一节开始说明无限个假设情况下机器学习的可行性。 1 Recap and Preview 首先回顾一下前几趟课学习的知识。 第一堂课说的是机器学习的假设是存在一个未知的未知的模式f，机器学习算法的目的是找到一个g使其与f十分接近。 第四堂课把这个问题写成了Eout(g)≈0E_{out}(g)\\approx 0Eout​(g)≈0 第二堂课说可以想一些办法使得Ein(g)≈0E_{in}(g)\\approx 0Ein​(g)≈0，如PLA算法。 第三堂课说我们的问题是在很特殊的情况下做的，如在批次分类下做二元分类问题。 第四堂课最后想办法得到了Eout(g)≈Ein(g)E_{out}(g)\\approx E_{in}(g)Eout​(g)≈Ein​(g)的问题。在hoeffding不等式上界不大的情况下，使得Ein(g)E_{in}(g)Ein​(g)越来越小就很有可能使得Eout(g)E_{out}(g)Eout​(g)越来越小。 到这里我们把learning拆成了两个问题： 到底Eout(g)E_{out}(g)Eout​(g)与Ein(g)E_{in}(g)Ein​(g)能不能很接近？ 如何使得Ein(g)E_{in}(g)Ein​(g)尽可能小？ 当假设的数量很小的时候，也就是M很小的时候，很容易达到第一个问题的约束，但是很难达到第二个约束，因为选择有限，往往不一定能够找到能够使得Ein(g)E_{in}(g)Ein​(g)很小的g 当假设的数量M很大的时候，第二个问题很容易满足，但是第一个问题不容易满足，坏事情（Ein(g)E_{in}(g)Ein​(g)与Eout(g)E_{out}(g)Eout​(g)相离较远）发生的概率增加了。 由此可见M很重要，不能太大也不能太小。 那是不是说明无限大的M就是没办法机器学习的？PLA算法是难以保证合理性的？ 解决上面问题的思路是想办法将无限大的M换成一个有限大的mHm_{H}mH​，如果能够达到这一目标，就可以用上一堂课的结果说明无限大的M的机器学习算法的合理性。 1 Effective Number of Lines 上一堂课说明M其实是坏事情发生的次数，当时没有考虑到其实有时不同的坏事情是重叠的，也就是说有些坏事情是可能同时发生在不同假设上的。 现在我们的目的是找到这些坏事情重叠的部分。 思路是考虑能不能把无线的假设分成有限的类。 首先考虑一个平面上只知道一个数据点的情况： 从图中可以看出，此时平面上所有的直线可以仅仅分为两类，分别是使得x1x_1x1​标签为圈和叉的直线。 再看两个点的情况： 此时有4种线。 不难知道三个点时最多有8种线： 但是当三个点在同一条线上时就只有六种线。 当四个点任意三个不共线时有14种情况，其中的七种如下，另外其中是将这七种的符号反过来： 在二维平面上，仅从输入点来讲，线的种类是可以得到的，我们称线的最大种类为effective number of lines 记N个点的effective number of lines为effective(N)，则有： P[∣Ein (g)−Eout (g)∣&gt;ϵ]≤2⋅ effective (N)⋅exp⁡(−2ϵ2N)\\begin{aligned} &amp; \\mathbb{P}\\left[\\left|E_{\\text {in }}(g)-E_{\\text {out }}(g)\\right|&gt;\\epsilon\\right] \\\\ \\leq &amp; 2 \\cdot \\text { effective }(N) \\cdot \\exp \\left(-2 \\epsilon^{2} N\\right) \\end{aligned}≤​P[∣Ein ​(g)−Eout ​(g)∣&gt;ϵ]2⋅ effective (N)⋅exp(−2ϵ2N)​ 由于点的所有情况有共有2N2^N2N种，因此首先能知道effective number of lines≤2N\\leq2^N≤2N，但是这样还不够，因为若采用2N2^N2N作为上界，则上式的右侧只能估计到小于等于2exp⁡{N(ln⁡2−2ϵ2)}2\\exp\\{N(\\ln2-2\\epsilon^2)\\}2exp{N(ln2−2ϵ2)}，显然，当ϵ\\epsilonϵ足够小时，(ln⁡2−2ϵ2)(\\ln2-2\\epsilon^2)(ln2−2ϵ2)大于0，此时随着数据量N的变大不等式右侧的估计值居然是上升的，也就是说N再大也没办法说明坏事情发生的概率降低。而我们想要证明的结论是右侧随着N的变大可以无限趋近于0（也就是坏事情发生的概率可以趋近于0），用到的effective(N)的上界估计的数量级必须缩小。（这一段是课程中没有的补充解释） 2 Effective Number of Hypotheses 假设N个数据，一个二分类的假设，每一种可能情况记为一个dichotomy，整个空间上的直线分类器是无限多个，但是dichotomies H(x1,...,xN)dichotomies \\ H(x_1,...,x_N)dichotomies H(x1​,...,xN​)不会超过2N2^N2N 显然∣H(x1,...,xN)∣|H(x_1,...,x_N)|∣H(x1​,...,xN​)∣依赖于N个数据点的选取（如共线不共线会影响dichotomy的数量），而我们只需要找到最大的那个dichotomy的数量，记mH(N)=max⁡x1,...,xN∈X∣H(x1,...,xN)∣m_{H}(N)=\\max_{x_1,...,x_N\\in X}|H(x_1,...,x_N)|mH​(N)=maxx1​,...,xN​∈X​∣H(x1​,...,xN​)∣ 下面看一下能不能写出来mH(N)m_{H}(N)mH​(N)的函数（称为成长函数）。 在单方向的数轴上情况如下： 此时mH(N)=N+1≪2N, when N large!m_H(N)=N+1\\ll 2^N,\\ when\\ N\\ large!mH​(N)=N+1≪2N, when N large! 若确定一个区间，中间是圈，外面是叉，这种设定下的diochotomy情况如下： &lt;img src=“https://myfoundationnote-1257754469.cos.ap-nanjing.myqcloud.com/机器学习基石笔记/05/fig7.png&quot;width=&quot;70%” height=“70%”&gt; 再举一个例子，用凸区域来shatter二维平面上的点： 3 Break Point 从前面加粗部分的分析我们知道：当mH(N)m_H(N)mH​(N)的上界取2N2^N2N时无法证明机器学习算法是可行的。 那么若能够证明其上界可以由多项式给出呢？ 容易知道Nkexp⁡(−2ϵ2N)=exp⁡(kln⁡N−2ϵ2N)N^k\\exp(-2\\epsilon^2N)=\\exp(k\\ln N-2\\epsilon^2N)Nkexp(−2ϵ2N)=exp(klnN−2ϵ2N)，无论k多么大，当n足够大时，总能保证指数项是负的且随着N增大而减小。因此多项式是可以说明机器学习的可行性的。 那么PLA算法的成长函数的上界可以由多项式给出吗？ 这个问题的答案要等到下节课才能真正给出，这里先找一下成长函数里面第一个看起来“有一些希望”的点，也就是第一次无法做出所有的dichotomy的点的数量，记为break point，根据先前的分析，二维平面上的PLA的break point是4（最多14种情况，小于24=162^4=1624=16）。 另外，若N个点可以做出所有dichotomy的情形，则遮住一个点时剩下的点也可以做出所有dichotomy的情形。也就是说，当N&gt;break point时，一定没办法做出来2N2^N2N种情形，否则遮住N-break point个点应该能做出做出所有dichotomy的情形，也就是说break point点找错了。 前述几种情况的break point与成长函数如下（第三种情况没有break point）： 这里可以预先透露一下，break point与成长函数是有关系的，事实上mH(N)=O(Nk−1)m_H(N)=O(N^{k-1})mH​(N)=O(Nk−1)，具体情况下节再谈。","permalink":"http://yangtf983.github.io/2020/03/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B05%EF%BC%9ATraining%20versus%20Testing/","photos":[]},{"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"title":"机器学习基石笔记4：Feasibility of Learning","date":"2020/03/13","text":"0 说明 本节的问题是探究learning的可行性。 1 Learning is Impossible? 首先看一个不可行的例子： 图中，第一行的三个不同输入的y值是-1，第二行的三个不同输入的y值是+1，目标是正确回答第三行的输入的y值。 若根据轴对称规则判断，第一行都不是轴对称图形，第二行都是轴对称图形，则可以判断g(x)=+1；若根据左上角方块是否为黑色判断，第一行左上角方块都为黑色，第二行左上角方块都为白色，则可以判断g(x)=-1. 若机器学习产生的结果是按照轴对称规则，而实际上的规则是左上角是否为黑色，则学习错误；若机器学习到的结果是左上角是否为黑色，而实际上的规则是是否轴对称，则学习也错误。 从这个例子中来看，通过数据学习一个正确的规则似乎是不可能的。 再看一个用数字表达的例子： 图中DDD是已知的数据集，y是标签值，真正的模式f未知，现在要训练一个g估计f. 输出只有两类（圈或叉），输入是一个三维的0-1数组，有八种。每一个输入x值有两种可能的标签，所以在没有信息的情况下g可以有28=2562^8=25628=256种可能性。此处已知了五种x的输出值，还有三种无法确定，因此此处备选的g可能有23=82^3=823=8种，看起来已知的数据集确实减少了g的可能性并且提高了g与f的相同概率，但是事实上，已知的五种情况并不是机器学习得到的结果而是本来就知道的，剩余的三种情况并没有从中学习到任何规则，而这三种情况才是机器学习的目的，这里的机器学习算法是“无用”的。 之所以说这里剩余的三种情况没有学习到任何规则，是因为我们没有额外的信息或假设判断其倾向于哪种规则。事实上，这正是no free lunch theorem所说的事情。 no free lunch theorem: 不存在一个与具体算法无关的，普遍适用的“最优分类器”； 学习算法必须作出一个与问题领域有关的“假设”，分类器必须与问题域相适应。 从这两个例子来看，至少存在一些情况learning是做不到的。 2 Probability to the Rescue 现在思考是否有什么工具可以帮助我们对一些未知的东西进行推论/估计。 再看一个例子： 图中罐子中有非常多的绿弹珠和橙弹珠，现在要求罐子中橙色弹珠的比例μ\\muμ是多少，但是无法全部拿出来数一遍，一种常见的方法是从罐子中随机取一把弹珠，用其中橙色弹珠的比例ν\\nuν来估计整个罐子中橙色弹珠的比例μ\\muμ，二者应该比较接近。 但是即便罐子中橙色弹珠占大多数，我们抓一把出来也可能全是绿色弹珠，也就是说估计值和真实值差别可能很大，那么这种估计方法的合理性何在？样本中真的体现了总体中的某些信息吗？ 答案是肯定的，只是这种关系应当用概率的语言来描述：当样本数量N非常大时，ν\\nuν很可能很接近μ\\muμ，数学表达式为： P[∣ν−μ∣&gt;ϵ]≤2exp⁡(−2ϵ2N)\\mathbb{P}[|\\nu-\\mu|&gt;\\epsilon] \\leq 2 \\exp \\left(-2 \\epsilon^{2} N\\right) P[∣ν−μ∣&gt;ϵ]≤2exp(−2ϵ2N) 对于任何一个大于0的ϵ\\epsilonϵ，当N足够大时，总有上式成立，这一公式被称为Hoeffding’s Inequality，ν\\nuν与μ\\muμ的关系被称为probably approximately correct(PAC). 这里稍微扩展一下：学过概率论的同学应该能感觉到这个公式与大数定律非常像，事实上这个公式与大数定律的本质是一样的，因为μ\\muμ就是ν\\nuν的期望值，大数定律的一般形式是: lim⁡n→∞P(∥1n∑i=1nXi−1n∑i=1nEXi∥&lt;ϵ)=1,∀ ϵ&gt;0\\lim_{n\\rightarrow\\infty}P(\\|\\frac1n\\sum^n_{i=1}X_i-\\frac1n\\sum^n_{i=1}EX_i\\|&lt;\\epsilon)=1,\\forall\\ \\epsilon&gt;0 n→∞lim​P(∥n1​i=1∑n​Xi​−n1​i=1∑n​EXi​∥&lt;ϵ)=1,∀ ϵ&gt;0 本例中样本是二项分布，可以用伯努利大数定律$$\\lim_{n\\rightarrow\\infty}P(|\\frac{S_{n}}{n}-p|&lt;\\epsilon)=1,\\forall\\ \\epsilon&gt;0$$直接得到Hoeffding’s Inequality. 由此也可以知道ν\\nuν与μ\\muμ的关系就是ν\\nuν依概率收敛于μ\\muμ，因为大数定律的结论就是依概率收敛（convergence in probability）。 值得注意的是，虽然我们知道有PAC这样的关系存在，但是要算Hoeffding’s Inequality右边的界一般是不可能的，因为我们要知道μ\\muμ，而在实际问题中当我们知道μ\\muμ时就没有必要再用μ\\muμ估计它。 3 Connection to Learning 下面就从刚刚的例子转化到learning的问题中。 仍然假设机器学习的目标是f(x)f(x)f(x)，现在已有假设h(x)h(x)h(x)，罐子中的每一个弹珠代表一个数据xxx，若h(x)=f(x)h(x)=f(x)h(x)=f(x)，则弹珠颜色是绿色的，否则弹珠颜色是橘色的，此时整个罐子中的橘色弹珠的比例就代表了在全部数据上h(x)h(x)h(x)的表现，随机取出的一把弹珠中橘色弹珠的比例就代表了样本数据上h(x)h(x)h(x)的表现。 此时可以将之前的机器学习流程图进行扩充如下： 其中在整个数据集上有两次抽样，分别用来做训练集和测试h(x)≈f(x)h(x)\\approx f(x)h(x)≈f(x) 此时在整个数据集上的表现记为EoutE_{out}Eout​，是未知的；样本数据集上的表现记为EinE_{in}Ein​，是已知的。 unknown Eout (h)=Ex∼P[∣h(x)≠f(x)] by known Ein (h)=1N∑n=1N[h(xn)≠yn]\\begin{aligned} &amp;\\text { unknown } E_{\\text {out }}(\\mathrm{h})=\\underset{\\mathbf{x} \\sim P}{\\mathcal{E}}[| h(\\mathbf{x}) \\neq f(\\mathbf{x})]\\\\ &amp;\\text { by known } E_{\\text {in }}(\\mathrm{h})=\\frac{1}{N} \\sum_{n=1}^{N}\\left[h\\left(\\mathbf{x}_{n}\\right) \\neq y_{n}\\right] \\end{aligned}​ unknown Eout ​(h)=x∼PE​[∣h(x)​=f(x)] by known Ein ​(h)=N1​n=1∑N​[h(xn​)​=yn​]​ 二者分别对应于前述Hoeffding’s Inequality中的μ\\muμ和ν\\nuν，因此得到新的表示形式为： P[∣Ein(h)−Eout(h)∣&gt;ϵ]≤2exp⁡(−2ϵ2N)\\mathbb{P}[|E_{in}(h)-E_{out}(h)|&gt;\\epsilon] \\leq 2 \\exp \\left(-2 \\epsilon^{2} N\\right) P[∣Ein​(h)−Eout​(h)∣&gt;ϵ]≤2exp(−2ϵ2N) 这说明Ein(h)=Eout(h)E_{in}(h)=E_{out}(h)Ein​(h)=Eout​(h)是probably approximately correct的关系，当Ein(h)E_{in}(h)Ein​(h)很小的时候，很大概率上Eout(h)E_{out}(h)Eout​(h)也很小，h≈fh\\approx fh≈f 机器学习流程图中最终选择的规则是ggg，那么如何知道是否g≈fg\\approx fg≈f呢？ ggg是从假设集中选择出来的，若假设集中存在hhh能使得Ein(h)E_{in}(h)Ein​(h)很小（此时也就有这样的h≈fh\\approx fh≈f），而我们恰好选择了这样的hhh作为最终的ggg，那么就有g≈fg\\approx fg≈f，若没有这样的hhh或者说没有能选出这样的hhh的算法，也就是说Ein(h)E_{in}(h)Ein​(h)很小的条件达不到，自然无法选出来一个’g≈fg\\approx fg≈f'PAC. 用这样的算法需要有一种方法能够确认某个待定的hhh是好的还是不好的，这个确认的过程我们称为verification，它的流程图为： 通过这种方法可以确认某个假设hhh的表现是否是好的。 4 Connection to Real Learning 当有很多假设hhh时如何做？ 显然可以分别进行验证。但是有一个问题是，若某个假设在验证集上的验证结果是全对，那么要不要立即把这个假设作为ggg？ 先思考另一个问题：请150个同学丢铜板，每个人丢5次，至少有一个同学5次丢到的结果都是正面，这样的概率是多少？ 当硬币是公平的时，对一个人来说得到五个正面的概率是3132\\frac{31}{32}3231​，对150个人来说至少一个人得到5个正面的概率是1−(3132)150&gt;0.991-(\\frac{31}{32})^{150}&gt;0.991−(3231​)150&gt;0.99 Hoeffding’s Inequality告诉我们当样本数量N比较大时，EinE_{in}Ein​与EoutE_{out}Eout​很大概率上是接近的，但也有差别较大的情况，我们称这种情况是坏的，得到这样的结果的样本称为bad sample. 当数据在某个假设hhh上是坏数据，我们就说 这组数据是坏数据。 对于每一个假设hhh，Hoeffding’s Inequality告诉我们坏数据的可能性很小，也就是下图中一行中标出bad的概率是小的。 现在还想知道在所有的假设上坏数据的概率是多少，其上界可以采用下图所示方法估计： 从中可以看出，在所有的假设上Ein(g)=Eout(g)E_{in}(g)=E_{out}(g)Ein​(g)=Eout​(g)仍然是PAC的（与算法无关）。 当假设的数量M是有限的时，只要N足够大，无论用什么样的算法，仍然可以保证Ein(g)≈Eout(g)E_{in}(g)\\approx E_{out}(g)Ein​(g)≈Eout​(g)；若Ein(g)≈0E_{in}(g)\\approx 0Ein​(g)≈0，则PAC条件保证了Eout(g)≈0E_{out}(g)\\approx 0Eout​(g)≈0 此时可以说，当M有限大时机器可以学习到规则，本问中前述的机器学习流程图中的验证假设h≈fh\\approx fh≈f的线可以直接连到g≈fg\\approx fg≈f上。 但是如同之前所讲的PLA这样的算法其假设hhh有无穷个，这样的情况留待后续说明。","permalink":"http://yangtf983.github.io/2020/03/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B04%EF%BC%9AFeasibility%20of%20Learning/","photos":[]},{"tags":[{"name":"Semi-supervised Learning","slug":"Semi-supervised-Learning","permalink":"http://yangtf983.github.io/tags/Semi-supervised-Learning/"}],"title":"听课笔记：半监督学习（李宏毅）","date":"2020/03/11","text":"0 说明 本文是一篇半监督学习的听课笔记，主讲者是李宏毅，课程视频网址 1 半监督学习介绍 半监督学习的数据集由有标签数据集{(xr,y^r)}r=1R\\left\\{\\left(x^{r}, \\hat{y}^{r}\\right)\\right\\}_{r=1}^{R}{(xr,y^​r)}r=1R​和无标签数据集{xu}u=RR+U\\left\\{x^{u}\\right\\}_{u=R}^{R+U}{xu}u=RR+U​共同组成，通常二者数量U≫RU\\gg RU≫R. 半监督学习可以分为tranductive learning和inductive learning两类，二者最简单的区分方法是transductive learning的无标签数据就是它的测试集，只不过相对于监督学习而言transductive learning使用了测试集的feature，而inductive learning的无标签数据不是测试集，无法事先知道测试集是什么，建立的模型需要能够预测整个样本空间上的点。 为什么要用半监督学习？ （1）在机器学习过程中从来不缺乏data，缺的是labeled data，例如网络上有数不清的图片，但其中有标签的图片占比却很少，且给如此大量的图片标注label的代价过于昂贵； （2）人类也是常常在做半监督学习，例如见到以前未曾见过的狗的相片进一步丰富对够和其他动物之间区别的认知。这说明无标签数据是可以用来提高用有标签数据学习的结果的，并且这种现象在人类学习中也很常见。 为什么半监督学习可能有用？ 无标签数据的分布能传递出一定信息，但是否有用取决于假设是否合理。 2 Semi-supervised Generative Model 已给标签数据集xr∈C1,C2x^{r}\\in C_{1},C_{2}xr∈C1​,C2​ 寻找一个最有可能的先验概率P(Ci)P(C_{i})P(Ci​)与所属类别的条件概率P(x∣Ci)P(x|C_{i})P(x∣Ci​) P(x∣Ci)P(x|C_{i})P(x∣Ci​)是一个参数为μi\\mu^{i}μi和Σ\\SigmaΣ的高斯分布。 P(C1∣x)=P(x∣C1)P(C1)P(x∣C1)P(C1)+P(x∣C2)P(C2)P\\left(C_{1} | x\\right)=\\frac{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)}{P\\left(x | C_{1}\\right) P\\left(C_{1}\\right)+P\\left(x | C_{2}\\right) P(C_{2})} P(C1​∣x)=P(x∣C1​)P(C1​)+P(x∣C2​)P(C2​)P(x∣C1​)P(C1​)​ 1. 若用上无标签数据（用浅绿色表示），图像可能会变成： 从图中可以看出，虽然浅绿色数据没有标签信息，但我们已经倾向于根据它们将决策边界由椭圆改为圆形，在这个过程中重新估计了$P(C_{1}),P(C_{2}),\\mu^{1},\\mu^{2},\\Sigma$，从而影响了decision boundary. 从上述过程可以看出无标签数据对生成模型做决策产生的作用，可以利用如下方法利用这种影响： 1. 初始化$\\theta=\\{P(C_{1}),P(C_{2}),\\mu^1,\\mu^2,\\Sigma\\}$; 2. E步：计算每个无标签数据的后验概率$P_{\\theta}(C_{1}|x^\\mu)$; 3. M步：更新模型 $$P(C_1)=\\frac{N_{1}+\\Sigma_{x^\\mu}P(C_1|x^\\mu)}{N}$$ $$\\mu^{1}=\\frac{1}{N_{1}} \\sum_{x^{r} \\in C_{1}} x^{r}+\\frac{1}{\\sum_{x^{u}} P\\left(C_{1} | x^{u}\\right)} \\sum_{x^{u}} P\\left(C_{1} | x^{u}\\right) x^{u}$$ $N$是总数据量，$N_1$是属于$C_1$的数据量。 4. 重复EM步。 理论上可以保证收敛，但是初始值会影响收敛的结果。 **为什么采取这种做法？** 只存在有标签数据时用最大似然法求参数的表达为$\\log L(\\theta)=\\sum_{x^{r}} \\log P_{\\theta}\\left(x^{r}, \\hat{y}^{r}\\right)$，其中$P_\\theta(x^r,\\hat{y}^r)=P_\\theta(x^r|\\hat{y}^r)P(\\hat{y}^r)$，此最大似然函数有解析解。 当加上无标签数据时，最大似然函数的对数函数变为$\\log L(\\theta)=\\sum_{x^{r}} \\log P_{\\theta}\\left(x^{r}, \\hat{y}^{r}\\right)+\\sum_{x^u}\\log P_\\theta(x^u)$，其中$P_\\theta(x^u)=P_\\theta(x^u|C_1)P(C_1)+P_\\theta(x^u|C_2)P(C_2)$（由于$x^u$可能在任何一个类中），此最大似然函数只有数值解，采用迭代方法。 ## 3 Low-Density Separation 思想：非黑即白。即不同类标签中有明显的分界线。 **Self-training** self-training是低密度分割假设最具代表性的最简单的算法。 1. 用有标签数据训练一个模型$f^*$ 2. 将模型$f^*$用在无标签数据集上，产生的标签称为pseudo-label 3. 从无标签数据集中移出一个数据集放入有标签数据集 4. 重复前三步 self-training是hard label，也就是模型结果一定是属于某一个类，而generative model则是soft label，只要算出属于某类的概率即可以迭代。 若用soft label的模型来训练self-training，则迭代不会更新$f^*$，因为新放入有标签数据集的无标签数据本身就是在$f^*$上的。 若用neural network训练self-training，则一定要用hard label，因为soft label不会更新模型。 **Entropy-based Regularization** 模型在无标签数据上对每一个数据属于每个类的可能性都会有一个估计值，这些估计值越集中，我们认为这样的模型越好，这些估计值越不集中，我们认为这样的估计值越不好。 上图中前两个分布的概率比较集中，是好的，最后一个概率分散，是不好的。 衡量$y^u$的集中度可以用熵（entropy），熵的计算公式是$E(y^u)=-\\sum^5_{m=1}y^u_m\\ln (y^u_m)$，前两种情况的$E(y^u)=0$，第三种情况的$E(y^u)=-\\ln (\\frac{1}{5})=\\ln 5$，是$E(y^u)$的最大值。 对于有标签数据，显然目的是使得模型结果与真实标签最为接近，这二者的距离表示为$\\sum_{x^r}C(y^r,\\hat{y}^r)$ 因此熵准则下目的是最小化： $$L=\\sum_{x^r}C(y^r,\\hat{y}^r)+\\lambda \\sum_{y^u}E(y^u)$$ 其中$\\lambda$的选取依赖于有标签数据与无标签数据相对重要程度。 由于$L$可以计算微分，因此可以用梯度下降法训练。 **Semi-supervised SVM** SVM的目的是找到一个能够最小化误差的最宽的线作为分类器。 半监督学习SVM的思路如上图所示，无标签数据有很多不同的标签可能，目的是找到一种情况使得SVM分类器的误差最小同时找到最粗的那条线。 当无标签数据点较多时，情况非常多以至于无法穷举。一种近似的思路是首先给每个无标签数据确定一个标签，之后每次改一些标签，若修改后能够产生更好的分类器，则修改被保留 ，否则重新修改。 4 Semi-supervised Learning Smoothness Assumption 思想：近朱者赤近墨者黑。即相似的x有相似的y^\\hat{y}y^​ 定义：若一个高密度区域中的两个点x1,x2x_{1},x_{2}x1​,x2​距离很近，那么其对应的输出y1,y2y_{1},y_{2}y1​,y2​也应当接近或一样。 文件分类问题常符合此假设。下图中did_idi​代表文章，实心点代表文章中包含的词汇类别。左图中很容易看出来文章d1d_1d1​和d3d_3d3​类别相近，d4d_4d4​和d2d_2d2​类别相近。右图中则看不出四者哪些相近。 但是当搜集到更多文章数据时，可以产生下图： 根据半监督学习平滑性假设，从上图中又可以看出文章d1d_1d1​和d3d_3d3​类别相近，d4d_4d4​和d2d_2d2​类别相近。 Cluster and then Label 上图中红色和绿色点分别表示两类数据，蓝色点表示无标签数据。先聚类再标注，可以将数据分为三簇，一簇属于第一类，两簇属于第二类。 Graph-based Approach 光滑性假设的应用前提是要知道怎么判断不同点是否在同一个高密度区域中很接近。 在图中可以较好地表现出来这一特征，但如何建立这样的图？ 有时这种图可以很自然得到。如进行网页的分类，网页和网页之间可以通过有超链接的关系连接起来；又如进行论文间的联系，可以 利用论文之间引用的关系进行连接不同论文。 但有时需要根据一些经验和方法建立图： step 1. 定义点xix^ixi与xjx^jxj之间的相似度s(xi,xj)s(x^i,x^j)s(xi,xj) step 2. 建立graph: K Nearest Neighbor：将每个点与距其最近的k个点连起来； e-Neighborhood：将每个点与距离其距离小于 e的点全部连起来； step 3. 边的权重与相似度s(xi,xj)s(x^i,x^j)s(xi,xj)成正比 一种建议的定义相似度的函数是Gaussian Radial Basis Function: s(xi,xj)=exp⁡(−γ∥xi−xj∥2)s(x^i,x^j)=\\exp(-\\gamma\\|x^i-x^j\\|^2) s(xi,xj)=exp(−γ∥xi−xj∥2) 经验上用这样的函数效果一般较好。这个函数对距离增加的惩罚很大。 图建立后，就可以使用图方法。 如假定有标签数据的标签会影响到其邻近点，标签会通过图传播，则会产生下图的情况： 一种常见的定义图的平滑性的方法： S=12∑i,j(yi−yj)2,i,j for all data(no matter labbelled or not)S=\\frac{1}{2}\\sum_{i,j}(y^i-y^j)^2,i,j\\ for\\ all\\ data(no\\ matter \\ labbelled\\ or\\ not) S=21​i,j∑​(yi−yj)2,i,j for all data(no matter labbelled or not) SSS越小越光滑。 图中左图S=0.5S=0.5S=0.5，右图S=3S=3S=3，左图更光滑。 用矩阵方法表示光滑度函数如下图： 其中矩阵WWW每个元素是边缘权重，DDD的对角线元素是WWW对应行元素的和。 SSS的值取决于网络的参数。 同样可以通过梯度下降最小化LLL","permalink":"http://yangtf983.github.io/2020/03/11/%E5%90%AC%E8%AF%BE%E7%AC%94%E8%AE%B0%EF%BC%9A%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%88%E6%9D%8E%E5%AE%8F%E6%AF%85%EF%BC%89/","photos":[]},{"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"title":"机器学习基石笔记笔记3：学习类型","date":"2020/01/24","text":"【0】说明 之前在笔记1中的机器学习三要素*（ 1. 存在一个可以被学习的潜在模式； 2. 不知道如何定义规则并写入程序； 3. 可以获得大量数据。）*中已经讲过，机器学习可以执行的前提是存在可以学习的模式以及足够的数据。在实际情况中，学习的模式和输入的数据都可能有很多不同的类型，相应的也诞生了很多不同的学习算法和思想，根据这些不同，机器学习有很多分类，这一节我们介绍一些分类方法和机器学习类型。 【1】Learning wiwh Different Output Space Y\\mathcal{Y}Y 上一节我们介绍了PLA算法和SVM算法，它们只能将一个空间分成两部分，也就是两个不同的类，即输出空间只有+1和-1,称这样的分类问题为二元分类。 此外，根据输出空间的更多不同情况，还存在多种类别，总结如下： 二元分类 binary classification 输出空间：Y={−1,+1}\\mathcal{Y}=\\{-1,+1\\}Y={−1,+1} 举例：判断是否发放信用卡（输出空间：发放；不发放） 多元分类 multiclass classification 输出空间：Y={1,2，…,K}(abstractly)\\mathcal{Y}=\\{1,2，\\ldots,K\\}(abstractly)Y={1,2，…,K}(abstractly) 举例：手写数字识别（输出空间：0,1,…,9） 回归 regression 输出空间：Y=R or Y=[lower,upper]⊂R\\mathcal{Y}=\\mathbb{R}\\ or\\ \\mathcal{Y}=[lower,upper]\\subset\\mathbb{R}Y=R or Y=[lower,upper]⊂R 举例：根据公司数据预测股价 结构化学习 structured learning 输出空间：Y=structures\\mathcal{Y}=structuresY=structures 距离：蛋白质测序；语句结构识别（自动识别 一个句子中哪些是动词哪些是名词等） 除了以上四种，还有更多分类，这里不再进行介绍。但是值得一提的是，在这么多问题中，最核心的是二分类和回归，许多其他的问题的算法都是由这两种问题的算法进行改进得到的，甚至对于回归问题做一些处理也可以解决而分类问题（如逻辑斯谛回归）。 【2】Learning with Different Data Label yny_{n}yn​ 根据标签值的不同特点可以将机器学习分成以下几类： 监督学习 supervised learning 所有的数据都有标签，学习的目的是给出正确的标签值。 无监督学习 unsupervised learning 所有数据都没有标签，学习的目的是找出感兴趣的数据结构，比如概率密度等。 半监督学习 semi-supervised learning 一部分数据有标签，另一部分数据无标签，无标签数据可以辅助提高学习的精度，一般而言无标签数据远多于有标签数据。 强化学习 reinforcement learning 输出结果是一个行为而不是一个标签，建立奖励函数对这个行为的好坏进行判断。一般而言强化学习更加适合有明确规则的情况，如围棋。规则明确并且简单时容易建立奖励函数。强化学习需要知道三个信息，分别是：action(行为),observation(观测值),reward(奖励)。目前其最广为人知的应用可能是alphago. 【3】Learning with Different Protocol batch learning 特点：每次抽取一批数据进行训练，这要求所有的数据在一开始就是确定的，计算机通过一定的程序每次从中随机抽取一批进行训练。 online learning 特点：数据一个个进来，每次数据的更新都能对模型进行优化，因此称作在线学习。例如在线邮件过滤系统可以根据当前算法判断下一封邮件的内容，再根据用户反馈（如用户反馈判断错误）及时对模型进行优化。 avtive learning 特点：是一种新的机器学习类型，其特点是让机器主动问问题来提升模型的性能。如手写数字识别中，可以让机器对自己判断困难的数字进行提问，由人来对其打标签，这类判断困难的 数据往往对于提升模型性能更加有效。其优势之一是在获取样本标签困难的时候可以节约时间和成本，只对一些重要的数据打标签。 以上三种学习类型可以分别类比为：填鸭式、老师教学和主动问问题。 【4】Learning with Different Input Space X\\mathcal{X}X 根据输入的值类型不同，可以分为具体特征、原始特征和抽象特征，下面分别介绍： concrete feature 具体特征是指输入值具有清晰的实际意义，例如输入用户的收入和存款，判断是否应该发放信用卡。这样的例子中输入数据的实际意义是很明显的，与需要判断的结果联系很密切。具体特征的选择要求实验者有一定的相关知识，这样才能选择最合适的特征。也由于具体特征与判断目标的相关性较大，无关信息少，因此相应的算法往往更加简单，计算难度也较小。 raw feature 原始特征是指具有一些简单的实际意义的数据。例如输入图片的像素值组成的数组来判断图像中是什么。这样的数据中包含与判断结果相关的信息，例如在鉴别数字1和5时可以通过像素值得到图像的对称性和密度，这两个特征对于判别1和5显然是较为有效的，如果提取出来这两个特征并用其来作为判别的数据，那么就变成了具体特征，这个提取特征的过程有一个好听的名字叫做“特征提取”。在传统机器学习中，这一步往往需要认为来进行尝试，在深度学习中已经可以自动提取，不过需要更加大量的数据。 abstract feature 抽象特征是指数据没有或者很少有实际意义，例如输入数据是用户ID，ID数字的大小和接近程度并没有实际的意义，不同的ID只是代表不同的用户，显然其中包含的信息非常少。抽象特征也是三种输入空间中最难进行机器学习的 一种，也需要特征工程。","permalink":"http://yangtf983.github.io/2020/01/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B03%EF%BC%9A%E5%AD%A6%E4%B9%A0%E7%B1%BB%E5%9E%8B/","photos":[]},{"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"title":"机器学习基石笔记2：从PLA到SVM","date":"2020/01/23","text":"【0】说明 PLA，全称perceptron learning algorithm，是一种二分类算法，也可以认为是单层神经网络，与SVM有着密不可分的关系。SVM，即support vector machine，中文名是支持向量机，也是一种常见的二分类算法，这种算法的 一种推导方式就是从PLA出发，找到一条“最好的”PLA分类器。在机器学习基石课程中林轩田老师只介绍了PLA，而SVM是他在后续的另一门课程——机器学习技法——中介绍的。由于二者具有紧密的联系，故在此一并介绍。 【1】PLA介绍 PLA是解决二分类问题的一种算法，其来源可以从几何上理解：自变量可以投影到一个超平面，通过构造这个超平面的一个线性分割，尽可能好地将不同的类别的点分开。 PLA(perceptron learning algorithm)，中文名：线性感知机算法，又称线性分类器。 【2】符号表示 已有数据指标X=(x1,…,xd)\\mathcal{X}=(x_{1},\\ldots,x_{d})X=(x1​,…,xd​)，数据标签Y=(+1,−1)\\mathcal{Y}=(+1,-1)Y=(+1,−1)，目标是通过PLA算法 计算出一系列权重W=(w1,…,wd)\\mathcal{W}=(w_{1},\\ldots,w_{d})W=(w1​,…,wd​)，使得当Σi=1dwixi&gt;threshold\\Sigma^{d}_{i=1}w_{i}x_{i}&gt;thresholdΣi=1d​wi​xi​&gt;threshold时，预测标签值y=1，当Σi=1dwixi&lt;threshold\\Sigma^{d}_{i=1}w_{i}x_{i}&lt;thresholdΣi=1d​wi​xi​&lt;threshold，预测标签值y=-1，即h(x)=sign((Σi=1dwixi)−threshold),h∈Hh(x)=sign((\\Sigma^{d}_{i=1}w_{i}x_{i})-threshold),h\\in\\mathcal{H}h(x)=sign((Σi=1d​wi​xi​)−threshold),h∈H h(x)h(x)h(x)的形式可以进行简化： h(x)=sign((Σi=1dwixi)+(−threshold)∗(+1))=sign(Σi=0dwixi)=sign(wTx),h∈H\\begin{array}{rl} h(x)&amp;=sign((\\Sigma^{d}_{i=1}w_{i}x_{i})+(-threshold)*(+1))\\\\ &amp;=sign(\\Sigma^{d}_{i=0}w_{i}x_{i})\\\\ &amp;=sign(w^{T}x),h\\in\\mathcal{H}\\end{array}h(x)​=sign((Σi=1d​wi​xi​)+(−threshold)∗(+1))=sign(Σi=0d​wi​xi​)=sign(wTx),h∈H​ 容易看出上述简化其实是将thresholdthresholdthreshold记为了w0w_{0}w0​，显然x0=1x_{0}=1x0​=1. 简化成这种形式有两个好处，一是可以把threshold放到权重w中一起求出，二是在几何表示上更加直观，付出了将指标维数+1的牺牲，但是可以将超平面转移到一定经过原点，这对于我们后面的图形表示以及理解PLA算法都很有帮助。 【3】PLA过程 从表达式h(x)=sign(wTx),h∈Hh(x)=sign(w^{T}x),h\\in\\mathcal{H}h(x)=sign(wTx),h∈H中可以看出，对于和向量www呈锐角的数据xxx，由于二者内积是正的，所以用该分类器给出的预测值是1；反之，对于呈钝角的数据，给出的预测值是-1，对于呈直角的数据，该式未给出合理的预测值，此时可以理解为从该分类器中无法辨别该数据究竟更加符合哪一类，可以随机制定一个类别。 根据上述分析，若将自变量xxx在空间中的位置标出，并在其中画出以www为法向量并经过原点的（超）平面，则平面一侧的所有点代表的向量都与www呈锐角，另一侧的所有点代表的向量都与www呈钝角，平面上的所有点代表的向量都与www呈直角，即该（超）平面就是由上式h(x)h(x)h(x)定义的分类器。 下面以二维情况为例对PLA算法进行解释。 显然，二维情况下H\\mathcal{H}H应当包含平面上所有的直线，最理想的算法应当是从平面上所有的直线中找到一条最好的直线作为ggg，不过我们无法将平面上所有的直线遍历，一个合理的办法是设定初值再进行迭代。这也是PLA算法的基本思路。下面看一下迭代过程： For t=0,1,… step1. find the next mistake of wtw_{t}wt​ called (xn(t),yn(t))(x_{n(t)}, y_{n(t)})(xn(t)​,yn(t)​) sign(wtTxn(t)≠yn(t))sign(w^{T}_{t}x_{n(t)}\\neq y_{n(t)}) sign(wtT​xn(t)​​=yn(t)​) step2. correct the mistake by wt+1←wt+yn(t)xn(t)w_{t+1}\\leftarrow w_{t}+y_{n(t)}x_{n(t)} wt+1​←wt​+yn(t)​xn(t)​ …until a full cycle of not encountering mistakes (“next” can follow navie cycle (1,…,N) or precomputed random cycle) 上述算法又被称为循环PLA，因为它必须经过一个确定的循环没有错误后才会终止。 从中可以看到每次迭代的规则是： wt+1←wt+yn(t)xn(t)w_{t+1}\\leftarrow w_{t}+y_{n(t)}x_{n(t)} wt+1​←wt​+yn(t)​xn(t)​ 由于只对sign(wtTxn(t)≠yn(t))sign(w^{T}_{t}x_{n(t)}\\neq y_{n(t)})sign(wtT​xn(t)​​=yn(t)​)的点进行迭代得到新的www，即找到yn(t)wtTxn(t)&lt;0y_{n(t)}w^{T}_{t}x_{n(t)}&lt;0yn(t)​wtT​xn(t)​&lt;0的点时进行一次迭代，因此最终得到的ggg是满足yn(t)wtTxn(t)&gt;0,∀ty_{n(t)}w^{T}_{t}x_{n(t)}&gt;0,\\forall tyn(t)​wtT​xn(t)​&gt;0,∀t的线性函数。 且每次迭代后，yn(t)wt+1Txn(t)≥yn(t)wtTxn(t)y_{n(t)}w^{T}_{t+1}x_{n(t)}\\geq y_{n(t)}w^{T}_{t}x_{n(t)}yn(t)​wt+1T​xn(t)​≥yn(t)​wtT​xn(t)​，即yn(t)wt+1Txn(t)y_{n(t)}w^{T}_{t+1}x_{n(t)}yn(t)​wt+1T​xn(t)​一定在变大，所以模型确实随着不断迭代在优化。 从图形上看，用向量运算的方法容易看出，每次迭代的结果都使得wt+1w_{t+1}wt+1​和xn(t)x_{n(t)}xn(t)​之间的夹角变得更优了，如下图： 不过值得一提的是，这种优化只是对于预测错误的那个点的优化，在总的数据集上的表现有没有优化是不确定的，有可能在某一步迭代过后得到分类器在总的数据集上的表现更差。下面是一个PLA迭代过程中每一步的表示： 从这个实例可以看到，第七到第八步迭代时，用来计算迭代式的点进行了优化并通过迭代取得了正确的结果，但是新的分类器在数据集上的错误却增加了一个，即表现变差了。此外，这个迭代 最终产生了一个在数据集上表现完美的分类器，说明了前述的循环PLA算法是有效的，我们的机器确实学到了东西。 【4】有穷性与Pocket算法 因为终止条件没有时间或步数限制，所以上述算法的有穷性有待考虑。 循环PLA算法能够终止的前提是对所有的数据点都得到正确的分类，因此显然其终止前提是至少存在一个分类器可以将不同类别的点 分开，因为PLA是线性分类器，所以这个前提的等价说法是不同类别的区域应当是线性可分的，也就是可以用线性分类器将其分开。这个要求可以用数学中一个概念来表述，那就是不同类别点组成的区域应当是凸区域。 上述表述说明线性可分（凸区域）是终止的前提条件，下面我们说明，当数据点线性可分时，我们可以找到算法步数的上界的表达式，从而证明线性可分是算法有界的充分条件；当数据点非线性可分时，我们应当对算法的终止条件进行修改避免死循环。 线性可分时 线性可分时，存在理想模式fff，记对应的分类器的权重为wfw_{f}wf​，满足yn=sign(wfTxn),∀ny_{n}=sign(w^{T}_{f}x_{n}),\\forall nyn​=sign(wfT​xn​),∀n，按照循环PLA算法流程，仍取w0=(0,…,0)w_{0}=(0,\\ldots,0)w0​=(0,…,0)，可以做如下推导： yn(t)wfTxn(t)≥minn{ynwfTxn}&gt;0wfTwt+1=wfT(wt+yn(t)xn(t))≥wfTwt+minn{ynwfTxn}≥…≥wfTw0+(t+1)∗minn{ynwfTxn}=(t+1)∗minnynwfTxn∣∣wt∣∣2=∣∣wt−1+yn(t−1)xn(t−1)∣∣2=∣∣wt−1∣∣2+2yn(t−1)wt−1Txn(t−1)+∣∣yn(t−1)xn(t−1)∣∣2≤∣∣wt−1∣∣2+0+∣∣yn(t−1)xn(t−1)∣∣2≤∣∣wt−1∣∣2+maxn{∣∣xn∣∣2}≤…≤∣∣w0∣∣2+t∗maxn{∣∣xn∣∣2}=t∗maxn{∣∣xn∣∣2}wfT∥wf∥wT∥wT∥≥T∗min⁡n{ynwfTxn}∥wfT∥∗∥wT∥≥T∗min⁡n{ynwfTxn}∥wfT∥∗T∗max⁡n∥xn∥≥T∗min⁡n{ynwfTxn}∥wfT∥∗max⁡n∥xn∥=T∗ constant \\begin{array}{rl} y_{n(t)}w^{T}_{f}x_{n(t)}&amp;\\geq min_{n}\\{ y_{n}w^{T}_{f}x_{n} \\}\\\\ &amp;&gt;0\\\\ w^{T}_{f} w_{t+1} &amp;= w^{T}_{f}(w_{t}+y_{n(t)}x_{n(t)}) \\\\ &amp;\\geq w^{T}_{f}w_{t} + min_{n}\\{ y_{n}w^{T}_{f}x_{n} \\} \\\\ &amp;\\geq \\ldots \\\\ &amp;\\geq w^{T}_{f}w_{0}+(t+1)*min_{n}\\{ y_{n}w^{T}_{f}x_{n} \\} \\\\ &amp;=(t+1)*min_{n}{y_{n}w^{T}_{f}x_{n}}\\\\ ||w_{t}||^{2} &amp;= || w_{t-1}+y_{n(t-1)x_{n(t-1)}} ||^{2}\\\\ &amp;= ||w_{t-1}||^{2}+2y_{n(t-1)}w^{T}_{t-1}x_{n(t-1)}+|| y_{n(t-1)}x_{n(t-1)} ||^{2}\\\\ &amp;\\leq ||w_{t-1}||^{2}+0+|| y_{n(t-1)}x_{n(t-1)} ||^{2} \\\\ &amp;\\leq ||w_{t-1}||^{2}+max_{n}\\{ ||x_{n}||^{2} \\}\\\\ &amp;\\leq \\ldots\\\\ &amp;\\leq ||w_{0}||^{2}+t*max_{n}\\{ ||x_{n}||^{2} \\}\\\\ &amp;= t*max_{n}\\{ ||x_{n}||^{2} \\}\\\\ \\frac{w_{f}^{T}}{\\left\\|w_{f}\\right\\|} \\frac{ w_{T} }{\\left\\|w_{T}\\right\\|} &amp;\\geq \\frac{T * \\min _{n}\\left\\{y_{n} \\mathrm{w}_{f}^{T} x_{n}\\right\\}}{\\left\\|w_{f}^{T}\\right\\|*\\left\\|w_{T}\\right\\|}\\\\ &amp;\\geq \\frac{T * \\min _{n}\\left\\{y_{n} w_{f}^{T} x_{n}\\right\\}}{\\left\\|w_{f}^{T}\\right\\| * \\sqrt{T} * \\max _{n}\\left\\|x_{n}\\right\\|} \\\\ &amp;\\geq \\frac{\\sqrt{T} * \\min _{n}\\left\\{y_{n} \\mathrm{w}_{f}^{T} x_{n}\\right\\}}{\\left\\|w_{f}^{T}\\right\\|*\\max _{n}\\left\\|x_{n}\\right\\|}\\\\ &amp;=\\sqrt{T} * \\text { constant } \\end{array}yn(t)​wfT​xn(t)​wfT​wt+1​∣∣wt​∣∣2∥wf​∥wfT​​∥wT​∥wT​​​≥minn​{yn​wfT​xn​}&gt;0=wfT​(wt​+yn(t)​xn(t)​)≥wfT​wt​+minn​{yn​wfT​xn​}≥…≥wfT​w0​+(t+1)∗minn​{yn​wfT​xn​}=(t+1)∗minn​yn​wfT​xn​=∣∣wt−1​+yn(t−1)xn(t−1)​​∣∣2=∣∣wt−1​∣∣2+2yn(t−1)​wt−1T​xn(t−1)​+∣∣yn(t−1)​xn(t−1)​∣∣2≤∣∣wt−1​∣∣2+0+∣∣yn(t−1)​xn(t−1)​∣∣2≤∣∣wt−1​∣∣2+maxn​{∣∣xn​∣∣2}≤…≤∣∣w0​∣∣2+t∗maxn​{∣∣xn​∣∣2}=t∗maxn​{∣∣xn​∣∣2}≥∥wfT​∥∗∥wT​∥T∗minn​{yn​wfT​xn​}​≥∥wfT​∥∗T​∗maxn​∥xn​∥T∗minn​{yn​wfT​xn​}​≥∥wfT​∥∗maxn​∥xn​∥T​∗minn​{yn​wfT​xn​}​=T​∗ constant ​ 由于wfT∥wf∥wT∥wT∥≤1\\frac{w_{f}^{T}}{\\left\\|w_{f}\\right\\|} \\frac{ w_{T} }{\\left\\|w_{T}\\right\\|} \\leq 1∥wf​∥wfT​​∥wT​∥wT​​≤1，故T∗min⁡n{ynwfTxn}∥wfT∥∗max⁡n∥xn∥≤1\\frac{\\sqrt{T} * \\min _{n}\\left\\{y_{n} \\mathrm{w}_{f}^{T} x_{n}\\right\\}}{\\left\\|w_{f}^{T}\\right\\|*\\max _{n}\\left\\|x_{n}\\right\\|}\\leq 1∥wfT​∥∗maxn​∥xn​∥T​∗minn​{yn​wfT​xn​}​≤1，进而得到T≤(T∗min⁡n{ynwfTxn}∥wfT∥∗max⁡n∥xn∥)2=constant′.T\\leq (\\frac{\\sqrt{T} * \\min _{n}\\left\\{y_{n} \\mathrm{w}_{f}^{T} x_{n}\\right\\}}{\\left\\|w_{f}^{T}\\right\\|*\\max _{n}\\left\\|x_{n}\\right\\|})^{2}=\\text{constant}&#x27;.T≤(∥wfT​∥∗maxn​∥xn​∥T​∗minn​{yn​wfT​xn​}​)2=constant′. 至此，我们证明了在存在wfw_{f}wf​的情况下T存在上界，但是由于机器学习问题中我们不可能提前知道wfw_{f}wf​，因此无法算出这个上界的精确值，只能知道其存在上界。此外，wfw_{f}wf​也只有在数据点线性可分的情况下才可能存在（不考虑noise）。 实际问题中我们还常常面临着测量误差（noise的一种）的问题，一组线性可分的数据，可能因为测量误差而并非线性可分，但是在测量误差不大的情况下，使用PLA算法仍然可以找到一个合适的近似函数ggg，只是此时需要对算法的终止条件进行改动，否则就会陷入死循环。 一个自然的想法是：既然g≈fg\\approx fg≈f，不妨求出一个在数据集上表现最好（判断错误数最少）的ggg作为fff的近似。此时，PLA问题等价于优化： wg←argmin⁡w∑n=1N∥yn≠sign⁡(wTxn)∥\\mathbf{w}_{g} \\leftarrow \\underset{\\mathbf{w}}{\\operatorname{argmin}} \\sum_{n=1}^{N} \\| y_{n} \\neq \\operatorname{sign}\\left(\\mathbf{w}^{T} \\mathbf{x}_{n}\\right)\\| wg​←wargmin​n=1∑N​∥yn​​=sign(wTxn​)∥ 这个问题经过证明是一个N-P难问题，我们求解这样的问题一般采用求近似解的方法，不是想办法找到该优化问题的最优解，而是找近似最优解。求PLA问题的近似最优解的算法又称作Pocket算法，其想法是我们总之把当前最好的一个分类器放在口袋中，只有迭代后的新的分类器的表现比当前口袋中分类器的表现更好时才会将当前口袋中的分类器扔掉将新的分类器放入口袋，这里评价好坏的标准就是看谁的wg\\mathbf{w}_{g}wg​更小，此外，为了避免当数据点过多时遍历花费太多时间，一般的选择是仅仅选取一个随机子集来判断分类器的好坏,再通过设定最大迭代步数对算法进行终止。根据这种思想，可以写出Pocket算法流程如下： initialize pocket weight w^\\hat{\\mathbf{w}}w^ For t=0,1,…\\ldots… step1. find a (random) mistake of wt\\mathbf{w}_{t}wt​ called (xn(t),yn(t))(\\mathbf{x}_{n(t)},y_{n(t)})(xn(t)​,yn(t)​) step2. (try to) correct the mistake by wt+1←wt+yn(t)xn(t)\\mathbf{w}_{t+1}\\leftarrow \\mathbf{w}_{t}+y_{n(t)}x_{n(t)} wt+1​←wt​+yn(t)​xn(t)​ step3. if wt+1\\mathbf{w}_{t+1}wt+1​ makes fewer mistakes than w^\\hat{\\mathbf{w}}w^, replace w^\\hat{\\mathbf{w}}w^ by wt+1\\mathbf{w}_{t+1}wt+1​ …until enough iterations return w^\\hat{\\mathbf{w}}w^ (called wPOCKET\\mathbf{w}_{POCKET}wPOCKET​) as ggg 从PLA到SVM PLA算法的可能结果不唯一，如下图展示了同一组数据的多个最优PLA分类器（这里的最优是对所有数据点均判断正确，后续会定义新的“最优”）： 在这种情况下，我们希望定义 更加“严格”的最优，最好是使得上述情况只能有一种最优解。一种合理的方法是将数据测量的误差考虑在内，尽量使得数据测量有误差时不对类别的判断产生影响，也称之为更稳定（more robust）。 一般而言数据测量的误差不大，在真实数据周围较近的位置，因此可以以不同数据点为圆心画等半径圆并要求所有圆不能与分类器相交，此时找出一个分类器使得能够做出最大的半径，这个分类器就是我们要找的最优分类器，如下图第三个分类器： 等价地，我们可以用线宽来表现这种稳定性，不断增加分割线的宽度，当分割线恰好与数据点相交时，线宽最大的线最稳定，二者的等价性是显然的，线宽的表示如图： 从图中也可以看出，最大线宽有时不是由所有点决定的，上例中最大线宽仅仅由三个数据点即可决定，还有一个数据点没有起作用。类似于概率统计中将概率密度不为0的集合定义为支撑集（support set），这里将这些对线宽起决定作用的点定义为支持向量（support vector），将求这个具有最大线宽的线性分类器的方法称为支持向量机（support vector machine）. 用公式表示这个最优化问题： max⁡wmargin⁡(w) subject to every ynwTxn&gt;0margin⁡(w)=min⁡n=1,…,Ndistance⁡(xn,w)\\begin{array}{rl} {\\max _{\\mathbf{w}}} &amp; {\\operatorname{margin}(\\mathbf{w})} \\\\ {\\text { subject to }} &amp; {\\text { every } y_{n} \\mathbf{w}^{T} \\mathbf{x}_{n}&gt;0} \\\\ {} &amp; {\\operatorname{margin}(\\mathbf{w})=\\min _{n=1, \\ldots, N} \\operatorname{distance}\\left(\\mathbf{x}_{n}, \\mathbf{w}\\right)} \\end{array}maxw​ subject to ​margin(w) every yn​wTxn​&gt;0margin(w)=minn=1,…,N​distance(xn​,w)​ 下面处理这个最优化问题。 在本篇文章的【2】符号表示一节中我们将thresholdthresholdthreshold并入了www中： h(x)=sign((Σi=1dwixi)+(−threshold)∗(+1))=sign(Σi=0dwixi)=sign(wTx),h∈H\\begin{array}{rl} h(x)&amp;=sign((\\Sigma^{d}_{i=1}w_{i}x_{i})+(-threshold)*(+1))\\\\ &amp;=sign(\\Sigma^{d}_{i=0}w_{i}x_{i})\\\\ &amp;=sign(w^{T}x),h\\in\\mathcal{H}\\end{array}h(x)​=sign((Σi=1d​wi​xi​)+(−threshold)∗(+1))=sign(Σi=0d​wi​xi​)=sign(wTx),h∈H​ 这里，我们将xxx和www缩短，也就是去掉x0x_{0}x0​和w0w_{0}w0​，将thresholdthresholdthreshold再单独出来，此时h(x)=sign(wTx+b)h(x)=sign(w^{T}x+b)h(x)=sign(wTx+b) 数据点x\\mathbf{x}x到超平面的距离由(x,b,w)( \\mathbf{x},b,\\mathbf{w} )(x,b,w)决定，记为distance⁡(x,b,w)\\operatorname{distance}\\left(\\mathbf{x}, b, \\mathbf{w}\\right)distance(x,b,w) 超平面上的点由wTx+b=0\\mathbf{w}^{T}\\mathbf{x}+b=0wTx+b=0确定。 若记x′\\mathbf{x}&#x27;x′为超平面上的点，则distance⁡(x,b,w)\\operatorname{distance}\\left(\\mathbf{x}, b, \\mathbf{w}\\right)distance(x,b,w)为x′−x\\mathbf{x}&#x27;-\\mathbf{x}x′−x到超平面的法向量w\\mathbf{w}w的映射，即： distance⁡(x,b,w)=project(x′−x)to⊥hyprtplane=∣wT∥w∥(x−x′)∣=(1)1∥w∥∣wTx+b∣\\begin{array}{rl} \\operatorname{distance}\\left(\\mathbf{x}, b, \\mathbf{w}\\right) &amp;= project(\\mathbf{x}&#x27;-\\mathbf{x})to\\perp hyprtplane\\\\ &amp;=\\left|\\frac{\\mathbf{w}^{T}}{\\|\\mathbf{w}\\|}\\left(\\mathbf{x}-\\mathbf{x}^{\\prime}\\right)\\right| \\\\ &amp;\\stackrel{(1)}{=} \\frac{1}{\\|\\mathbf{w}\\|}\\left|\\mathbf{w}^{T} \\mathbf{x}+b\\right| \\end{array}distance(x,b,w)​=project(x′−x)to⊥hyprtplane=∣∣∣​∥w∥wT​(x−x′)∣∣∣​=(1)∥w∥1​∣∣​wTx+b∣∣​​ 这样，原问题就转化为： max⁡b,wmargin⁡(b,w) subject to every yn(wTxn+b)&gt;0margin⁡(b,w)=min⁡n=1,…,N1∥w∥yn(wTxn+b)\\begin{array}{rl} {\\max _{b,\\mathbf{w}}} &amp; {\\operatorname{margin}(b, \\mathbf{w})} \\\\ {\\text { subject to }} &amp; {\\text { every } y_{n} (\\mathbf{w}^{T} \\mathbf{x}_{n}+b)&gt;0} \\\\ &amp; {\\operatorname{margin}(b,\\mathbf{w})}=\\min _{n=1, \\ldots, N} \\frac{1}{\\|\\mathbf{w}\\|}y_{n}(\\mathbf{w}^{T} \\mathbf{x}_{n}+b) \\end{array}maxb,w​ subject to ​margin(b,w) every yn​(wTxn​+b)&gt;0margin(b,w)=minn=1,…,N​∥w∥1​yn​(wTxn​+b)​ 易知wTx+b=0\\mathbf{w}^{T} \\mathbf{x}+b=0wTx+b=0与3wTxn+3b=03\\mathbf{w}^{T} \\mathbf{x}_{n}+3b=03wTxn​+3b=0表示相同的超平面，因此wT\\mathbf{w}^{T}wT与bbb的大小不影响分类器，其比例才影响分类器，故可以通过改变其大小使得min⁡n=1,…,Nyn(wTxn+b)=1\\min _{n=1, \\ldots, N} y_{n}(\\mathbf{w}^{T} \\mathbf{x}_{n}+b)=1minn=1,…,N​yn​(wTxn​+b)=1，此时优化目标可以表示为margin⁡(b,w)=1∥w∥{\\operatorname{margin}(b,\\mathbf{w})}=\\frac{1}{\\|\\mathbf{w}\\|}margin(b,w)=∥w∥1​，优化问题可以转化如下： max⁡b,w1∥w∥ subject to every yn(wTxn+b)&gt;0min⁡n=1,…,Nyn(wTxn+b)=1\\begin{array}{cl} {\\max _{b, \\mathbf{w}}} &amp; {\\frac{1}{\\|\\mathbf{w}\\|}} \\\\ {\\text { subject to }} &amp; {\\text { every } y_{n}\\left(\\mathrm{w}^{T} \\mathrm{x}_{n}+b\\right)&gt;0} \\\\ {} &amp; {\\min _{n=1, \\ldots, N} y_{n}\\left(\\mathbf{w}^{T} \\mathrm{x}_{n}+b\\right)=1} \\end{array} maxb,w​ subject to ​∥w∥1​ every yn​(wTxn​+b)&gt;0minn=1,…,N​yn​(wTxn​+b)=1​ 进一步将问题转化为： min⁡b,w12wTw subject to yn(wTxn+b)≥1for all n\\begin{array}{cl} {\\min _{b, \\mathbf{w}}} &amp; {\\frac{1}{2}\\mathbf{w}^{T}\\mathbf{w}} \\\\ {\\text { subject to }} &amp; { y_{n}\\left(\\mathrm{w}^{T} \\mathrm{x}_{n}+b\\right)\\geq 1} for\\ all\\ n \\end{array} minb,w​ subject to ​21​wTwyn​(wTxn​+b)≥1for all n​ 上述表达为二次规划标准型，二次规划的求解已经是一个比较成熟的领域，在很多软件中都有求解该问题的函数，按照对应的格式将数据传入相应函数即可求出SVM的解。","permalink":"http://yangtf983.github.io/2020/01/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B02%EF%BC%9A%E4%BB%8EPLA%E5%88%B0SVM/","photos":[]},{"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"title":"机器学习基石笔记1：机器学习简介","date":"2020/01/23","text":"【0】 说明 机器学习基石是一门很好的机器学习入门课，能够让人在短时间内掌握大量的机器学习的基础理论和原理。从这篇文章开始，我会用大约十几篇文章的内容将机器学习基石我听机器学习基石这门课时候的所学和思考表达出来，我的笔记中会包含大部分基石课程中的内容，并且不会略去任何课程中的证明。对于课程中证明讲的不清楚的一些地方，我还会附上我的理解。关于这门课程的课件可以从课程主页上下载。 【1】机器学习概念 人类的学习行为可以看作是从观察和实践中学习获得技能，类似地，可以定义机器学习为机器从数据中学习获得技能。 【2】应用领域 人类无法手动编程的系统——探索火星的系统； 人类难以轻易写出程序规则的系统——语音/图像识别； 人类无法做到的快速决策系统——高速股票交易系统； 个性化服务系统——广告精准投放。 【3】机器学习三要素 一般认为，一个系统要想应用机器学习方法，需要具备三个条件： 存在一个可以被学习的潜在模式； 不知道如何定义规则并写入程序； 可以获得大量数据。 这三个要素缺一不可：首先，被学习的问题中必须有一个可以被学习的模式，否则机器学习算法永远不可能学到有用的内容，例如用机器学习算法预测一个公平骰子下一次掷出几点；其次，这个规则应当是不知道如何定义，或至少不知道如何准确定义的，否则直接用程序写出规则更加简单和节省算力；最后，要有机器学习的来源，也就是大量的数据，后面我们将会提到，机器学习的可信度与数据量密不可分，而且随着模型变得复杂，达到同样的可信度需要的数据量会剧增。 【4】例子 预测婴儿在某一时刻会不会哭（没有规则，不适合机器学习）； 判断一个 已给的图像中 是否包含一个圆（可以写出规则，不适合机器学习）； 决定是发否应该发给某人信用卡（满足三个条件，适合机器学习）； 预测接下来的十年地球是否会被核武器毁灭（没有充足的数据，不适合机器学习）； 【5】符号化表示 以上述例3为例，将机器学习中的要素进行符号化表示如下： 输入：x∈Xx\\in \\mathcal{X}x∈X（刻画消费者特征的相关指标） 输出：y∈Yy\\in \\mathcal{Y}y∈Y（适合/不适合发放信用卡） 未知的模式：f:X→Yf:\\mathcal{X}\\rightarrow \\mathcal{Y}f:X→Y（理想的信用卡发放公式） 数据（训练集）：D={(x1,y1),…,(xN,yN}\\mathcal{D}=\\{ (x_{1},y_{1}),\\ldots,(x_{N},y_{N} \\}D={(x1​,y1​),…,(xN​,yN​}（银行的历史数据） 假设：g:X→Yg:\\mathcal{X}\\rightarrow \\mathcal{Y}g:X→Y（g可以通过对数据进行学习逼近fff） 模型的假设：g∈Hg\\in \\mathcal{H}g∈H，ggg是假设H\\mathcal{H}H中表现最好的那个； 学习流程：fff（未知）→ {(xn,yn)}\\rightarrow \\ \\{(x_{n},y_{n})\\}→ {(xn​,yn​)}（来自fff的训练集）$\\rightarrow\\ $算法 → g≈f\\rightarrow\\ g\\approx f→ g≈f 【6】机器学习 vs 数据挖掘 vs 人工智能 vs 统计学 机器学习 数据挖掘 使用数据得到一个逼近目标f的近似模式g 从大量数据中寻找感兴趣的特性 当感兴趣的性质就是寻找估计的目标函数时，二者一致 当感兴趣的性质与寻找估计的目标函数时有关时，两者的方法可以相互帮助 机器学习 人工智能 使用数据得到一个逼近目标f的近似模式g 使机器能够做一些智能行为 得到近似于f的g的过程就是一个显示智能的过程，因此，机器学习是实现人工智能的一种方法 机器学习 统计学 使用数据得到一个逼近目标f的近似模式g 使用数据推断未知的参数或目标 更重视如何通过计算机得到结果 在一定的假设下通过数学证明得到一些可信的结果，很少关注计算的实现 当g是需要推断的结果，f是未知的目标时，统计学与机器学习的目标和方法是一致的，因此统计学的很多方法和工具可以用于机器学习。","permalink":"http://yangtf983.github.io/2020/01/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B01%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/","photos":[]},{"tags":[{"name":"Semi-supervised Learning","slug":"Semi-supervised-Learning","permalink":"http://yangtf983.github.io/tags/Semi-supervised-Learning/"}],"title":"论文翻译：半监督学习介绍","date":"2020/01/09","text":"0 说明 本文是一篇关于半监督学习介绍性论文集的第一篇的翻译，大部分是人工翻译，少部分是机器翻译加人工修改。不过限于知识有限，其中很多内容并不了解，所以一些地方翻译不到位，仅供参考。 论文集电子版链接onedrive 1 监督学习、无监督学习与半监督学习 为了理解半监督学习的本质，有必要首先了解一下监督学习和无监督学习。 1.1 监督学习与无监督学习 一般来说，传统的机器学习有两种不同类型，即无监督学习（Unsupervised Learning ）和监督学习（Supervised Learning）。 首先看一下无监督学习。假设存在一个包含n个数据点的集合X=(x1,…,xn)X=(x_{1},\\ldots,x_{n})X=(x1​,…,xn​)，其中xi∈X,i∈[n]:={1,…,n}x_{i}\\in\\mathcal{X},i\\in[n]:=\\{1,\\ldots,n\\}xi​∈X,i∈[n]:={1,…,n}.通常这些数据点被认为是独立同分布于分布X\\mathcal{X}X，常将其方便地定义为n∗dn*dn∗d维矩阵X=(xiT)i∈[n]TX=(x_{i}^{T})^{T}_{i\\in[n]}X=(xiT​)i∈[n]T​，该矩阵的每一行也就是一个数据点，无监督学习的目标就是从这样的数据XXX中找到一个有趣的结构。通常可以认为无监督学习的基本的问题是找到一个产生数据XXX的概率密度。不过无监督学习也有一些相对更弱的形式，例如分位数估算（quantile estimation），聚类（clustering），离群值检测（outlier detection）和降维（dimensionality reduction）。 接下来看一下监督学习。它的目标是从一个由成对数据(xi,yi)(x_{i},y_{i})(xi​,yi​)组成的数据集中学习一个从xxx到yyy的映射。其中yi∈Yy_{i}\\in\\mathcal{Y}yi​∈Y被称作数据xxx的标签或目标值。如果标签也是数字，就可以用y=(yi)i∈[n]Ty=(y_{i})^{T}_{i\\in[n]}y=(yi​)i∈[n]T​表示由标签组成的列向量。与无监督学习类似，一个基本的要求是数据(xi,yi(x_{i},y_{i}(xi​,yi​独立同分布取样于X×Y\\mathcal{X}\\times\\mathcal{Y}X×Y.由于得到的映射很容易通过其在测试集上的表现进行评估，所以监督学习的任务可以被很好的定义。当Y=R\\mathcal{Y}=\\mathbb{R}Y=R或Y=Rd\\mathcal{Y}=\\mathbb{R}^{d}Y=Rd时（也可以直接说当标签值的取值范围是连续的数字时），这个任务又被叫做回归。本书的大部分内容都是关于分类的，即Y\\mathcal{Y}Y的值来源于一个有限集，或者说标签是分离的。书中也有一些关于回归问题的探讨，在第23章。监督学习有两类算法：生成式算法（Generative algorithms）和判别式算法（Discriminative algorithm）。生成式算法试图用一些无监督学习过程构造条件密度模型p(x∣y)p(x|y)p(x∣y)（这里我们假定所有的分布都是有概率密度函数的，这样可以简化问题，我们将自己限定在处理概率密度的问题上），通过这个概率密度，再结合贝叶斯公式p(y∣x)=p(x∣y)p(y)∫Yp(x∣y)p(y)dyp(y|x)=\\frac{p(x|y)p(y)}{\\int_{\\mathcal{Y}}p(x|y)p(y)dy}p(y∣x)=∫Y​p(x∣y)p(y)dyp(x∣y)p(y)​就可以预测新的数据的标签值。事实上生成式算法中的p(x∣y)p(y)=p(x,y)p(x|y)p(y)=p(x,y)p(x∣y)p(y)=p(x,y)就是生成成对数据(xi,yi)(x_{i},y_{i})(xi​,yi​)的联合概率密度。而判别式算法则并不试图估计产生数据xix_{i}xi​的概率密度，它仅仅关注如何估计p(y∣x)p(y|x)p(y∣x)。一些判别式算法的目的甚至限制在仅仅关注p(y∣x)p(y|x)p(y∣x)是否大于0.5，例如支持向量机算法（SVM）。通常认为由于判别式算法与监督学习的目标之间的关联更加直接，在实际问题中判别式算法往往更加有效。这两种框架的更多细节会在2.2.1和2.2.2部分进一步讨论。 1.2 半监督学习 半监督学习（Semi-Supervised Learning），简记为SSL，是一种介于监督学习与无监督学习之间的机器学习类型。其特点是数据以无标签为主，再加上一部分的有标签信息的数据，通常所说的标签信息就是这部分数据的标签。此时，数据集x=(xi)i∈[n]x=(x_{i})_{i\\in[n]}x=(xi​)i∈[n]​可以被分成两部分，一部分是标签Yl:=(y1,…,yn)Y_{l}:=(y_{1},\\ldots,y_{n})Yl​:=(y1​,…,yn​)已知的点Xl:=(x1,…,xl)X_{l}:=(x_{1},\\ldots,x_{l})Xl​:=(x1​,…,xl​)，另一部分是标签未知的点Xu:=(xl+1,…,xl+u)X_{u}:=(x_{l+1},\\ldots,x_{l+u})Xu​:=(xl+1​,…,xl+u​).这种情况就是这本书所研究的“标准的”半监督学习的形式，书中大部分章节研究的问题都是这种形式。 其他形式的部分监督也是可能的。例如，我们已知的标签信息是某些点有（或者没有）相同的标签（参照Abu-Mostafa, 1995）.这种更广的设定会在第五章中被讨论。不同的设定将会导致对半监督学习产生不同的观点：在第五章中，SSL被看作在限制条件下的无监督学习。与之相反的是，大部分方法都把SSL看作是知道了额外的关于xxx的信息的有监督学习。后一种解释与SSL的大多数应用相一致，因为大部分时候SSL的目标与监督学习的目标相一致，都是预测所给数据xix_{i}xi​的目标值。然而，当类别的数量和本质不能提前知道，并且需要从数据中推断出这些的话，后一种观点就不适用了。此时，将SSL看作在限制条件下的无监督学习的观点则是适用的。 几十年前，Vapnik就提出了一个与SSL有关的问题，这就是所谓的转导学习（transductive learning）。在转导学习的设定下，实验者拥有一个有标签的训练集和一个无标签的测试集，其特点是预测的结果仅仅作用在测试集上。相对应的是归纳学习（inductive learning），其目的是得到一个定义在整个空间X\\mathcal{X}X上的预测函数，可以用这个函数预测整个空间上所有的点的标签值。这本书中的许多方法都是转导式的，尤其是基于图形类数据进行推断时，转导式方法更加自然，转导学习将在1.2.4部分被进一步解释。 1.3 半监督学习的历史简介 最早将无标签数据用于分类问题的方法很可能使自学习（self-learning），这种方法又被称为自训练（self-training）、自标签（self-labeling）或决策导向或学习（decision-directed learning）。这种方法从在有标签数据上训练开始，每一步都会用当前的决策函数（decision function）预测一部分无标签点的标签，然后把这部分被标注的点纳入训练集，重新应用有监督学习方法训练模型。这种方法很早就出现在一些文献中了（如：Scudder (1965); Fralick (1967); Agrawala (1970)）。 这种算法令人不满意的一个方面是包装（wrapper）效果依赖于所使用的监督学习方法。如果用实际风险最小化与1-0损失来做自学习，那原来的无标签数据加上后对新训练的模型没有任何影响。而如果用余量最大法（margin maximizing method）来做自学习，其结果是决策边界将被推离无标签点（参见第6章）。在其他情况下似乎还不清楚自学习方法真正的作用情况以及对应于哪种假设。 与 半监督学习关系密切的是转导推理（transductive inference或transduction），这种方法的先行者是Vapnik(Vapnik and Chervonenkis, 1974;Vapnik and Sterin, 1977).与归纳推理（inductive inference）相反，转导推理不追求推断出一般的决策规则，而是仅专注于预测无标签点（测试集），不做外推。一个早期的转导推理的例子在1968年就已经由Hartley和Rao提出来了，虽然当时还没有明确定义这个概念。他们建议对测试集的标签进行组合优化以最大化其模型的可能性。 半监督学习真正意义上的起飞似乎是从1970年代人们开始用无标签数据估计费舍尔线性判别式（Fisher linear discriminant）规则开始（Hosmer, 1973; McLachlan, 1977; O’Neill, 1978; McLachlan and Ganesalingam, 1982）.更确切地说，这种情况是指每一类的条件密度都是满足有着相同协方差矩阵的高斯条件的，这种模型可以通过诸如EM算法等迭代算法进行优化，同时使用有标签数据和无标签数据，求出数据标签的最大可能性。与混合高斯情况不同，Cooper和Freeman已经研究过了同时使用有标签数据和无标签数据估计多项分布的情况。 后来，每类一种成分的设定已经被推广到了每类几种成分（Shahshahani and Landgrebe, 1994），而后又被Miller和Uyar进一步推广（1997）。 在PAC（probably approximately correct）框架下，二元高斯混合情况下的学习率已经在1995由Ratsaby和Venkatesh得到。在混合情况可识别的情况下，Castelli和Cover证明，在无标签点的数量是有限个的情况下，错误率呈指数级收敛到贝叶斯风险。可识别的意思是说，在给定P(x)P(x)P(x)的情况下，其分解式ΣyP(y)P(x∣y)\\Sigma_{y}P(y)P(x|y)Σy​P(y)P(x∣y)是唯一的。这似乎是一个相对较强的假设，但在高斯混合条件下是满足的。Castelli和Cover在1996年发表的一篇论文中对此进行了分析，该论文中类别的条件密度是已知的而类别的先验密度是未知的。 之后，研究半监督学习的兴趣在1990年代增加了，其中大部分原因要归因于自然语言处理与文本分类问题的需要 (Yarowsky, 1995; Nigam et al., 1998; Blum and Mitchell, 1998; Collins and Singer, 1999; Joachims, 1999). 注意，就我们所知，Mert等人于1992年第一次使用术语“半监督学习”来描述同时使用有标签数据和无标签数据进行分类的问题。不过，事实上这种问题在之前就已经有研究了，但是研究的情况和本书中所要讲的有所不同，一个例子是(Board and Pitt, 1989). 2 半监督学习何时发挥作用 一个很自然的问题产生了：半监督学习有意义吗？更确切地说，与仅仅使用有标签数据进行学习的情况相比，考虑到无标签数据真的能够产生更加精确的预测吗？你可能已经从你手头这本书的厚度猜到了，答案是yes!不过有一个重要的先决条件：无标签数据与分类问题相关，它包含能够阐明数据分布的信息。 用更数学化的表达可以这么说：从无标签数据中获得的关于p(x)p(x)p(x)的信息中 必须包含有用于推断p(y∣x)p(y|x)p(y∣x)的信息。如果不满足这个先决条件，半监督学习无法产生比监督学习更好地结果，强行加入无标签数据进行推断甚至可能产生误导，降低预测的精确程度。这个效应的细节将在第4章深入研究。 只有在假设成立的情况下半监督学习才会有效，因此不必对半监督学习的作用过于惊讶。而且纯监督学习也必须满足某些假设才行。之后将会给出几种假设，事实上第22章会讨论一种在PAC风格的框架下形式化这些假设的方法，其中一种受欢迎的假设可以被形式化如下： 监督学习的平滑性假设（Smoothnss assumption of supervise learningSmoothnss\\ assumption\\ of\\ supervise\\ learningSmoothnss assumption of supervise learning）:如果两个点x1,x2x_{1},x_{2}x1​,x2​距离很近，那么其对应的输出y1,y2y_{1},y_{2}y1​,y2​也应当相应很近。 严格说来，这个假设仅仅指的是连续性而不是光滑性，不过smoothesssmoothesssmoothess这个术语却常常被使用在这里，或许是由于实际问题的回归中yyy常常被当作xxx的光滑函数建立模型。 说的更清楚一点，一旦不满足这个假设，就永远不可能讲一个在有限训练集上训练出来的模型一般化到一个测试集可能是无限情况下。 2.1 半监督学习平滑性假设 我们现在要提出一个对半监督学习有用的平滑性假设的一般化形式，我们称其为“半监督学习平滑性假设（semi-supervised smoothness assumption）”。在监督情况下，在我们先前的假设下，输出会随着距离而平滑变化，现在我们考虑加入输入的密度。这个假设是说预测函数在高密度区域将会比在低密度区域更加平滑： 半监督学习平滑性假设（semi−supervised smoothness assumptionsemi-supervised \\ smoothness\\ assumptionsemi−supervised smoothness assumption）：如果一个高密度区域中的两个点x1,x2x_{1},x_{2}x1​,x2​距离很近，那么其对应的输出y1,y2y_{1},y_{2}y1​,y2​也应当相应很近。 注意，根据传递性，这个假设隐含了一个信息：如果两个点可以通过一个高密度区域中的路径被连接，那么它们的输出值也可能很近。另一方面，如果它们被一个 低维区域隔开，那么它们的输出也就不必较近。 半监督学习平滑性假设同时适用于回归问题与分类问题。在下一部分，我们将展示在分类问题中将它简化为半监督问题常用的假设。目前尚不清楚该假设对于回归问题有多有用。作为替代方案，第23章提出了一种使用无标签数据进行模型选择的方法，该方法同时适用于回归和分类。 2.2 聚类假设 假设我们一直每一类别的点都倾向于聚成一团，那么无标签数据将会有助于精确化分类边界，做法是：可以运行一个聚类算法并用有标签数据给每个聚类指定类别（见第二章）。这个问题的基本的，现在也是经典的假设可以被描述如下： 聚类假设（Cluster assumptionCluster\\ assumptionCluster assumption）：在相同聚类中的点可能属于同一类。 这个假设在类别之间是陡峭的情况下可能是合理的，也就是说：如果分类点一个对象密集的连续体，那么就无法从中辨别出不同的类别。 聚类假设并不是说每一类数据形成一个单一的密集的区域，它仅仅意味着：在通常情况下，我们不会看到两中不同类别的点聚在同一类。 聚类假设可以被看作是上面提出的半监督学习平滑性假设的一个特例，因为一个类通常被定义为一个包含一堆互相之间可以由一条穿过高密度区域的短线连接起来的点组成的点集。 聚类假设的等价的规范化定义如下： 低密度分割（Low density separationLow\\ density\\ separationLow density separation）：决策边界应当落在低密度区域中。 很容易看出来二者的等价性：高密度区域中的决策边界将会把一个聚类切分成两类；同一聚类中的许多不同类别的点将需要决策边界来切割聚类，即穿过高密度区域。 尽管两种形式在概念上是等价的，但是却能产生出不同的算法，我们将在1.3节讨论相关问题。低密度版本还提供了直觉上的理解，为什么这种假设在许多实际问题中都是明智的。 例如，考虑数字识别，并假设人们想学习如何区分手写数字“ 0”与数字“ 1”。 准确地从决策边界获取的采样点将在0到1之间，最有可能是一个看起来像很长的零的数字。 但是有人写下这个“怪异”数字的可能性很小。 2.3 流形假设 流形假设是一个与前述假设不同但是相关，而且形成了几种半监督学习方法的基础的假设，定义如下： 流形假设（Manifold assumptionManifold\\ assumptionManifold assumption）：高维数据（大致）存在于低维流形上。 这个假设怎么发挥作用？一个广为认知的统计方法和学习算法的问题是所谓的维度诅咒（curse of dimensions）（参见11.6.2部分），维度诅咒基于一个事实：物体的体积随着其维度增加呈指数级增长。与之相应的是统计任务需要指数级增长的数据量来保证估计的可靠性。这个问题直接影响到基于输入空间的密度故居的生成式算法。对于判别式算法可能更严重的一个相关的问题是：随着维度的上升，成对的数据看起来更加相似，传递出更少的信息。 不过，如果数据恰好位于低维流形上，学习算法基本上就可以在相应的低维度空间上空间上操作，从而避免维度诅咒。 如上所述，可以认为使用流形的算法可能被视为近似实现了半监督平滑度假设：此类算法使用流形的度量来计算测地距离。 如果我们将流形视为高密度区域的近似值，那么很明显，在这种情况下，半监督平滑度假设降低为应用于流形的监督学习的标准平滑度假设。 请注意，如果流形以弯曲的方式嵌入到高维输入空间中（即，它不仅是子空间），则测地线距离将与输入空间中的测地线距离不同。 通过确保更准确的密度估计和更合适的距离，流形假设对于分类以及回归分析都可能有用。 2.4 转导 如前所述，某些算法本质上是在转导设定（transductive setting）下运行的。 根据Vapnik提出的哲学，高维估计问题应尝试遵循以下原则： VapnikVapnikVapnik 原则（Vapnik′s principleVapnik&#x27;s\\ principleVapnik′s principle）：当解决某个问题的时候，应避免将解决一个更加困难的问题作为中间的一个步骤。 以监督学习为例，其中需要对与某些点x相对应的标签y进行预测。生成式模型估计x的密度作为中间步骤，而判别式模型则直接估计标签。 同样地，如果仅对给定的测试集需要标签预测，则可以认为转导比归纳更直接：归纳方法在整个空间X\\mathcal{X}X上推导函数f:X→Yf:\\mathcal{X}\\rightarrow \\mathcal{Y}f:X→Y，然后在测试点上估计f(xi)f(x_{i})f(xi​)并返回，转导方法则直接估计有限的测试标签集，即仅在测试集上定义的函数f:Xu→Yf:\\mathcal{X}_{u}\\rightarrow \\mathcal{Y}f:Xu​→Y。 请注意，转导（如本书中所定义）并不等同于SSL：一些半监督算法是转导性的，而另一些是归纳性的。 现在假定给定了一种转导算法，该算法产生的解决方案优于对相同标记数据（丢弃未标记数据）训练的归纳算法，则此时的性能差异可能是由于以下两点之一（或其组合）引起的： 转导式方法比归纳式方法更加契合Vapnik原则； 转导式算法用一种类似于半监督学习算法的方式利用了无标签数据。 目前有充分的证据表明第二点对模型提升具有影响，不过尚不清楚有选择地支持第一点的经验结果。特别是，对本书相关基准的评估（第21章）似乎并未暗示转导方法的系统优势。 转导的性质仍然是争论的话题，第25章试图提出不同的意见。 3 算法类别与本书的架构 尽管许多方法不能明确地说来源于以上哪种假设，但大多数算法可以被看作对应或产生于上述假设之一或之几。我们尝试将本书中介绍的半监督学习方法组织为四个大致与基本假设相对应的类。 尽管分类并不总是唯一的，但我们希望这样的组织能够提供一个指导体系，使读者可以更轻松地阅读本书内容。出于同样的原因，本书按“部分”进行组织。每一类SSL算法都有一部分，此外还有一部分侧重于生成式方法。 另外两个部分专门介绍SSL的应用和前景。 在下文中，我们简要介绍每本书各部分所涵盖的内容。 3.1 生成式模型 第一部分展示了半监督学习中生成式模型的历史和前沿，第二章就从这个领域的概览开始谈起。用生成式模型估计条件密度p(x∣y)p(x|y)p(x∣y)， 使用生成模型进行推断涉及条件密度p(x∣y)p(x|y)p(x∣y)的估算。在这样的设定下，任何关于p(x)p(x)p(x)的额外信息都是有用的。一个简单的例子是假定p(x∣y)p(x|y)p(x∣y)是高斯分布的，这样就可以用EM找到每一种类别对应的高斯分布的参数的值。与聚类中使用的标准的标准的高斯分布的唯一不同之处在于与任何带标签的示例关联的“隐藏变量”实际上并未被隐藏，而是已知并且等于其类别标签。它实现了聚类假设（参见第2.2.1节），因为给定的聚类仅属于一个类。 这个小例子强调了了生成模型对半监督学习的不同解释： 在具有边缘密度的基础上添加了一些额外信息的分类问题； 具有额外信息的聚类问题。在标准设定下，这些额外信息就是一个数据子集的标签，但是它也可以采用更一般的约束形式。这是第5章的主题。 生成式方法的一个优点在于：关于问题或数据结构的知识可以通过建模自然地整合进模型中。在第3章中，将EM算法应用于文本数据的应用将展示这一优势。可以观察到，当建模假设不正确时，未标记的数据会降低预测准确性。在第4章中将对这种效果进行深入研究。在统计学习中，在进行推理之前，人们会选择一类函数或先验函数。必须根据事先对该问题的了解来选择它。在半监督学习环境中，如果能从数据的结构中推测到一些目标函数的信息，则在看到未标记的数据后可以更精确地选择先验函数 ：这样通常可以提高函数满足聚类假设的先验概率。从理论上讲，这是获取第22章所述的半监督学习边界的自然方法。 3.2 低维分割 第二部分聚焦于试图通过将决策边界推离无标签点而直接应用低维分割假设的算法。 最常用的达到这个目标的方法是用一个实现此目标的最常见方法是使用最大余量算法，例如支持向量机。最大化未标记点和标记点的边距的方法称为转导SVM（TSVM）。不过，相应的问题是非凸的，因此难以优化。 第6章介绍了一种TSVM的优化算法。从仅对标记数据进行训练的SVM解决方案开始，对未标记的点进行SVM预测标记，并对SVM进行所有点的重新训练。在未标记点的权重缓慢增加的同时进行迭代。另一种可能性是在第7章中建议的半定义SDP松弛编程。 然后，提出了TSVM的两种替代方法，分别用概率论和信息理论框架提出。在第8章中，通过引入空类来增加二进制高斯过程分类，该空类占用了两个常规类之间的空间。作为与TSVM相比的优势，这允许概率输出。 第9章介绍的熵最小化同样具有这一优势。它鼓励类条件概率P(x∣y)P(x|y)P(x∣y)在标记和未标记的点接近1或0。作为平滑度假设的结果，在任何高密度区域中，概率将趋于接近0或1，而类别边界对应于中间概率。 使用熵或信息的另一种方法是在第10章中开发的与数据相关的正则化（data-dependent regularization）。与TSVM相比，这似乎更直接地实现了低密度分离：标准平方范数正则化因子乘以一项反映接近决策边界的密度。 3.3 基于图的方法 在过去的两年中，半监督学习最活跃的领域是基于图的方法（graph-based methods），这是本书第三部分的主题。 这些方法的共同点是，数据由图的节点表示，图的边缘用入射节点的成对距离标记（而缺失的边缘则与无限距离相对应）。如果通过最小化连接两个点的所有路径上的总路径距离来计算两个点的距离，则这可以看作是两个点的测地线距离相对于数据点的流形的近似值。 因此，图方法可以看作是基于流形假设的。 大多数图方法通过利用图拉普拉斯算子（Laplacian）来引用图。令g=(V,E)g=(V,E)g=(V,E)是由w:E→Rw:E\\rightarrow \\mathbb{R}w:E→R给出实际边缘权重的图。这里，边缘e的权重w(e)w(e)w(e)表示入射节点的相似度（丢失的边缘对应于零）。现在，图g=(V,E)g=(V,E)g=(V,E)的加权邻接矩阵（weighted adjacency matrix）WWW定义如下： Wij:={w(e) if e=(i,j)∈E0 if e=(i,j)∈E\\mathbf{W}_{i j}:=\\left\\{\\begin{array}{ll} {w(e)} &amp; {\\text { if } e=(i, j) \\in E} \\\\ {0} &amp; {\\text { if } e=(i, j) \\in E} \\end{array}\\right. Wij​:={w(e)0​ if e=(i,j)∈E if e=(i,j)∈E​ 定义由Dii:=ΣjWijD_{ii}:=\\Sigma_{j}W_{ij}Dii​:=Σj​Wij​组成的诊断矩阵DDD被称作g的度矩阵（degree matrix），接下来就可以用不同的方法定义图拉普拉斯算子。两个最著名的图拉普拉斯算子分别是正规化拉普拉斯算子L\\mathcal{L}L和非正规化拉普拉斯算子LLL： L:=I−D−1/2WD−1/2L:=D−W\\begin{array}{l} {\\mathcal{L}:=\\mathbf{I}-\\mathbf{D}^{-1 / 2} \\mathbf{W D}^{-1 / 2}} \\\\ {L:=\\mathbf{D}-\\mathbf{W}} \\end{array} L:=I−D−1/2WD−1/2L:=D−W​ 许多惩罚加权图边缘的非光滑度的图方法可以看作是一个较为通用的算法系列的不同实例，这将在第11章进行阐述。第13章采用了更理论的观点，并将从连续情况的图形的光滑度转移到离散情况。由此，基于数据的图形表示，提出了不同的正则化器。通常，预测由未标记节点的标记组成。因此，这种算法本质上是转导的，即，它仅返回未标记点上决策函数的值，而不返回决策函数本身。但是，最近进行了一些工作，以扩展基于图的方法以产生归纳解，如第12章所述。 在图上的信息传播还可以考虑到未标记的数据，从而改善给定（可能受到严格监督）的分类。第14章介绍了以这种方式使用有向图的概率方法。 通常，图g是通过以某种其他表示形式计算对象的相似性来构造的，例如，使用欧几里得数据点上的核函数。但是有时原始数据已经具有图形形式。示例包括网页的链接模式和蛋白质的相互作用（请参见第20章）。在这种情况下，边缘的方向性可能很重要。 3.4 表示形式的改变 第四部分的主题是本质上不是半监督的算法，这类算法表现为两步学习： 在所有有标签和无标签数据上应用监督学习算法，但是不考虑已有的标签。例如，这可以是表示形式的更改，也可以是新metric或新kernel的构造。 忽略未标记的数据，并使用新的距离、表示形式或kernel执行纯监督学习。 这可以看作是半监督平滑度假设的直接实现，因为以一种方式更改了表示形式，从而可以节省高密度区域中的小距离。 请注意，基于图的方法（第3部分）与本部分中介绍的方法密切相关：根据数据构建图的过程可以看作是表示形式的无监督更改。因此，第四部分的第15章讨论了这种图形的频谱变换（spectral transforms），以构建kernel.频谱方法（spectral methods）也可以用于非线性降维，如第16章所述。此外，在第17章中，研究了从图形派生的度量，例如，从最短路径派生的度量。 3.5 半监督学习的实际应用 每当未标记数据比标记数据多得多时，半监督学习将是最有用的。如果获取数据点很便宜，但是获取标签会花费大量时间，精力或金钱，则很可能会发生这种情况。在机器学习的许多应用领域中就是这种情况，例如： 在语音识别中，录制大量语音几乎不需要花什么钱，但是标记它需要一些人工来听并键入笔录。 数十亿个网页可直接用于自动化处理，但是要可靠地对其进行分类，人们必须逐一浏览它们。 如今，蛋白质序列是以工业速度获得的（通过基因组测序，计算基因发现和自动翻译），但是要解析三维（3D）结构或确定单个蛋白质的功能可能需要多年的科学工作。 第3章在生成模型的背景下介绍了网页分类。 由于未标记的数据携带的信息少于标记的数据，因此需要大量使用它们才能显着提高预测准确性。这意味着需要快速有效的SSL算法。 第18章和第19章介绍了处理具有大量数据的问题的两种方法。在第18章中，开发了用于加快在第11章中介绍的标签传播方法的方法。在第19章中，显示了cluster kernel是一种有效的SSL方法。 第19章还介绍了半监督学习在重要的生物信息学应用中的两种方法中的第一种：蛋白质序列的分类。尽管这里的预测是基于蛋白质序列本身的，但第20章将进行一些更为复杂的设置：这里的信息假定以表征蛋白质相互作用的图表形式出现。存在几个这样的图，必须以适当的方式进行组合。 本书的结尾部分有一个非常实用的章节：与本书相关的基准的介绍和评估（第21章）。旨在给从业者一些提示，说明如何根据问题的性质选择合适的方法。 3.6 概览 本书的最后一部分，第六部分，致力于解释SSL中一些目前正在被研究的最有趣的方向。 到现在为止，这本书基本上只限于分类。 第23章介绍了另一种适用于分类和回归的SSL方法，并从中推导了算法。有趣的是，这似乎不需要第1章中提出的假设。 此外，本书主要介绍了SSL算法。 尽管上面讨论的假设提供了一些有关SSL何时以及为什么起作用的直觉，并且第4章研究了何时以及为什么SSL可能失败，但是对SSL有了透彻的理论理解显然会更令人满意。为此第22章提供了一个PAC框架，该框架产生SSL问题的错误边界。 在第24章中，依据VC bound以及其他理论和哲学概念对归纳式半监督学习和转导式学习进行了比较。 本书以三位机器学习研究人员之间关于半监督学习与转导学习之间的关系（以及两者之间的差异）的假设讨论（第25章）作为结尾。","permalink":"http://yangtf983.github.io/2020/01/09/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%EF%BC%9A%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D/","photos":[]},{"tags":[{"name":"Reinforcement Learning","slug":"Reinforcement-Learning","permalink":"http://yangtf983.github.io/tags/Reinforcement-Learning/"}],"title":"Imitation Learning & Dagger Algorithm","date":"2020/01/02","text":"[0] 说明 进入Reinforcement Learning的世界，一般而言应当是从tabular method学起，但是从imitation learning学起推广到Reinforcrmrnt Learning可以帮助我们更好的理解RL的解决问题的思路和发展目的，因此我们本次继续跟随cs294的脚步，学习Imitation learning的相关知识以及相对简单的Dagger算法。 值得一提的是，IL应当归类于监督学习(Supervised Learning)，但是其可以应用于IRL(Inverse Reinforcement Learning)领域，这也是IL与IRL的重要联系之一。 [1] Terminology &amp; Notation 首先我们应当明白我们探讨的是什么类型的问题，我们称之为时序决策问题(sequrntial decision)，这类问题的主要特点是具有马尔科夫性(markov property)，也即是说，在当前条件已知的情况下，未来和过去是独立的。换句话说，如果我掌握了现在的情况，那么从纯理性的角度讲，是否掌握过去的情况对我做出下一步决策毫无影响。其数学定义参照下式： P(X(tn)≤xn∣X(tn−1)=xn−1,…,X(t1)=x1)=P(X(tn)≤xn∣X(tn−1)=xn−1)P( X(t_{n} )\\leq x_{n} | X(t_{n-1})=x_{n-1}, \\ldots,X( t_{1} ) = x_{1} ) = P( X(t_{n} )\\leq x_{n} | X(t_{n-1})=x_{n-1}) P(X(tn​)≤xn​∣X(tn−1​)=xn−1​,…,X(t1​)=x1​)=P(X(tn​)≤xn​∣X(tn−1​)=xn−1​) 其中，t1&lt;t2&lt;…&lt;tnt_{1}&lt;t_{2}&lt;\\ldots &lt;t_{n}t1​&lt;t2​&lt;…&lt;tn​. 对于这样一个决策问题，系统首先需要一个输入，也即是系统的观测值，记为oto_{t}ot​。众所周知，神经网络会对oto_{t}ot​进行一系列处理，得到一个可能采取的行为的分布函数（对于有限行为决策，即为分布列），我们称这个结果称为我们的决策策略。我们将行为记作ata_{t}at​，策略记作πθ(at∣ot)\\pi_{\\theta}(a_{t}|o_{t})πθ​(at​∣ot​). 上面都很好理解，但是我们这里还要定义另外一个符号，这个符号叫做状态，记为sts_{t}st​. 定义sts_{t}st​是因为oto_{t}ot​并不总是无损的。举例来说，假如我有一张图片，图片上是一辆行驶的汽车，我们将这张图片输入给一个神经网络系统，oto_{t}ot​就是这张照片的像素，是一个或几个二维数组，我们可以通过神经网络对这张图片进行分析，得到这张图片上各种信息，但是我们永远不能判断车后面有没有一只动物，有可能这辆车刚刚好挡住了一只动物，但是这只动物不在照片里，或者说不在oto_{t}ot​里，但是我们说它在sts_{t}st​里，sts_{t}st​包含此刻决策相关的所有信息，但是oto_{t}ot​不一定。 我们再通过下图来理解这个决策系统： 我们将ot=sto_{t}=s_{t}ot​=st​的系统称为完全观察(fully observed)系统，将ot≠sto_{t} \\neq s{t}ot​​=st的系统称为部分观察(partial observed)系统。显然，对于一个partial observed问题，若我们只有oto_{t}ot​而没有sts_{t}st​，ot−1o_{t-1}ot−1​对我们做出下一步行动是有用的，因为它可能帮助我们发现车后究竟有没有动物。此时，我们需要一个决策系统可以不仅仅考虑到oto_{t}ot​，还可以考虑到ot−1o_{t-1}ot−1​甚至于更早的观测值。 [2] Introduce to Behavior Cloning 我们将模仿专家行为进行学习的一类算法又称为behavior cloning，这类算法以专家行为作为数据标签，通过对数据集进行监督学习得到模型πθ(at∣ot)\\pi_{\\theta}( a_{t}|o_{t} )πθ​(at​∣ot​)，以此进行决策。 我们以自动驾驶为例，behavior cloning的一个可能做法是找一些驾驶熟练的人类司机，让他们驾车行驶，并且通过摄像头拍摄沿路情况，并记录下司机的行为(左转或者右转)作为数据标签，接着用这些有标签的数据进行监督学习，因此本质上IL/behavior cloning是监督学习的分支，下图是我们所举例子的一个直观表示： [3] DAgger Algorithm DAgger算法的诞生是为了解决behavior cloning中一个很严重的问题，因此我们先来看看这个问题。 前面已经讲过，我们在使用behavior cloning时采用了监督学习算法。我们都知道，无论数据量有多大，采用的监督学习算法有多准确，都可能存在一种情况，就是在行驶过程中某一步和原来的专家行为产生偏差，因为我们不是完全的模仿专家行为，而是采用了一个监督学习模型(如果完全模仿专家行为，则模型的利用场景过于有限，一旦碰到任何没有碰到的情况，汽车就会手足无措，而在行驶过程中，可能发生的各种情况是难以模拟遍的)。一旦这个偏差产生，由于它不是在数据集中的，做出的进一步决策很可能使其朝着进一步偏差的方向进行，而随着汽车碰到的状况与数据集的偏差越来越大，其做出的决策则越来越不具有合理性，偏离原数据集的速度会越来越快，放在自动驾驶上，就是偏离车道的速度加快。这一现象在很多实验中得到了验证，下图是一张验证这一现象的模拟图： 针对之前提到的自动驾驶，NVIDIV提出了一种解决方案，就是同时拍摄左中右三个方向的情况，给左侧的数据打上标签“右”，右侧的数据打上标签“左”,这样当检测到车头方向偏离时，及时调整回去。经过实际检验，这种方案的效果很不错。 这个问题看似已经解决了，但是我们还是要思考一下，有没有更加通用的方法，使得我们不用重新获取这些数据，并且可以用于一些其他领域的behavior cloning问题？ 还是用自动驾驶举例，想像一下，如果我们能够得到整个轨迹的分布，也就是所有的汽车可能碰到的情况，然后让专家给出所有的label，这样无论汽车遇到什么情况，都可以精确按照专家给出的建议行驶。但是这种假设的问题在于，我们无法得到一个有无数种可能的情况的分布，即便是我们找出了所有情况，也无法承担让专家给所有情况打分的高额花费。 那我们能不能找到一种方法，来只对个数不多的有限种情况进行打分，并尽量让汽车在这些情况能够给出有效指导的范围内行动呢？ 我们将原始数据集记为data，我们知道，我们的模型能够应对的情况是Pdata(ot)P_{data}(o_{t})Pdata​(ot​)，而汽车在行驶过程中真正面对的情况是Pπθ(ot)P_{\\pi_{\\theta}}(o_{t})Pπθ​​(ot​) ，当data ≠πθ\\neq \\pi_{\\theta}​=πθ​ 时，自然有 Pdata(ot)≠Pπθ(ot)P_{data}(o_{t})\\neq P_{\\pi_{\\theta}}(o_{t})Pdata​(ot​)​=Pπθ​​(ot​)，因此产生了偏离。如果我们能找到一种方法使得Pdata(ot)≠Pπθ(ot)P_{data}(o_{t})\\neq P_{\\pi_{\\theta}}(o_{t})Pdata​(ot​)​=Pπθ​​(ot​)，便能解决这个问题。因此我们的一个思路是尽可能让data=πθdata=\\pi_{\\theta}data=πθ​. 依照这种思路，我们找到了DAgger(Dataset Aggregation)算法。DAgger是一种很简单的方法，并且已经被证明在在线学习的情况下这种算法是收敛的。下面介绍一下DAgger算法的流程： train πθ(at∣ot)\\pi_{\\theta}(a_{t}|o_{t})πθ​(at​∣ot​) from human data D= {o1,a1,…,oN,aN}\\{o_{1},a_{1},\\ldots,o_{N},a_{N}\\}{o1​,a1​,…,oN​,aN​}. run πθ(at∣ot)\\pi_{\\theta}(a_{t}|o_{t})πθ​(at​∣ot​) to get dataset Dπ={o1,…,oM}D_{\\pi}=\\{o_{1},\\ldots,o_{M}\\}Dπ​={o1​,…,oM​}. Ask human to label DπD_{\\pi}Dπ​ with actions ata_{t}at​. Aggregate: D→D⋃Dπ.D\\rightarrow D\\bigcup D_{\\pi}.D→D⋃Dπ​. repeat step1 ~ step4. 这里有一个问题，是cs294中的一个学生提出来的：为什么step4要将D和DπD_{\\pi}Dπ​聚合，而不是用DπD_{\\pi}Dπ​代替D？ 视频中老师给出了两个原因：(1)这样做效果不好;(2)DAgger算法收敛的基础是在线学习。 但是视频中没有给出更详细的解释，我理解了一下，可以大概给出一种解释方法： 首先，behavior cloning使用的是监督学习方法，DAggger算法的目的是尽可能让车的行驶路径在我们的模型的无偏差的计算范围内，即便是偏差了也要尽可能是我们考虑过的偏差情况。那么如果我们不聚合D和DπD_{\\pi}Dπ​，我们也能仅仅用DπD_{\\pi}Dπ​去训练一个新模型，因为这样同样是部分的，与用D训练一个新模型并没有本质差别，因此如果我们每一步仅仅用新的DπD_{\\pi}Dπ​训练模型，最后必然要将所有循环中得到的数据进行一次聚合再进行训练得到使用的模型，这样训练的模型收敛的可能显然不如每一次循环将所有已有的数据聚合起来进行训练的收敛的可能大（直观理解），而后者已经被证明是收敛的。 其次，我们也可以考虑在不聚合数据的情况下每次不去重新训练模型，而是在已有模型的基础上训练模型，相当于迁移学习。但是这种方法效果无法超过每次都聚合的方法，这是很多实验的结果。当然这种方法比较节省时间，所以当时间不充足或者计算资源较少的情况下可以使用这种方法，但是得到的效果不会太好。 理解完了DAgger算法，下面自然就要理解一下这个算法的缺陷。 事实上，除去一些所有监督学习方法的共同缺陷外，这个算法的缺陷并不多，其中值得我们认证考虑的缺陷只有一个，就是我们如何划算的给所有数据打标签？ 将DAgger算法迭代越多，则得到的数据量越大，得到的模型效果越好，但是面对增加的数据量，打标签的花费也在上升，有些时候打标签花费比较廉价，但有时可以很昂贵，尤其是面对巨大的数据量，一般最后都不会太廉价。我们还没有考虑其他方面带来的花费，比如采集数据。此外，打标签有时也不是一件简单的事情。仍然以自动驾驶为例，让司机通过看录像打标签，很可能比直接开车做出正确选择的概率会小一点，虽然我们无法直接得到这个概率差别是多少。 谈到这里，我们自然会诞生一个疑问，就是我们怎么克服数据量这个缺陷？显然数据量的需求来源于模型的要求，要克服这个缺陷，我们就要问一个问题，就是能不能找到一种不需要大量数据的模型来完成IL这件事？ 针对这个问题，我们可以提出一种思路，但是真正的解决还是要用到RL模型。下面是这种思路的想法： DAgger addresses the problem of distributional “drift” What if our model is so good that it doesn’t drift? Need to mimic expert behavior very accurately. But don’t overfit! [4] Why might we fail to fit the expert? 下面我们抛开具体算法来讨论一个问题：为什么我们拟合专家行为可能会失败？ 其中一个原因是专家行为可能是非马尔科夫行为(Non-Markovian behavior)。我们之前已经提到过，sequential decision的基本假设是markov property，这一特性在我们的模型中体现为我们使用的监督学习模型是无记忆性的，事实上，大多数监督学习方法都是无记忆性的。但是现实生活中，习惯、心情等事物都可能使一个人在某时刻更加偏爱某种决策，甚至于这种影响有时候其本人都无法察觉。如当我选择一条上班的路线时，我一般会选择最常走的那条，但是某天心情好，我就很想走一条之前未曾走过的路线。这两种选择都不是markovian behavior， 因为正确判断我要选择的路线不仅仅需要知道我当时的状态（心情），还需要知道我之前哪些路走得多，哪些路走得少。 IL面对的另一个重要问题是多方式行为(Multimodal behavior)。意思是说，有些情况下，我有多种方法达到同种效果，但是不能综合这些方法去达到这种效果，只能选择其一。例如，当我想绕过面前的一棵树时，我可以从左边绕过，也可以从右边绕过，但是我不能综合两种方法从中间绕过。 下面我们针对两种问题分别给出一些对应的解决思路，由于笔者能力有限，暂时对这些问题不能给出更深入的理解，其中大部分是对cs294课程的重述，有感兴趣的读者可以进一步研究，同时，笔者会在对此部分有进一步理解时在博文中更新本部分。 [a] Non-Markovian behavior 前面说过，这个问题导致的结果就是：我们在做出下一步决策时，不仅仅要考虑到现在的状况，还要考虑到之前的状况。也就是说，我们要从计算 πθ(at∣ot)\\pi_{\\theta}(a_{t}|o_{t})πθ​(at​∣ot​) 转变为计算 πθ(at∣o1,…,ot).\\pi_{\\theta}(a_{t}|o_{1},\\ldots,o_{t}).πθ​(at​∣o1​,…,ot​). 要达到这种效果也很简单，就是采用递归神经网络，这类神经网络已经有了很大发展，相信大家并不陌生，比如iphone的语音助手siri，它可以联系用户的前几句话来理解用户的意图，其根本原因就是采用了递归神经网络。 [b] Muitimodal behavior [I] 离散模型 对于离散模型的multimodal behavior行为，我们要做的是从一些有限的方案中选出一种方案，只需要在神经网络最后加上一层softmax层。 [II]连续结构 对于能够采取连续行为的问题，模型最后的输出应当能够对应连续行为的决策，这类问题一般采用高斯分布实现（有时也用均方误差，等价于使用高斯分布，因为均方误差即高斯分布的对数概率），实现方式一般有三种，各有优劣： [i] Output mixture of Gaussians π(a∣o)=ΣiωiN(μi,Σi)\\pi(a|o)=\\Sigma_{i}\\omega_{i}N(\\mu_{i},\\Sigma_{i})π(a∣o)=Σi​ωi​N(μi​,Σi​) 特点：对低维决策效果较好。 [ii] Latent varible models step1. 不改变输出结构，仍以单高斯分布模型的简单形式存在； step2. 在神经网络底部输入额外的随机数（分布不唯一）。 难点：如何让神经网络有效利用噪声。 [iii] Autoregressive discretization step1. 从一个决策维度开始，一个神经网络增加一个决策维度； step2. 每个网络都结合前一个网络的输出和新的条件得到新的输出，并且决策的维度增加一。 特点：简单，但对网络结构改变较大，需要重新设计。 [5] Other topics in imitation learning Structured prediction 这一领域对输出的结构往往有一定要求，应用比较广泛的如机器翻译领域等。 Inverse reinforcement learning 通过模仿，反向理解行为的目的，之后寻找更好的方法来达到该目的。 [6] Imitation learning: What’s the problem? Data is typical finite; Humans are not good at providing some kinds of actions; Humans can learn autonomously; can our mechines do the same? 参考资料 cs294-112, lec-2","permalink":"http://yangtf983.github.io/2020/01/02/Imitation_Learing&Dagger_Algorithm/","photos":[]},{"tags":[{"name":"Reinforcement Learning","slug":"Reinforcement-Learning","permalink":"http://yangtf983.github.io/tags/Reinforcement-Learning/"}],"title":"A simple introduce of reinforcement learning","date":"2019/01/30","text":"[0] 说明 这是本博客RL系列的第一篇文章，旨在对RL进行一个简单的介绍，不涉及高深的理论。作为一个RL初学者，本博客的一系列RL文章都将是我在学习过相应的课程后写的，我会在学习已有资料的基础上加上自己的理解，但是一般不会注明哪些是自己的理解，哪些是材料中的观点。当然我的理解难免会有差错，因此我会在每一篇博文后注明我的学习材料，有需要的读者可以自行寻找阅读，若发现我的差错，欢迎批评指正。 [1] How do we build intelligent machines? 首先我们尝试理解一个大问题，就是：How do we build intelligent machines?理解这个问题是为了解决本节一个更为核心的问题：What is reinforcement learning, and why should we care? 机器是能够根据设定的指令行事的一种无生命的形式，比如手表等物品，但是我们不说手表是智能机器。我们在智能手机名称前加了个智能，但是我们仍然不认为它是智能机器。甚至更复杂一点的，如飞机、火箭、宇宙飞船，集结了人类无数智慧结晶的东西，我们仍然不说它们是智能机器。但是，我们会说microsoft的情感机器人是智能机器、击败人类围棋冠军的alphago是智能机器，哪怕它们目前技术含量并不一定比得上火箭之类的“非智能机器”，但是它们能够对人类的行为做出反应，并且这些反应并不是人类提前用判断语句判断好写入的，而是它们自己学习并适应的，只要给他们足够的时间，他们可以适应越来越多的情况。所以我们可以说：Intelligent machines must be able to adapt. 要想建立这种智能机器，我们就必须要有一种能够让它们学习适应各种环境的方法，这种方法几十年前就有人提出了，但是受限于算力和数据的问题，没有得到很好的发展，直到近年这些问题被解决后才又一次受到大众的观众，这种方法就是**“deep learning”(简称DL)** DL是一种可以处理非结构化环境(unstructures environment)的方法，你也可以说它是一种黑箱算法，它改变了我们传统的自己定义特征的方法，转而由数据进行训练。在看不出过拟合现象（训练集效果很好，预测集效果很差）的前提下，你可以尽可能地增加层数，即便你也不知道每一层分别得到了什么特征。这种方法简化了解决问题的难度，同时具有很高的通用性（不是说迁移能力强），前提是你要有足够的数据。 但是在reinforcement learning(简称RL)诞生并与DL结合前，DL的应用仅仅是在处理感知信息上，比如对数据进行分类，不管数据是有标签的还是无标签的都可以做到，我们分别称之为supervised learning和unsupervised learning，直到RL诞生，才提供了一种让机器学习行为的方式，后来将RL于DL结合，等于是将“是什么”(DL)和“怎么做”(RL)结合到了一起，诞生了一种说法叫做deep reinforcement learning(简称DRL)，人类得以开始建造intelligient mechines.RL领域现在备受关注的主要原因也是由于其与DL方法的结合产生了很多新的成果，甚至于我们可以说RL取得的最大成功就是与神经网络和深度网络的结合。在后续系列博客中，我们将不再区分RL与DRL，一般均用RL代替。 Two ways to build intelligent learning 标准方法：解析→分块生成→组合 学习方法：建立学习算法→自动学习功能 [2] What is deep RL, and why should we care? 在上一部分中我们已经介绍了什么是DL以及什么是DRL，并且向大家简单说明了在使用DL之前的方法（我们称之为“标准方法”）是怎样的。简而言之，标准方法中，特征是人为定义和提取的，机器的行为策略也是人为定义的。下面我们将总结一下引入RL后的改变，帮助大家理解Why should we care about RL? 计算机视觉领域 标准方法：人为提取每一层特征，很复杂，甚至可以作为一个人整个博士期间的研究工作； DL：end-to-end training (端到端) 优点： 减少人工量；2. 找到的方法往往比用标准方法更好。 游戏领域 标准方法：人为建立许多特征与策略； DL：end-to-end training (端到端) 优点： 减少人工量；2. 找到的方法往往比用标准方法更好。 [3] What does end-to-end learning mean for sequential decision making? 我们先来解释一下什么是end-to-end learning. 这个问题的解释其实和你如何定义一个完整的过程有关，假如我们现在定义一个人的一个完整的反射过程是从他接收到环境信息到他对环境做出反应这一整个过程，那么一个end-to-end learning的意思就是我的学习系统只需要这两个端口的信息（也就是环境信息和人做出的反应）就可以进行学习，而不必对中间的过程再进行拆解。 举个例子，假如你现在在野外，你接受到的环境信息是看到了一只凶猛的野生老虎，一个end-to-end learning系统会直接告诉你快点逃跑（当然如果你有武器你可以选择一搏，我们这里只用一般情况做例子），如果你想问这个系统为什么发出这样的指令，它会告诉你不跑你很可能会死掉，但不会告诉你任何判断的中间分析过程。而如果是其他方法，可能会将这个反射过程分解，分解为信息处理系统与决策系统，在信息处理系统中，它专注于得到信息的精准描述，如判断是不是老虎，如果是，再判断是不是动物园的老虎，判断发现这是一只野生老虎，再判断你是否有武器，发现你没有，这个系统的工作就算结束了，接着把这些信息发送给决策系统，决策系统受到信息：你正面对一只野生老虎，没有武器。接着，给你下达了逃跑的指令。想一想如果你问这个系统你为什么需要逃跑它会怎么回答你？它很可能把信息处理系统的结果告诉你，然后说根据这个信息，判断结果你需要逃跑。这就是二者的区别。 再考虑一下它们的实现机理，发现什么不同了吗？在后面的决策过程中，这个系统根本不需要知道如果你不逃跑会有什么后果，我们只需要把指令写进去，它进行简单的逻辑判断就可以发出决策，但是在前者中，你必须知道哪一种行为会得到什么后果，并且知道你需要什么样的后果，当然这里的后果可以通过不断的尝试得到，因为我们可以通过计算机模拟，不像前面举的例子，计算机模拟中尝试后出现不满意后果是完全可以接受的。 实际上我们完全可以这样总结，end-to-end learning在sequential decision中最大的特点就是需要知道不同的结果是好是坏。 Conclusion1: Deep mdoels are what allow reinforcement learning algorithms to solve complex problems end-to-end. [4] 从机器学习到RL 前面的部分中我们已经讲了很多对RL的理解，下面我们引用RL领域一本经典书籍中对RL的一个说明作为其定义，为后面的叙述做铺垫。这本书是:Reinforcement learning: An introduce，关于该定义的详细解释可以自己从书中去找。 Beyond the agent and the environment, one can identify four main subelements of a reinforcement learning system: a policy, a reward signal, a value function, and, optionally, a model of the environment. 结合该定义与之前的例子，我们很容易知道，实现一个RL方法，我们至少需要三个信息：action(行为),observation(观测值),reward(奖励)。下面我们通过重构分类问题以及NPL问题来说明一种观点：RL问题是大多数其他机器学习的一种更一般的表现形式。 分类问题→RL action:输出的标签 observation:图像的像素 reward:分类正确率 NPL(以翻译为例)→RL action:翻译 observation:原语言 reward:翻译质量 [5] Why should we study this now? 这里，我们不加解释的给出三条理由，相信大家能够理解： Advances in deep learning Advances in reinforcement learning Advances in computational capability 一些成功的例子可能更能够让你感受到这个领域目前的发展前景： 用Q-learning学习玩游戏 用policy training控制机器人 alphago, alphastar [6] What other problems do we need to solve to enable real-world sequential decision making? Beyond learning from reward 基础RL方法处理最大化奖励问题，但是sequential decision还涉及其他方面，这对RL方法提出了新的要求。 Where do rewards come from 现实世界中的奖励函数很难得到，甚至在某些事情上，感知做完了和完成这件事一样困难。 Are there other forms of supervision Learning from demonstrations（模仿、推断） Learning from observing the world Prediction [7] Why deep RL? deep = can process complex sensory input and also compute really complex functions RL = can choose complex actions [8] What can deep learning &amp; RL do well now? 有明确的已知简单规则的事物，如围棋； 有原始感觉的输入以及足够经验的事物，如机器人； 通过专家行为的模仿学习. [9] What has proven challenging so far? 学习速度：DRL学习速度比人类慢很多； 迁移能力弱； 难以找到合适的奖励函数，奖励函数对于学习行为与学习速度都至关重要； 应该大力发展基于模型的or无模型的RL. 参考资料 cs294-112, lec-1 Reinforcement Learning: An Introduction","permalink":"http://yangtf983.github.io/2019/01/30/A_simple_introduce_of_RL/","photos":[]}]}