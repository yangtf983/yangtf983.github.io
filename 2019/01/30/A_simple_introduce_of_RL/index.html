<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  
  
    <meta name="keywords" content="12">
  
  
    <meta name="description" content="keep foolish, keep hungry">
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <title>
    A simple introduce of reinforcement learning |
    
    Young&#39;s Blog</title>
  
    <link rel="shortcut icon" href="/favicon.ico">
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<script src="/js/pace.min.js"></script>

  <link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css" rel="stylesheet" type="text/css">
<meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="Young's Blog" type="application/atom+xml">
</head>

<body>
<main class="content">
  <section class="outer">
  

<article id="post-A_simple_introduce_of_RL" class="article article-type-post" itemscope itemprop="blogPost" data-scroll-reveal>
  
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      A simple introduce of reinforcement learning
    </h1>
  
  




      </header>
    

    
      <div class="article-meta">
        <a href="/2019/01/30/A_simple_introduce_of_RL/" class="article-date">
  <time datetime="2019-01-29T16:28:02.000Z" itemprop="datePublished">2019-01-30</time>
</a>
        
      </div>
    

    
      
    <div class="tocbot"></div>





    

    <div class="article-entry" itemprop="articleBody">
      


      

      
        <h2 id="0-说明"><a class="markdownIt-Anchor" href="#0-说明"></a> [0] 说明</h2>
<p>这是本博客RL系列的第一篇文章，旨在对RL进行一个简单的介绍，不涉及高深的理论。作为一个RL初学者，本博客的一系列RL文章都将是我在学习过相应的课程后写的，我会在学习已有资料的基础上加上自己的理解，但是一般不会注明哪些是自己的理解，哪些是材料中的观点。当然我的理解难免会有差错，因此我会在每一篇博文后注明我的学习材料，有需要的读者可以自行寻找阅读，若发现我的差错，欢迎批评指正。</p>
<a id="more"></a>
<h2 id="1-how-do-we-build-intelligent-machines"><a class="markdownIt-Anchor" href="#1-how-do-we-build-intelligent-machines"></a> [1] How do we build intelligent machines?</h2>
<p>首先我们尝试理解一个大问题，就是：How do we build intelligent machines?理解这个问题是为了解决本节一个更为核心的问题：<strong>What is reinforcement learning, and why should we care?</strong></p>
<p>机器是能够根据设定的指令行事的一种无生命的形式，比如手表等物品，但是我们不说手表是智能机器。我们在智能手机名称前加了个智能，但是我们仍然不认为它是智能机器。甚至更复杂一点的，如飞机、火箭、宇宙飞船，集结了人类无数智慧结晶的东西，我们仍然不说它们是智能机器。但是，我们会说microsoft的情感机器人是智能机器、击败人类围棋冠军的alphago是智能机器，哪怕它们目前技术含量并不一定比得上火箭之类的“非智能机器”，但是它们能够对人类的行为做出反应，并且这些反应并不是人类提前用判断语句判断好写入的，而是它们自己学习并适应的，只要给他们足够的时间，他们可以适应越来越多的情况。所以我们可以说：<strong>Intelligent machines must be able to adapt.</strong></p>
<p>要想建立这种智能机器，我们就必须要有一种能够让它们学习适应各种环境的方法，这种方法几十年前就有人提出了，但是受限于算力和数据的问题，没有得到很好的发展，直到近年这些问题被解决后才又一次受到大众的观众，这种方法就是**“deep learning”(简称DL)**</p>
<p>DL是一种可以处理非结构化环境(unstructures environment)的方法，你也可以说它是一种黑箱算法，它改变了我们传统的自己定义特征的方法，转而由数据进行训练。在看不出过拟合现象（训练集效果很好，预测集效果很差）的前提下，你可以尽可能地增加层数，即便你也不知道每一层分别得到了什么特征。这种方法简化了解决问题的难度，同时具有很高的通用性（不是说迁移能力强），前提是你要有足够的数据。</p>
<p>但是在<strong>reinforcement learning(简称RL)<strong>诞生并与DL结合前，DL的应用仅仅是在处理感知信息上，比如对数据进行分类，不管数据是有标签的还是无标签的都可以做到，我们分别称之为supervised learning和unsupervised learning，直到RL诞生，才提供了一种让机器学习行为的方式，后来将RL于DL结合，等于是将“是什么”(DL)和“怎么做”(RL)结合到了一起，诞生了一种说法叫做</strong>deep reinforcement learning(简称DRL)</strong>，人类得以开始建造intelligient mechines.RL领域现在备受关注的主要原因也是由于其与DL方法的结合产生了很多新的成果，甚至于我们可以说RL取得的最大成功就是与神经网络和深度网络的结合。在后续系列博客中，我们将不再区分RL与DRL，一般均用RL代替。</p>
<h4 id="two-ways-to-build-intelligent-learning"><a class="markdownIt-Anchor" href="#two-ways-to-build-intelligent-learning"></a> Two ways to build intelligent learning</h4>
<ol>
<li>标准方法：解析→分块生成→组合</li>
<li>学习方法：建立学习算法→自动学习功能</li>
</ol>
<h2 id="2-what-is-deep-rl-and-why-should-we-care"><a class="markdownIt-Anchor" href="#2-what-is-deep-rl-and-why-should-we-care"></a> [2] What is deep RL, and why should we care?</h2>
<p>在上一部分中我们已经介绍了什么是DL以及什么是DRL，并且向大家简单说明了在使用DL之前的方法（我们称之为“标准方法”）是怎样的。简而言之，标准方法中，特征是人为定义和提取的，机器的行为策略也是人为定义的。下面我们将总结一下引入RL后的改变，帮助大家理解<strong>Why should we care about RL?</strong></p>
<ul>
<li>计算机视觉领域</li>
</ul>
<blockquote>
<p>标准方法：人为提取每一层特征，很复杂，甚至可以作为一个人整个博士期间的研究工作；</p>
</blockquote>
<blockquote>
<p>DL：end-to-end training (端到端)</p>
</blockquote>
<blockquote>
<blockquote>
<p>优点：</p>
<ol>
<li>减少人工量；2. 找到的方法往往比用标准方法更好。</li>
</ol>
</blockquote>
</blockquote>
<ul>
<li>游戏领域</li>
</ul>
<blockquote>
<p>标准方法：人为建立许多特征与策略；</p>
</blockquote>
<blockquote>
<p>DL：end-to-end training (端到端)</p>
</blockquote>
<blockquote>
<blockquote>
<p>优点：</p>
<ol>
<li>减少人工量；2. 找到的方法往往比用标准方法更好。</li>
</ol>
</blockquote>
</blockquote>
<h2 id="3-what-does-end-to-end-learning-mean-for-sequential-decision-making"><a class="markdownIt-Anchor" href="#3-what-does-end-to-end-learning-mean-for-sequential-decision-making"></a> [3] What does end-to-end learning mean for sequential decision making?</h2>
<p>我们先来解释一下什么是end-to-end learning.</p>
<p>这个问题的解释其实和你如何定义一个完整的过程有关，假如我们现在定义一个人的一个完整的反射过程是从他接收到环境信息到他对环境做出反应这一整个过程，那么一个end-to-end learning的意思就是我的学习系统只需要这两个端口的信息（也就是环境信息和人做出的反应）就可以进行学习，而不必对中间的过程再进行拆解。</p>
<p>举个例子，假如你现在在野外，你接受到的环境信息是看到了一只凶猛的野生老虎，一个end-to-end learning系统会直接告诉你快点逃跑（当然如果你有武器你可以选择一搏，我们这里只用一般情况做例子），如果你想问这个系统为什么发出这样的指令，它会告诉你不跑你很可能会死掉，但不会告诉你任何判断的中间分析过程。而如果是其他方法，可能会将这个反射过程分解，分解为信息处理系统与决策系统，在信息处理系统中，它专注于得到信息的精准描述，如判断是不是老虎，如果是，再判断是不是动物园的老虎，判断发现这是一只野生老虎，再判断你是否有武器，发现你没有，这个系统的工作就算结束了，接着把这些信息发送给决策系统，决策系统受到信息：你正面对一只野生老虎，没有武器。接着，给你下达了逃跑的指令。想一想如果你问这个系统你为什么需要逃跑它会怎么回答你？它很可能把信息处理系统的结果告诉你，然后说根据这个信息，判断结果你需要逃跑。这就是二者的区别。</p>
<p>再考虑一下它们的实现机理，发现什么不同了吗？在后面的决策过程中，这个系统根本不需要知道如果你不逃跑会有什么后果，我们只需要把指令写进去，它进行简单的逻辑判断就可以发出决策，但是在前者中，你必须知道哪一种行为会得到什么后果，并且知道你需要什么样的后果，当然这里的后果可以通过不断的尝试得到，因为我们可以通过计算机模拟，不像前面举的例子，计算机模拟中尝试后出现不满意后果是完全可以接受的。</p>
<p>实际上我们完全可以这样总结，end-to-end learning在sequential decision中最大的特点就是需要知道不同的结果是好是坏。</p>
<h4 id="conclusion1-deep-mdoels-are-what-allow-reinforcement-learning-algorithms-to-solve-complex-problems-end-to-end"><a class="markdownIt-Anchor" href="#conclusion1-deep-mdoels-are-what-allow-reinforcement-learning-algorithms-to-solve-complex-problems-end-to-end"></a> Conclusion1: Deep mdoels are what allow reinforcement learning algorithms to solve complex problems end-to-end.</h4>
<h2 id="4-从机器学习到rl"><a class="markdownIt-Anchor" href="#4-从机器学习到rl"></a> [4] 从机器学习到RL</h2>
<p>前面的部分中我们已经讲了很多对RL的理解，下面我们引用RL领域一本经典书籍中对RL的一个说明作为其定义，为后面的叙述做铺垫。这本书是:<em>Reinforcement learning: An introduce</em>，关于该定义的详细解释可以自己从书中去找。</p>
<blockquote>
<p>Beyond the agent and the environment, one can identify four main subelements of a reinforcement learning system: a policy, a reward signal, a value function, and, optionally, a model of the environment.</p>
</blockquote>
<p>结合该定义与之前的例子，我们很容易知道，实现一个RL方法，我们至少需要三个信息：action(行为),observation(观测值),reward(奖励)。下面我们通过重构分类问题以及NPL问题来说明一种观点：RL问题是大多数其他机器学习的一种更一般的表现形式。</p>
<ul>
<li>分类问题→RL</li>
</ul>
<blockquote>
<p>action:输出的标签</p>
</blockquote>
<blockquote>
<p>observation:图像的像素</p>
</blockquote>
<blockquote>
<p>reward:分类正确率</p>
</blockquote>
<ul>
<li>NPL(以翻译为例)→RL</li>
</ul>
<blockquote>
<p>action:翻译</p>
</blockquote>
<blockquote>
<p>observation:原语言</p>
</blockquote>
<blockquote>
<p>reward:翻译质量</p>
</blockquote>
<h2 id="5-why-should-we-study-this-now"><a class="markdownIt-Anchor" href="#5-why-should-we-study-this-now"></a> [5] Why should we study this now?</h2>
<p>这里，我们不加解释的给出三条理由，相信大家能够理解：</p>
<ol>
<li>Advances in deep learning</li>
<li>Advances in reinforcement learning</li>
<li>Advances in computational capability</li>
</ol>
<p>一些成功的例子可能更能够让你感受到这个领域目前的发展前景：</p>
<ol>
<li>用Q-learning学习玩游戏</li>
<li>用policy training控制机器人</li>
<li>alphago, alphastar</li>
</ol>
<h2 id="6-what-other-problems-do-we-need-to-solve-to-enable-real-world-sequential-decision-making"><a class="markdownIt-Anchor" href="#6-what-other-problems-do-we-need-to-solve-to-enable-real-world-sequential-decision-making"></a> [6] What other problems do we need to solve to enable real-world sequential decision making?</h2>
<h4 id="beyond-learning-from-reward"><a class="markdownIt-Anchor" href="#beyond-learning-from-reward"></a> Beyond learning from reward</h4>
<p>基础RL方法处理最大化奖励问题，但是sequential decision还涉及其他方面，这对RL方法提出了新的要求。</p>
<h4 id="where-do-rewards-come-from"><a class="markdownIt-Anchor" href="#where-do-rewards-come-from"></a> Where do rewards come from</h4>
<p>现实世界中的奖励函数很难得到，甚至在某些事情上，感知做完了和完成这件事一样困难。</p>
<h4 id="are-there-other-forms-of-supervision"><a class="markdownIt-Anchor" href="#are-there-other-forms-of-supervision"></a> Are there other forms of supervision</h4>
<ol>
<li>Learning from demonstrations（模仿、推断）</li>
<li>Learning from observing the world</li>
<li>Prediction</li>
</ol>
<h2 id="7-why-deep-rl"><a class="markdownIt-Anchor" href="#7-why-deep-rl"></a> [7] Why deep RL?</h2>
<ol>
<li>deep = can process complex sensory input and also compute really complex functions</li>
<li>RL = can choose complex actions</li>
</ol>
<h2 id="8-what-can-deep-learning-rl-do-well-now"><a class="markdownIt-Anchor" href="#8-what-can-deep-learning-rl-do-well-now"></a> [8] What  can deep learning &amp; RL do well now?</h2>
<ol>
<li>有明确的已知简单规则的事物，如围棋；</li>
<li>有原始感觉的输入以及足够经验的事物，如机器人；</li>
<li>通过专家行为的模仿学习.</li>
</ol>
<h2 id="9-what-has-proven-challenging-so-far"><a class="markdownIt-Anchor" href="#9-what-has-proven-challenging-so-far"></a> [9] What has proven challenging so far?</h2>
<ol>
<li>学习速度：DRL学习速度比人类慢很多；</li>
<li>迁移能力弱；</li>
<li>难以找到合适的奖励函数，奖励函数对于学习行为与学习速度都至关重要；</li>
<li>应该大力发展基于模型的or无模型的RL.</li>
</ol>
<h4 id="参考资料"><a class="markdownIt-Anchor" href="#参考资料"></a> 参考资料</h4>
<ol>
<li>cs294-112, lec-1</li>
<li>Reinforcement Learning: An Introduction</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yangtf983.github.io/2019/01/30/A_simple_introduce_of_RL/" data-id="ck4vz1v4q000110vbh0bgf83o"
         class="article-share-link">Share</a>
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Reinforcement-Learning/" rel="tag">Reinforcement Learning</a></li></ul>

    </footer>

  </div>

  
    
  <nav class="article-nav">
    
      <a href="/2020/01/02/Imitation_Learing&Dagger_Algorithm/" class="article-nav-link">
        <strong class="article-nav-caption">Newer posts</strong>
        <div class="article-nav-title">
          
            Imitation Learning &amp; Dagger Algorithm
          
        </div>
      </a>
    
    
      <a href="/2019/01/28/Reinforcement-Learning%E5%AD%A6%E4%B9%A0%E8%AE%A1%E5%88%92/" class="article-nav-link">
        <strong class="article-nav-caption">Olde posts</strong>
        <div class="article-nav-title">Reinforcement Learning学习计划</div>
      </a>
    
  </nav>


  

  
    
  

</article>



</section>
  <footer class="footer">
  <div class="outer">
    <div class="float-right">
      <ul class="list-inline">
  
    <li><i class="fe fe-smile-alt"></i> <span id="busuanzi_value_site_uv"></span></li>
  
</ul>
    </div>
    <ul class="list-inline">
      <li>&copy; 2020 Young&#39;s Blog</li>
      <li>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a></li>
      <li>Theme  <a href="https://github.com/zhwangart/hexo-theme-ocean" target="_blank" rel="noopener">Ocean</a></li>
    </ul>
  </div>
</footer>

</main>

<aside class="sidebar sidebar-specter">
  
    <button class="navbar-toggle"></button>
<nav class="navbar">
  
    <div class="logo">
      <a href="/"><img src="/images/hexo.svg" alt="Young&#39;s Blog"></a>
    </div>
  
  <ul class="nav nav-main">
    
      <li class="nav-item">
        <a class="nav-item-link" href="/">Home</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/archives">Archives</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/categories">Categories</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/tags">Tag</a>
      </li>
    
    <li class="nav-item">
      <a class="nav-item-link nav-item-search" title="Search">
        <i class="fe fe-search"></i>
        Search
      </a>
    </li>
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      <div class="totop" id="totop">
  <i class="fe fe-rocket"></i>
</div>
    </li>
    <li class="nav-item">
      
        <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
          <i class="fe fe-feed"></i>
        </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
  </aside>
  
<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/jquery.justifiedGallery.min.js"></script>


<script src="/js/lazyload.min.js"></script>


<script src="/js/busuanzi-2.3.pure.min.js"></script>


  
<script src="/fancybox/jquery.fancybox.min.js"></script>




  
<script src="/js/tocbot.min.js"></script>

  <script>
    // Tocbot_v4.7.0  http://tscanlin.github.io/tocbot/
    tocbot.init({
      tocSelector: '.tocbot',
      contentSelector: '.article-entry',
      headingSelector: 'h1, h2, h3, h4, h5, h6',
      hasInnerContainers: true,
      scrollSmooth: true,
      positionFixedSelector: '.tocbot',
      positionFixedClass: 'is-position-fixed',
      fixedSidebarOffset: 'auto',
    });
  </script>



<script src="/js/ocean.js"></script>


</body>
</html>