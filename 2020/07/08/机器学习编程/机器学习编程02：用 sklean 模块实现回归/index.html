<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/blog/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/blog/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/blog/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/blog/images/logo.svg" color="#222">
  <meta http-equiv="Cache-Control" content="no-transform">
  <meta http-equiv="Cache-Control" content="no-siteapp">

<link rel="stylesheet" href="/blog/css/main.css">


<link rel="stylesheet" href="/blog/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/blog/lib/pace/pace-theme-bounce.min.css">
  <script src="/blog/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"youngtf.gitee.io","root":"/blog/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"right","width":350,"display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"default"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="0 说明 本文希望实现的回归模型有 SVR, linear regression, ridge regression, logistic regression.">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习编程 02：用 sklean 模块实现回归">
<meta property="og:url" content="http://youngtf.gitee.io/blog/2020/07/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BC%96%E7%A8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BC%96%E7%A8%8B02%EF%BC%9A%E7%94%A8%20sklean%20%E6%A8%A1%E5%9D%97%E5%AE%9E%E7%8E%B0%E5%9B%9E%E5%BD%92/index.html">
<meta property="og:site_name" content="Young&#39;s Blog">
<meta property="og:description" content="0 说明 本文希望实现的回归模型有 SVR, linear regression, ridge regression, logistic regression.">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://myfoundationnote-1257754469.cos.ap-nanjing.myqcloud.com/%E7%BC%96%E7%A8%8B%E5%AE%9E%E6%88%98/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BC%96%E7%A8%8B/02/output_10_1.png">
<meta property="og:image" content="https://myfoundationnote-1257754469.cos.ap-nanjing.myqcloud.com/%E7%BC%96%E7%A8%8B%E5%AE%9E%E6%88%98/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BC%96%E7%A8%8B/02/output_24_0.png">
<meta property="og:image" content="https://myfoundationnote-1257754469.cos.ap-nanjing.myqcloud.com/%E7%BC%96%E7%A8%8B%E5%AE%9E%E6%88%98/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BC%96%E7%A8%8B/02/output_32_0.png">
<meta property="og:image" content="https://myfoundationnote-1257754469.cos.ap-nanjing.myqcloud.com/%E7%BC%96%E7%A8%8B%E5%AE%9E%E6%88%98/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BC%96%E7%A8%8B/02/output_42_0.png">
<meta property="article:published_time" content="2020-07-08T13:36:07.000Z">
<meta property="article:modified_time" content="2020-07-19T13:49:06.155Z">
<meta property="article:author" content="Alexis Young">
<meta property="article:tag" content="编程">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://myfoundationnote-1257754469.cos.ap-nanjing.myqcloud.com/%E7%BC%96%E7%A8%8B%E5%AE%9E%E6%88%98/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BC%96%E7%A8%8B/02/output_10_1.png">

<link rel="canonical" href="http://youngtf.gitee.io/blog/2020/07/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BC%96%E7%A8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BC%96%E7%A8%8B02%EF%BC%9A%E7%94%A8%20sklean%20%E6%A8%A1%E5%9D%97%E5%AE%9E%E7%8E%B0%E5%9B%9E%E5%BD%92/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>机器学习编程 02：用 sklean 模块实现回归 | Young's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/blog/atom.xml" title="Young's Blog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/blog/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Young's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/blog/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/blog/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/blog/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags<span class="badge">4</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/blog/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories<span class="badge">5</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/blog/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">39</span></a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://youngtf.gitee.io/blog/2020/07/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BC%96%E7%A8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BC%96%E7%A8%8B02%EF%BC%9A%E7%94%A8%20sklean%20%E6%A8%A1%E5%9D%97%E5%AE%9E%E7%8E%B0%E5%9B%9E%E5%BD%92/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/images/avatar.gif">
      <meta itemprop="name" content="Alexis Young">
      <meta itemprop="description" content="keep foolish, keep hungry">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Young's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          机器学习编程 02：用 sklean 模块实现回归
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-07-08 21:36:07" itemprop="dateCreated datePublished" datetime="2020-07-08T21:36:07+08:00">2020-07-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-07-19 21:49:06" itemprop="dateModified" datetime="2020-07-19T21:49:06+08:00">2020-07-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/blog/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BC%96%E7%A8%8B/" itemprop="url" rel="index"><span itemprop="name">机器学习编程</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/blog/2020/07/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BC%96%E7%A8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BC%96%E7%A8%8B02%EF%BC%9A%E7%94%A8%20sklean%20%E6%A8%A1%E5%9D%97%E5%AE%9E%E7%8E%B0%E5%9B%9E%E5%BD%92/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/blog/2020/07/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BC%96%E7%A8%8B/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BC%96%E7%A8%8B02%EF%BC%9A%E7%94%A8%20sklean%20%E6%A8%A1%E5%9D%97%E5%AE%9E%E7%8E%B0%E5%9B%9E%E5%BD%92/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="0-说明"><a class="markdownIt-Anchor" href="#0-说明"></a> 0 说明</h1>
<p>本文希望实现的回归模型有 SVR, linear regression, ridge regression, logistic regression.</p>
<a id="more"></a>
<h1 id="1-svr"><a class="markdownIt-Anchor" href="#1-svr"></a> 1 SVR</h1>
<p>SVR 在机器学习技法课程第 6 讲中第一次被提出，这种方法是回归方法，但是具有 svm 的优良性质，例如可以用二次规划求解和能够使用核技巧，因此被广泛使用.</p>
<p>关于这个方法<a href="https://scikit-learn.org/stable/modules/svm.html#regression" target="_blank" rel="noopener">sklearn官方文档</a>中的介绍摘录如下：</p>
<p>The method of Support Vector Classification can be extended to solve regression problems. This method is called Support Vector Regression.</p>
<p>The model produced by support vector classification (as described above) depends only on a subset of the training data, because the cost function for building the model does not care about training points that lie beyond the margin. Analogously, the model produced by Support Vector Regression depends only on a subset of the training data, because the cost function ignores samples whose prediction is close to their target.</p>
<p>There are three different implementations of Support Vector Regression: SVR, NuSVR and LinearSVR. LinearSVR provides a faster implementation than SVR but only considers the linear kernel, while NuSVR implements a slightly different formulation than SVR and LinearSVR. See Implementation details for further details.</p>
<h2 id="11-第-1-个例子"><a class="markdownIt-Anchor" href="#11-第-1-个例子"></a> 1.1 第 1 个例子</h2>
<p>参见<a href="https://scikit-learn.org/stable/modules/svm.html#regression" target="_blank" rel="noopener">sklearn官方文档</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line">X = [[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">2</span>, <span class="number">2</span>]]</span><br><span class="line">y = [<span class="number">0.5</span>, <span class="number">2.5</span>]</span><br><span class="line">regr = svm.SVR(kernel=<span class="string">'linear'</span>)</span><br><span class="line">regr.fit(X, y)</span><br></pre></td></tr></table></figure>
<pre><code>SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='scale',
    kernel='linear', max_iter=-1, shrinking=True, tol=0.001, verbose=False)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">regr.predict([[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">0</span>,<span class="number">0</span>], [<span class="number">2</span>,<span class="number">2</span>]])</span><br></pre></td></tr></table></figure>
<pre><code>array([1.5, 0.6, 2.4])
</code></pre>
<p>这个例子中只有两个训练的数据点，因此使用线性回归模型时回归函数值一定等于数据点的值，因为必然存在直线过这两个数据点。但是使用 SVR 的默认参数时却不是这样，这是因为 SVR 权衡了 L2 正则项。可以通过减小 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">ϵ</span></span></span></span> 或增大 C 达到这样的效果。展示如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line">X = [[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">2</span>, <span class="number">2</span>]]</span><br><span class="line">y = [<span class="number">0.5</span>, <span class="number">2.5</span>]</span><br><span class="line">regr = svm.SVR(C=<span class="number">1000</span>, epsilon=<span class="number">0</span>, kernel=<span class="string">'linear'</span>)</span><br><span class="line">regr.fit(X, y)</span><br></pre></td></tr></table></figure>
<pre><code>SVR(C=1000, cache_size=200, coef0=0.0, degree=3, epsilon=0, gamma='scale',
    kernel='linear', max_iter=-1, shrinking=True, tol=0.001, verbose=False)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">regr.predict([[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">0</span>,<span class="number">0</span>], [<span class="number">2</span>,<span class="number">2</span>]])</span><br></pre></td></tr></table></figure>
<pre><code>array([1.5, 0.5, 2.5])
</code></pre>
<h2 id="12-第-2-个例子svr-using-linear-and-non-linear-kernels"><a class="markdownIt-Anchor" href="#12-第-2-个例子svr-using-linear-and-non-linear-kernels"></a> 1.2 第 2 个例子：SVR using linear and non-linear kernels</h2>
<p>参见<a href="https://scikit-learn.org/stable/auto_examples/svm/plot_svm_regression.html#sphx-glr-auto-examples-svm-plot-svm-regression-py" target="_blank" rel="noopener">sklearn官方文档</a></p>
<p>Toy example of 1D regression using linear, polynomial and RBF kernels.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVR</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># #############################################################################</span></span><br><span class="line"><span class="comment"># Generate sample data</span></span><br><span class="line">X = np.sort(<span class="number">5</span> * np.random.rand(<span class="number">40</span>, <span class="number">1</span>), axis=<span class="number">0</span>)</span><br><span class="line">y = np.sin(X).ravel() <span class="comment"># ravel()可以把一个高维数组整理成一个一维数组</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># #############################################################################</span></span><br><span class="line"><span class="comment"># Add noise to targets</span></span><br><span class="line">y[::<span class="number">5</span>] += <span class="number">3</span> * (<span class="number">0.5</span> - np.random.rand(<span class="number">8</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># #############################################################################</span></span><br><span class="line"><span class="comment"># Fit regression model</span></span><br><span class="line">svr_rbf = SVR(kernel=<span class="string">'rbf'</span>, C=<span class="number">100</span>, gamma=<span class="number">0.1</span>, epsilon=<span class="number">.1</span>)</span><br><span class="line">svr_lin = SVR(kernel=<span class="string">'linear'</span>, C=<span class="number">100</span>, gamma=<span class="string">'auto'</span>)</span><br><span class="line">svr_poly = SVR(kernel=<span class="string">'poly'</span>, C=<span class="number">100</span>, gamma=<span class="string">'auto'</span>, degree=<span class="number">3</span>, epsilon=<span class="number">.1</span>,</span><br><span class="line">               coef0=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">svr_rbf</span><br></pre></td></tr></table></figure>
<pre><code>SVR(C=100, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.1,
    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># #############################################################################</span></span><br><span class="line"><span class="comment"># Look at the results</span></span><br><span class="line">lw = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">svrs = [svr_rbf, svr_lin, svr_poly]</span><br><span class="line">kernel_label = [<span class="string">'RBF'</span>, <span class="string">'Linear'</span>, <span class="string">'Polynomial'</span>]</span><br><span class="line">model_color = [<span class="string">'m'</span>, <span class="string">'c'</span>, <span class="string">'g'</span>]</span><br><span class="line"></span><br><span class="line">fig, axes = plt.subplots(nrows=<span class="number">1</span>, ncols=<span class="number">3</span>, figsize=(<span class="number">15</span>, <span class="number">10</span>), sharey=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">for</span> ix, svr <span class="keyword">in</span> enumerate(svrs):</span><br><span class="line">    axes[ix].plot(X, svr.fit(X, y).predict(X), color=model_color[ix], lw=lw,</span><br><span class="line">                  label=<span class="string">'&#123;&#125; model'</span>.format(kernel_label[ix]))</span><br><span class="line">    axes[ix].scatter(X[svr.support_], y[svr.support_], facecolor=<span class="string">"none"</span>,</span><br><span class="line">                     edgecolor=model_color[ix], s=<span class="number">50</span>,</span><br><span class="line">                     label=<span class="string">'&#123;&#125; support vectors'</span>.format(kernel_label[ix]))</span><br><span class="line">    axes[ix].scatter(X[np.setdiff1d(np.arange(len(X)), svr.support_)],</span><br><span class="line">                     y[np.setdiff1d(np.arange(len(X)), svr.support_)],</span><br><span class="line">                     facecolor=<span class="string">"none"</span>, edgecolor=<span class="string">"k"</span>, s=<span class="number">50</span>,</span><br><span class="line">                     label=<span class="string">'other training data'</span>)</span><br><span class="line">    axes[ix].legend(loc=<span class="string">'upper center'</span>, bbox_to_anchor=(<span class="number">0.5</span>, <span class="number">1.1</span>),</span><br><span class="line">                    ncol=<span class="number">1</span>, fancybox=<span class="literal">True</span>, shadow=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">fig.text(<span class="number">0.5</span>, <span class="number">0.04</span>, <span class="string">'data'</span>, ha=<span class="string">'center'</span>, va=<span class="string">'center'</span>)</span><br><span class="line">fig.text(<span class="number">0.06</span>, <span class="number">0.5</span>, <span class="string">'target'</span>, ha=<span class="string">'center'</span>, va=<span class="string">'center'</span>, rotation=<span class="string">'vertical'</span>)</span><br><span class="line">fig.suptitle(<span class="string">"Support Vector Regression"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<pre><code>E:\Anaconda\lib\site-packages\sklearn\utils\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
E:\Anaconda\lib\site-packages\sklearn\utils\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
E:\Anaconda\lib\site-packages\sklearn\utils\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
</code></pre>
<img src="https://myfoundationnote-1257754469.cos.ap-nanjing.myqcloud.com/%E7%BC%96%E7%A8%8B%E5%AE%9E%E6%88%98/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BC%96%E7%A8%8B/02/output_10_1.png" alt="png" style="zoom:70%;" />
<h1 id="2-linear-regression-ridge-regression-logistic-regression"><a class="markdownIt-Anchor" href="#2-linear-regression-ridge-regression-logistic-regression"></a> 2 linear regression, ridge regression, logistic regression</h1>
<p>这三种回归方法的使用都包含在 sklearn.linear_model 中，分别为 linear_model.LinearRegression(), linear_model.Ridge(), linear_model.LogisticRegression()</p>
<h2 id="21-linear_modellinearregression"><a class="markdownIt-Anchor" href="#21-linear_modellinearregression"></a> 2.1 linear_model.LinearRegression()</h2>
<p>class sklearn.linear_model.LinearRegression(*, fit_intercept=True, normalize=False, copy_X=True, n_jobs=None)</p>
<p>参数的含义见<a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression" target="_blank" rel="noopener">官方文档</a></p>
<h3 id="211-第-1-个例子简单线性回归"><a class="markdownIt-Anchor" href="#211-第-1-个例子简单线性回归"></a> 2.1.1 第 1 个例子：简单线性回归</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">X = np.array([[<span class="number">1</span>, <span class="number">13</span>], [<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">30</span>]])</span><br><span class="line"><span class="comment"># y = 1 * x_0 + 2 * x_1 + 3</span></span><br><span class="line">y = np.dot(X, np.array([<span class="number">1</span>, <span class="number">2</span>])) + <span class="number">3</span></span><br><span class="line">reg = LinearRegression().fit(X, y)</span><br><span class="line">reg.score(X, y) <span class="comment"># 返回决定系数</span></span><br></pre></td></tr></table></figure>
<pre><code>1.0
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X</span><br></pre></td></tr></table></figure>
<pre><code>array([[ 1, 13],
       [ 1,  2],
       [ 2,  2],
       [ 2, 30]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">reg.coef_</span><br></pre></td></tr></table></figure>
<pre><code>array([1., 2.])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">reg.intercept_</span><br></pre></td></tr></table></figure>
<pre><code>3.0000000000000107
</code></pre>
<h3 id="212-第-2-个例子数据标准化后再线性回归"><a class="markdownIt-Anchor" href="#212-第-2-个例子数据标准化后再线性回归"></a> 2.1.2 第 2 个例子：数据标准化后再线性回归</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> make_pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"></span><br><span class="line">X = np.array([[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">2</span>], [<span class="number">2</span>, <span class="number">3</span>]])</span><br><span class="line"><span class="comment"># y = 1 * x_0 + 2 * x_1 + 3</span></span><br><span class="line">y = np.dot(X, np.array([<span class="number">1</span>, <span class="number">2</span>])) + <span class="number">3</span></span><br><span class="line"></span><br><span class="line">reg = make_pipeline(StandardScaler(), LinearRegression())</span><br><span class="line">reg.fit(X, y)</span><br></pre></td></tr></table></figure>
<pre><code>Pipeline(memory=None,
         steps=[('standardscaler',
                 StandardScaler(copy=True, with_mean=True, with_std=True)),
                ('linearregression',
                 LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,
                                  normalize=False))],
         verbose=False)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">reg.predict(X)</span><br></pre></td></tr></table></figure>
<pre><code>array([ 6.,  8.,  9., 11.])
</code></pre>
<h3 id="213-第-3-个例子简单线性回归与作图使用内置数据集"><a class="markdownIt-Anchor" href="#213-第-3-个例子简单线性回归与作图使用内置数据集"></a> 2.1.3 第 3 个例子：简单线性回归与作图（使用内置数据集）</h3>
<p>参见<a href="https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html#linear-regression-example" target="_blank" rel="noopener">官方文档</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets, linear_model</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error, r2_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load the diabetes dataset</span></span><br><span class="line">diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use only one feature</span></span><br><span class="line"><span class="comment"># 提取 diabetes_y 的第 3 列数据并转为列向量</span></span><br><span class="line">diabetes_X = diabetes_X[:, np.newaxis, <span class="number">2</span>] </span><br><span class="line"></span><br><span class="line"><span class="comment"># Split the data into training/testing sets</span></span><br><span class="line">diabetes_X_train = diabetes_X[:<span class="number">-20</span>]</span><br><span class="line">diabetes_X_test = diabetes_X[<span class="number">-20</span>:]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split the targets into training/testing sets</span></span><br><span class="line">diabetes_y_train = diabetes_y[:<span class="number">-20</span>]</span><br><span class="line">diabetes_y_test = diabetes_y[<span class="number">-20</span>:]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create linear regression object</span></span><br><span class="line">regr = linear_model.LinearRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train the model using the training sets</span></span><br><span class="line">regr.fit(diabetes_X_train, diabetes_y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make predictions using the testing set</span></span><br><span class="line">diabetes_y_pred = regr.predict(diabetes_X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The coefficients</span></span><br><span class="line">print(<span class="string">'Coefficients: \n'</span>, regr.coef_)</span><br><span class="line"><span class="comment"># The mean squared error</span></span><br><span class="line">print(<span class="string">'Mean squared error: %.2f'</span></span><br><span class="line">      % mean_squared_error(diabetes_y_test, diabetes_y_pred))</span><br><span class="line"><span class="comment"># The coefficient of determination: 1 is perfect prediction</span></span><br><span class="line">print(<span class="string">'Coefficient of determination: %.2f'</span></span><br><span class="line">      % r2_score(diabetes_y_test, diabetes_y_pred))</span><br></pre></td></tr></table></figure>
<pre><code>Coefficients: 
 [938.23786125]
Mean squared error: 2548.07
Coefficient of determination: 0.47
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot outputs</span></span><br><span class="line">plt.scatter(diabetes_X_test, diabetes_y_test,  color=<span class="string">'black'</span>)</span><br><span class="line">plt.plot(diabetes_X_test, diabetes_y_pred, color=<span class="string">'blue'</span>, linewidth=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 隐藏坐标轴数值</span></span><br><span class="line"><span class="comment">#plt.xticks(())</span></span><br><span class="line"><span class="comment">#plt.yticks(())</span></span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<img src="https://myfoundationnote-1257754469.cos.ap-nanjing.myqcloud.com/%E7%BC%96%E7%A8%8B%E5%AE%9E%E6%88%98/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BC%96%E7%A8%8B/02/output_24_0.png" alt="png" style="zoom:70%;" />
<h2 id="22-sklearnlinear_modelridge"><a class="markdownIt-Anchor" href="#22-sklearnlinear_modelridge"></a> 2.2 sklearn.linear_model.Ridge()</h2>
<p>class sklearn.linear_model.Ridge(alpha=1.0, *, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver=‘auto’, random_state=None)</p>
<p>参见<a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn-linear-model-ridge" target="_blank" rel="noopener">官方文档</a>和<a href="https://yangtf983.github.io/2020/03/19/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B014%EF%BC%9ARegularization/" target="_blank" rel="noopener">基石笔记第 14 讲</a></p>
<p>这里使用的优化函数的形式是：||y - Xw||^2_2 + alpha * ||w||^2_2</p>
<p>因此这里的 alpha 即我们在基石课程中用到的 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">λ</span></span></span></span>，越大则说明正则项影响越强。</p>
<h3 id="221-第-1-个例子直接使用岭回归"><a class="markdownIt-Anchor" href="#221-第-1-个例子直接使用岭回归"></a> 2.2.1 第 1 个例子：直接使用岭回归</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">n_samples, n_features = <span class="number">10</span>, <span class="number">5</span></span><br><span class="line">rng = np.random.RandomState(<span class="number">0</span>) <span class="comment"># 设定随机数种子</span></span><br><span class="line">y = rng.randn(n_samples)</span><br><span class="line">X = rng.randn(n_samples, n_features)</span><br><span class="line">clf = Ridge(alpha=<span class="number">1.0</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">clf.fit(X, y)</span><br></pre></td></tr></table></figure>
<pre><code>Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,
      normalize=False, random_state=None, solver='auto', tol=0.001)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">clf.predict(X)</span><br></pre></td></tr></table></figure>
<pre><code>array([ 0.95424223,  0.48712089,  1.10388121,  1.79105759,  1.05769745,
       -0.07915715,  1.12314109, -0.22941441,  0.64906337,  0.52259943])
</code></pre>
<h3 id="222-第-2-个例子画出岭迹"><a class="markdownIt-Anchor" href="#222-第-2-个例子画出岭迹"></a> 2.2.2 第 2  个例子：画出岭迹</h3>
<p>岭迹是用来选择合适的 alpha 的重要方法。在岭回归中，当 alpha 越大，得到模型的平方误差函数也就越大，但是系数绝对值越小，系数的方差也越小。当选定 alpha 时，我们的目标是使得具有 L2 正则项的平方误差项最小。但是 alpha 的选择之前的基石课程中却没有讲。实际上 alpha 的选择是为了使得岭回归的系数与真实的线性模型系数的均方误差达到最小（因为使用岭回归的情景就是原问题的系数方差太大，这可能是因为需要求逆的矩阵不可逆或难以求逆等原因导致的）。为了达到这个目的，一般我们使用岭迹确定合适的 alpha，这个方法就是把岭回归中的各种系数随着 alpha 变化的值画到一张图上，我们称这样连出来的线为岭迹，我们选择曲线趋于平稳时的较小的 alpha 作为合适的 alpha。这样做的合理性为：当曲线趋于平稳时，说明系数的方差变化趋于平稳，这样当继续增大 alpha 时，系数方差的减小量很有可能就小于系数与真实系数偏差的增大量了，而均方误差就等于方差加上偏差，所以这样选择比较有可能得到使得均方误差较小的 alpha。</p>
<p>下面看一个画岭迹的例子，参见<a href="https://scikit-learn.org/stable/auto_examples/linear_model/plot_ridge_path.html#sphx-glr-auto-examples-linear-model-plot-ridge-path-py" target="_blank" rel="noopener">官方文档</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</span><br><span class="line"></span><br><span class="line"><span class="comment"># X is the 10x10 Hilbert matrix</span></span><br><span class="line">X = <span class="number">1.</span> / (np.arange(<span class="number">1</span>, <span class="number">11</span>) + np.arange(<span class="number">0</span>, <span class="number">10</span>)[:, np.newaxis])</span><br><span class="line">y = np.ones(<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># #############################################################################</span></span><br><span class="line"><span class="comment"># Compute paths</span></span><br><span class="line"></span><br><span class="line">n_alphas = <span class="number">200</span></span><br><span class="line">alphas = np.logspace(<span class="number">-10</span>, <span class="number">-2</span>, n_alphas)</span><br><span class="line"></span><br><span class="line">coefs = []</span><br><span class="line"><span class="keyword">for</span> a <span class="keyword">in</span> alphas:</span><br><span class="line">    ridge = linear_model.Ridge(alpha=a, fit_intercept=<span class="literal">False</span>)</span><br><span class="line">    ridge.fit(X, y)</span><br><span class="line">    coefs.append(ridge.coef_)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># #############################################################################</span></span><br><span class="line"><span class="comment"># Display results</span></span><br><span class="line"></span><br><span class="line">ax = plt.gca()</span><br><span class="line"></span><br><span class="line">ax.plot(alphas, coefs)</span><br><span class="line">ax.set_xscale(<span class="string">'log'</span>)</span><br><span class="line">ax.set_xlim(ax.get_xlim()[::<span class="number">-1</span>])  <span class="comment"># reverse axis</span></span><br><span class="line">plt.xlabel(<span class="string">'alpha'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'weights'</span>)</span><br><span class="line">plt.title(<span class="string">'Ridge coefficients as a function of the regularization'</span>)</span><br><span class="line">plt.axis(<span class="string">'tight'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<img src="https://myfoundationnote-1257754469.cos.ap-nanjing.myqcloud.com/%E7%BC%96%E7%A8%8B%E5%AE%9E%E6%88%98/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BC%96%E7%A8%8B/02/output_32_0.png" alt="png" style="zoom:70%;" />
<p>从上图中可以看出，这组数据使用 10e-5 或 10e-6 比较合适</p>
<h2 id="23-sklearnlinear_modellogisticregression"><a class="markdownIt-Anchor" href="#23-sklearnlinear_modellogisticregression"></a> 2.3 sklearn.linear_model.LogisticRegression()</h2>
<p>这里除了介绍 LogisticRegression()，再介绍一下 sklearn.linear_model.LogisticRegressionCV()，后者是 Logistic regression with built-in cross validation.</p>
<p><strong>首先看 LogisticRegression()</strong></p>
<p>class sklearn.linear_model.LogisticRegression(penalty=‘l2’, *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver=‘lbfgs’, max_iter=100, multi_class=‘auto’, verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)</p>
<p>参见<a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn-linear-model-logisticregression" target="_blank" rel="noopener">官方文档</a>和<a href="https://yangtf983.github.io/2020/03/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B010%EF%BC%9ALogistic%20Regression/" target="_blank" rel="noopener">基石笔记第 10 讲</a></p>
<h3 id="231-一个-logisticregression-的小例子"><a class="markdownIt-Anchor" href="#231-一个-logisticregression-的小例子"></a> 2.3.1 一个 LogisticRegression() 的小例子</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line">X, y = load_iris(return_X_y=<span class="literal">True</span>)</span><br><span class="line">clf = LogisticRegression(random_state=<span class="number">0</span>).fit(X, y)</span><br><span class="line">clf.predict(X[:<span class="number">2</span>, :])</span><br></pre></td></tr></table></figure>
<pre><code>E:\Anaconda\lib\site-packages\sklearn\linear_model\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)

array([0, 0])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y[:<span class="number">2</span>]</span><br></pre></td></tr></table></figure>
<pre><code>array([0, 0])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">clf.predict_proba(X[:<span class="number">2</span>, :])</span><br></pre></td></tr></table></figure>
<pre><code>array([[9.81797141e-01, 1.82028445e-02, 1.44269293e-08],
       [9.71725476e-01, 2.82744937e-02, 3.01659208e-08]])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">clf.score(X, y) <span class="comment"># Return the mean accuracy on the given test data and labels.</span></span><br></pre></td></tr></table></figure>
<pre><code>0.9733333333333334
</code></pre>
<h4 id="232-三类别逻辑斯谛回归-logisticregression"><a class="markdownIt-Anchor" href="#232-三类别逻辑斯谛回归-logisticregression"></a> 2.3.2 三类别逻辑斯谛回归 LogisticRegression()</h4>
<p>与上一个例子一样使用 iris 数据集，这个数据集包含三种类别的鸢尾花，y 值是它们的种类，x 值有四个维度，分别代表：花萼长度、花萼宽度、花瓣长度、花瓣宽度。</p>
<p>参见<a href="https://scikit-learn.org/stable/auto_examples/linear_model/plot_iris_logistic.html#sphx-glr-auto-examples-linear-model-plot-iris-logistic-py" target="_blank" rel="noopener">官方文档</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line"><span class="comment"># import some data to play with</span></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">X = iris.data[:, :<span class="number">2</span>]  <span class="comment"># we only take the first two features.</span></span><br><span class="line">Y = iris.target</span><br><span class="line"></span><br><span class="line">logreg = LogisticRegression(C=<span class="number">1e5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create an instance of Logistic Regression Classifier and fit the data.</span></span><br><span class="line">logreg.fit(X, Y)</span><br></pre></td></tr></table></figure>
<pre><code>LogisticRegression(C=100000.0, class_weight=None, dual=False,
                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,
                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot the decision boundary. For that, we will assign a color to each</span></span><br><span class="line"><span class="comment"># point in the mesh [x_min, x_max]x[y_min, y_max].</span></span><br><span class="line">x_min, x_max = X[:, <span class="number">0</span>].min() - <span class="number">.5</span>, X[:, <span class="number">0</span>].max() + <span class="number">.5</span></span><br><span class="line">y_min, y_max = X[:, <span class="number">1</span>].min() - <span class="number">.5</span>, X[:, <span class="number">1</span>].max() + <span class="number">.5</span></span><br><span class="line">h = <span class="number">.02</span>  <span class="comment"># step size in the mesh</span></span><br><span class="line">xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))</span><br><span class="line">Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Put the result into a color plot</span></span><br><span class="line">Z = Z.reshape(xx.shape)</span><br><span class="line">plt.figure(<span class="number">1</span>, figsize=(<span class="number">4</span>, <span class="number">3</span>))</span><br><span class="line">plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot also the training points</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=Y, edgecolors=<span class="string">'k'</span>, cmap=plt.cm.Paired)</span><br><span class="line">plt.xlabel(<span class="string">'Sepal length'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Sepal width'</span>)</span><br><span class="line"></span><br><span class="line">plt.xlim(xx.min(), xx.max())</span><br><span class="line">plt.ylim(yy.min(), yy.max())</span><br><span class="line">plt.xticks(())</span><br><span class="line">plt.yticks(())</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<img src="https://myfoundationnote-1257754469.cos.ap-nanjing.myqcloud.com/%E7%BC%96%E7%A8%8B%E5%AE%9E%E6%88%98/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%BC%96%E7%A8%8B/02/output_42_0.png" alt="png" style="zoom:70%;" />
<p><strong>接着看一下 LogisticRegressionCV()</strong></p>
<p>class sklearn.linear_model.LogisticRegressionCV(*, Cs=10, fit_intercept=True, cv=None, dual=False, penalty=‘l2’, scoring=None, solver=‘lbfgs’, tol=0.0001, max_iter=100, class_weight=None, n_jobs=None, verbose=0, refit=True, intercept_scaling=1.0, multi_class=‘auto’, random_state=None, l1_ratios=None)</p>
<p>参见<a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV" target="_blank" rel="noopener">官方文档</a></p>
<p>与交叉验证有关的重要的参数有两个：</p>
<pre><code>1. Cs: int or list of floats, default=10
</code></pre>
<blockquote>
<p>Each of the values in Cs describes the inverse of regularization strength. If Cs is as an int, then a grid of Cs values are chosen in a logarithmic scale between 1e-4 and 1e4. Like in support vector machines, smaller values specify stronger regularization.</p>
</blockquote>
<pre><code>2. cv: int or cross-validation generator, default=None
</code></pre>
<blockquote>
<p>The default cross-validation generator used is Stratified K-Folds. If an integer is provided, then it is the number of folds used. See the module sklearn.model_selection module for the list of possible cross-validation objects.</p>
</blockquote>
<h4 id="233-一个-logisticregressioncv-的例子"><a class="markdownIt-Anchor" href="#233-一个-logisticregressioncv-的例子"></a> 2.3.3 一个 LogisticRegressionCV() 的例子</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegressionCV</span><br><span class="line">X, y = load_iris(return_X_y=<span class="literal">True</span>)</span><br><span class="line">clf = LogisticRegressionCV(cv=<span class="number">5</span>, random_state=<span class="number">0</span>)</span><br><span class="line">clf.fit(X, y)</span><br></pre></td></tr></table></figure>
<pre><code>E:\Anaconda\lib\site-packages\sklearn\linear_model\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)
E:\Anaconda\lib\site-packages\sklearn\linear_model\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)
E:\Anaconda\lib\site-packages\sklearn\linear_model\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)
E:\Anaconda\lib\site-packages\sklearn\linear_model\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)
E:\Anaconda\lib\site-packages\sklearn\linear_model\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)





LogisticRegressionCV(Cs=10, class_weight=None, cv=5, dual=False,
                     fit_intercept=True, intercept_scaling=1.0, l1_ratios=None,
                     max_iter=100, multi_class='auto', n_jobs=None,
                     penalty='l2', random_state=0, refit=True, scoring=None,
                     solver='lbfgs', tol=0.0001, verbose=0)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">clf.predict(X[:<span class="number">2</span>, :])</span><br></pre></td></tr></table></figure>
<pre><code>array([0, 0])
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">clf.predict_proba(X[:<span class="number">2</span>, :]).shape</span><br></pre></td></tr></table></figure>
<pre><code>(2, 3)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">clf.score(X, y)</span><br></pre></td></tr></table></figure>
<pre><code>0.98
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">clf.get_params()</span><br></pre></td></tr></table></figure>
<pre><code>{'Cs': 10,
 'class_weight': None,
 'cv': 5,
 'dual': False,
 'fit_intercept': True,
 'intercept_scaling': 1.0,
 'l1_ratios': None,
 'max_iter': 100,
 'multi_class': 'auto',
 'n_jobs': None,
 'penalty': 'l2',
 'random_state': 0,
 'refit': True,
 'scoring': None,
 'solver': 'lbfgs',
 'tol': 0.0001,
 'verbose': 0}
</code></pre>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/blog/tags/%E7%BC%96%E7%A8%8B/" rel="tag"># 编程</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/blog/2020/07/08/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%B3%95%E7%AC%94%E8%AE%B05%EF%BC%9AKernel%20Logistic%20Regression/" rel="prev" title="机器学习技法笔记5：Kernel Logistic Regression">
      <i class="fa fa-chevron-left"></i> 机器学习技法笔记5：Kernel Logistic Regression
    </a></div>
      <div class="post-nav-item">
    <a href="/blog/2020/07/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%B3%95/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%B3%95%E7%AC%94%E8%AE%B07%EF%BC%9ABlending%20and%20Bagging/" rel="next" title="机器学习技法笔记7：Blending and Bagging">
      机器学习技法笔记7：Blending and Bagging <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
  
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#0-说明"><span class="nav-number">1.</span> <span class="nav-text"> 0 说明</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#1-svr"><span class="nav-number">2.</span> <span class="nav-text"> 1 SVR</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#11-第-1-个例子"><span class="nav-number">2.1.</span> <span class="nav-text"> 1.1 第 1 个例子</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#12-第-2-个例子svr-using-linear-and-non-linear-kernels"><span class="nav-number">2.2.</span> <span class="nav-text"> 1.2 第 2 个例子：SVR using linear and non-linear kernels</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-linear-regression-ridge-regression-logistic-regression"><span class="nav-number">3.</span> <span class="nav-text"> 2 linear regression, ridge regression, logistic regression</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#21-linear_modellinearregression"><span class="nav-number">3.1.</span> <span class="nav-text"> 2.1 linear_model.LinearRegression()</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#211-第-1-个例子简单线性回归"><span class="nav-number">3.1.1.</span> <span class="nav-text"> 2.1.1 第 1 个例子：简单线性回归</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#212-第-2-个例子数据标准化后再线性回归"><span class="nav-number">3.1.2.</span> <span class="nav-text"> 2.1.2 第 2 个例子：数据标准化后再线性回归</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#213-第-3-个例子简单线性回归与作图使用内置数据集"><span class="nav-number">3.1.3.</span> <span class="nav-text"> 2.1.3 第 3 个例子：简单线性回归与作图（使用内置数据集）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#22-sklearnlinear_modelridge"><span class="nav-number">3.2.</span> <span class="nav-text"> 2.2 sklearn.linear_model.Ridge()</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#221-第-1-个例子直接使用岭回归"><span class="nav-number">3.2.1.</span> <span class="nav-text"> 2.2.1 第 1 个例子：直接使用岭回归</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#222-第-2-个例子画出岭迹"><span class="nav-number">3.2.2.</span> <span class="nav-text"> 2.2.2 第 2  个例子：画出岭迹</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#23-sklearnlinear_modellogisticregression"><span class="nav-number">3.3.</span> <span class="nav-text"> 2.3 sklearn.linear_model.LogisticRegression()</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#231-一个-logisticregression-的小例子"><span class="nav-number">3.3.1.</span> <span class="nav-text"> 2.3.1 一个 LogisticRegression() 的小例子</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#232-三类别逻辑斯谛回归-logisticregression"><span class="nav-number">3.3.1.1.</span> <span class="nav-text"> 2.3.2 三类别逻辑斯谛回归 LogisticRegression()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#233-一个-logisticregressioncv-的例子"><span class="nav-number">3.3.1.2.</span> <span class="nav-text"> 2.3.3 一个 LogisticRegressionCV() 的例子</span></a></li></ol></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Alexis Young</p>
  <div class="site-description" itemprop="description">keep foolish, keep hungry</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/blog/archives/">
        
          <span class="site-state-item-count">39</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/blog/categories/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/blog/tags/">
          
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="mailto:pdfty@qq.com" title="E-Mail → mailto:pdfty@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>
	  

    </div>
	

  
  </aside>
  
  
  
  <div id="sidebar-dimmer"></div>





      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Alexis Young</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/blog/lib/anime.min.js"></script>
  <script src="/blog/lib/velocity/velocity.min.js"></script>
  <script src="/blog/lib/velocity/velocity.ui.min.js"></script>

<script src="/blog/js/utils.js"></script>

<script src="/blog/js/motion.js"></script>


<script src="/blog/js/schemes/pisces.js"></script>


<script src="/blog/js/next-boot.js"></script>




  
  <script>
    (function(){
      var canonicalURL, curProtocol;
      //Get the <link> tag
      var x=document.getElementsByTagName("link");
		//Find the last canonical URL
		if(x.length > 0){
			for (i=0;i<x.length;i++){
				if(x[i].rel.toLowerCase() == 'canonical' && x[i].href){
					canonicalURL=x[i].href;
				}
			}
		}
    //Get protocol
	    if (!canonicalURL){
	    	curProtocol = window.location.protocol.split(':')[0];
	    }
	    else{
	    	curProtocol = canonicalURL.split(':')[0];
	    }
      //Get current URL if the canonical URL does not exist
	    if (!canonicalURL) canonicalURL = window.location.href;
	    //Assign script content. Replace current URL with the canonical URL
      !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=canonicalURL,t=document.referrer;if(!e.test(r)){var n=(String(curProtocol).toLowerCase() === 'https')?"https://sp0.baidu.com/9_Q4simg2RQJ8t7jm9iCKT-xh_/s.gif":"//api.share.baidu.com/s.gif";t?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var i=new Image;i.src=n}}(window);})();
  </script>




  
<script src="/blog/js/local-search.js"></script>













  

  
      
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/katex@0/dist/katex.min.css">
  <script src="//cdn.jsdelivr.net/npm/katex@0/dist/contrib/copy-tex.min.js"></script>
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/katex@0/dist/contrib/copy-tex.min.css">


  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : true,
      appId      : 'cB5O17hsXHPszGY3QjDjcDsu-9Nh9j0Va',
      appKey     : '5qyQKImjkxPmtLFmrIGYQiN7',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : true,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
