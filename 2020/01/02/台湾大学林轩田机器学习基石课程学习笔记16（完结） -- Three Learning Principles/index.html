<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  
  
    <meta name="keywords" content="12">
  
  
    <meta name="description" content="keep foolish, keep hungry">
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <title>
    台湾大学林轩田机器学习基石课程学习笔记16（完结） -- Three Learning Principles |
    
    Young&#39;s Blog</title>
  
    <link rel="shortcut icon" href="/favicon.ico">
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<script src="/js/pace.min.js"></script>

  <link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css" rel="stylesheet" type="text/css">
<meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="Young's Blog" type="application/atom+xml">
</head>

<body>
<main class="content">
  <section class="outer">
  

<article id="post-台湾大学林轩田机器学习基石课程学习笔记16（完结） -- Three Learning Principles" class="article article-type-post" itemscope itemprop="blogPost" data-scroll-reveal>
  
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      台湾大学林轩田机器学习基石课程学习笔记16（完结） -- Three Learning Principles
    </h1>
  
  




      </header>
    

    
      <div class="article-meta">
        <a href="/2020/01/02/%E5%8F%B0%E6%B9%BE%E5%A4%A7%E5%AD%A6%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E8%AF%BE%E7%A8%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016%EF%BC%88%E5%AE%8C%E7%BB%93%EF%BC%89%20--%20Three%20Learning%20Principles/" class="article-date">
  <time datetime="2020-01-02T07:12:51.072Z" itemprop="datePublished">2020-01-02</time>
</a>
        
      </div>
    

    
      




    

    <div class="article-entry" itemprop="articleBody">
      


      

      
        <blockquote>
<p>作者：红色石头</p>
</blockquote>
<blockquote>
<p>公众号：AI有道（ID：redstonewill）</p>
</blockquote>
<p>上节课我们讲了一个机器学习很重要的工具——Validation。我们将整个训练集分成两部分：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>D</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">D_{train}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mord mathdefault mtight" style="margin-right:0.02778em;">r</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>D</mi><mrow><mi>v</mi><mi>a</mi><mi>l</mi></mrow></msub></mrow><annotation encoding="application/x-tex">D_{val}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03588em;">v</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，一部分作为机器学习模型建立的训练数据，另一部分作为验证模型好坏的数据，从而选择到更好的模型，实现更好的泛化能力。这节课，我们主要介绍机器学习中非常实用的三个“锦囊妙计”。</p>
<p>###<strong>一、Occam’s Razor</strong></p>
<p>奥卡姆剃刀定律（Occam’s Razor），是由14世纪逻辑学家、圣方济各会修士奥卡姆的威廉（William of Occam，约1285年至1349年）提出。奥卡姆（Ockham）在英格兰的萨里郡，那是他出生的地方。他在《箴言书注》2卷15题说“切勿浪费较多东西去做用较少的东西同样可以做好的事情。” 这个原理称为“如无必要，勿增实体”（Entities must not be multiplied unnecessarily），就像剃刀一样，将不必要的部分去除掉。</p>
<p>Occam’s Razor反映到机器学习领域中，指的是在所有可能选择的模型中，我们应该选择能够很好地解释已知数据并且十分简单的模型。</p>
<p><img src="http://img.blog.csdn.net/20170605194127527?" alt="这里写图片描述" /></p>
<p>上图就是一个模型选择的例子，左边的模型很简单，可能有分错的情况；而右边的模型非常复杂，所有的训练样本都分类正确。但是，我们会选择左边的模型，它更简单，符合人类直觉的解释方式。这样的结果带来两个问题：一个是什么模型称得上是简单的？另一个是为什么简单模型比复杂模型要好？</p>
<p>简单的模型一方面指的是简单的hypothesis h，简单的hypothesis就是指模型使用的特征比较少，例如多项式阶数比较少。简单模型另一方面指的是模型H包含的hypothesis数目有限，不会太多，这也是简单模型包含的内容。</p>
<p><img src="http://img.blog.csdn.net/20170605195055446?" alt="这里写图片描述" /></p>
<p>其实，simple hypothesis h和simple model H是紧密联系的。如果hypothesis的特征个数是l，那么H中包含的hypothesis个数就是<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mn>2</mn><mi>l</mi></msup></mrow><annotation encoding="application/x-tex">2^l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.849108em;vertical-align:0em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span></span></span></span></span></span></span></span>，也就是说，hypothesis特征数目越少，H中hypothesis数目也就越少。</p>
<p>所以，为了让模型简单化，我们可以一开始就选择简单的model，或者用regularization，让hypothesis中参数个数减少，都能降低模型复杂度。</p>
<p>那为什么简单的模型更好呢？下面从哲学的角度简单解释一下。机器学习的目的是“找规律”，即分析数据的特征，总结出规律性的东西出来。假设现在有一堆没有规律的杂乱的数据需要分类，要找到一个模型，让它的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>E</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">E_{in}=0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span></span></span></span>，是很难的，大部分时候都无法正确分类，但是如果是很复杂的模型，也有可能将其分开。反过来说，如果有另一组数据，如果可以比较容易找到一个模型能完美地把数据分开，那表明数据本身应该是有某种规律性。也就是说杂乱的数据应该不可以分开，能够分开的数据应该不是杂乱的。如果使用某种简单的模型就可以将数据分开，那表明数据本身应该符合某种规律性。相反地，如果用很复杂的模型将数据分开，并不能保证数据本身有规律性存在，也有可能是杂乱的数据，因为无论是有规律数据还是杂乱数据，复杂模型都能分开。这就不是机器学习模型解决的内容了。所以，模型选择中，我们应该尽量先选择简单模型，例如最简单的线性模型。</p>
<p>###<strong>二、Sampling Bias</strong></p>
<p>首先引入一个有趣的例子：1948年美国总统大选的两位热门候选人是Truman和Dewey。一家报纸通过电话采访，统计人们把选票投给了Truman还是Dewey。经过大量的电话统计显示，投给Dewey的票数要比投个Truman的票数多，所以这家报纸就在选举结果还没公布之前，信心满满地发表了“Dewey Defeats Truman”的报纸头版，认为Dewey肯定赢了。但是大选结果公布后，让这家报纸大跌眼镜，最终Truman赢的了大选的胜利。</p>
<p>为什么会出现跟电话统计完全相反的结果呢？是因为电话统计数据出错还是投票运气不好？都不是。其实是因为当时电话比较贵，有电话的家庭比较少，而正好是有电话的美国人支持Dewey的比较多，而没有电话的支持Truman比较多。也就是说样本选择偏向于有钱人那边，可能不具有广泛的代表性，才造成Dewey支持率更多的假象。</p>
<p>这个例子表明，抽样的样本会影响到结果，用一句话表示“If the data is sampled in a biased way, learning will produce a similarly biased outcome.”意思是，如果抽样有偏差的话，那么学习的结果也产生了偏差，这种情形称之为抽样偏差Sampling Bias。</p>
<p>从技术上来说，就是训练数据和验证数据要服从同一个分布，最好都是独立同分布的，这样训练得到的模型才能更好地具有代表性。</p>
<p>###<strong>三、Data Snooping</strong></p>
<p>之前的课程，我们介绍过在模型选择时应该尽量避免偷窥数据，因为这样会使我们人为地倾向于某种模型，而不是根据数据进行随机选择。所以，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">Φ</mi></mrow><annotation encoding="application/x-tex">\Phi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord">Φ</span></span></span></span>应该自由选取，最好不要偷窥到原始数据，这会影响我们的判断。</p>
<p>事实上，数据偷窥发生的情况有很多，不仅仅指我们看到了原始数据。什么意思呢？其实，当你在使用这些数据的任何过程，都是间接地偷看到了数据本身，然后你会进行一些模型的选择或者决策，这就增加了许多的model complexity，也就是引入了污染。</p>
<p>下面举个例子来说明。假如我们有8年的货比交易数据，我们希望从这些数据中找出规律，来预测货比的走势。如果选择前6年数据作为训练数据，后2年数据作为测试数据的话，来训练模型。现在我们有前20天的数据，根据之前训练的模型，来预测第21天的货比交易走势。</p>
<p><img src="http://img.blog.csdn.net/20170605213904953?" alt="这里写图片描述" /></p>
<p>现在有两种训练模型的方法，如图所示，一种是使用前6年数据进行模型训练，后2年数据作为测试，图中蓝色曲线表示后2年的预测收益；另一种是直接使用8年数据进行模型训练，图中红色曲线表示后2年的预测收益情况。图中，很明显，使用8年数据进行训练的模型对后2年的预测的收益更大，似乎效果更好。但是这是一种自欺欺人的做法，因为训练的时候已经拿到了后2年的数据，用这样的模型再来预测后2年的走势是不科学的。这种做法也属于间接偷窥数据的行为。直接偷窥和间接偷窥数据的行为都是不科学的做法，并不能表示训练的模型有多好。</p>
<p><img src="http://img.blog.csdn.net/20170605214612062?" alt="这里写图片描述" /></p>
<p>还有一个偷窥数据的例子，比如对于某个基准数据集D，某人对它建立了一个模型H1，并发表了论文。第二个人看到这篇论文后，又会对D，建立一个新的好的模型H2。这样，不断地有人看过前人的论文后，建立新的模型。其实，后面人选择模型时，已经被前人影响了，这也是偷窥数据的一种情况。也许你能对D训练很好的模型，但是可能你仅仅只根据前人的模型，成功避开了一些错误，甚至可能发生了overfitting或者bad generalization。所以，机器学习领域有这样一句有意思的话“If you torture the data long enough, it will confess.”所以，我们不能太“折磨”我们的数据了，否则它只能“妥协”了~哈哈。</p>
<p>在机器学习过程中，避免“偷窥数据”非常重要，但实际上，完全避免也很困难。实际操作中，有一些方法可以帮助我们尽量避免偷窥数据。第一个方法是“看不见”数据。就是说当我们在选择模型的时候，尽量用我们的经验和知识来做判断选择，而不是通过数据来选择。先选模型，再看数据。第二个方法是保持怀疑。就是说时刻保持对别人的论文或者研究成果保持警惕与怀疑，要通过自己的研究与测试来进行模型选择，这样才能得到比较正确的结论。</p>
<p><img src="http://img.blog.csdn.net/20170605220807260?" alt="这里写图片描述" /></p>
<p>###<strong>四、Power of Three</strong></p>
<p>本小节，我们对16节课做个简单的总结，用“三的威力”进行概括。因为课程中我们介绍的很多东西都与三有关。</p>
<p>首先，我们介绍了跟机器学习相关的三个领域：</p>
<ul>
<li>
<p>Data Mining</p>
</li>
<li>
<p>Artificial Intelligence</p>
</li>
<li>
<p>Statistics</p>
</li>
</ul>
<p><img src="http://img.blog.csdn.net/20170606080342125?" alt="这里写图片描述" /></p>
<p>我们还介绍了三个理论保证：</p>
<ul>
<li>
<p>Hoeffding</p>
</li>
<li>
<p>Multi-Bin Hoeffding</p>
</li>
<li>
<p>VC</p>
</li>
</ul>
<p><img src="http://img.blog.csdn.net/20170606080556063?" alt="这里写图片描述" /></p>
<p>然后，我们又介绍了三种线性模型：</p>
<ul>
<li>
<p>PLA/pocket</p>
</li>
<li>
<p>linear regression</p>
</li>
<li>
<p>logistic regression</p>
</li>
</ul>
<p><img src="http://img.blog.csdn.net/20170606080846308?" alt="这里写图片描述" /></p>
<p>同时，我们介绍了三种重要的工具：</p>
<ul>
<li>
<p>Feature Transform</p>
</li>
<li>
<p>Regularization</p>
</li>
<li>
<p>Validation</p>
</li>
</ul>
<p><img src="http://img.blog.csdn.net/20170606081141583?" alt="这里写图片描述" /></p>
<p>还有我们本节课介绍的三个锦囊妙计：</p>
<ul>
<li>
<p>Occam’s Razer</p>
</li>
<li>
<p>Sampling Bias</p>
</li>
<li>
<p>Data Snooping</p>
</li>
</ul>
<p><img src="http://img.blog.csdn.net/20170606081339252?" alt="这里写图片描述" /></p>
<p>最后，我们未来机器学习的方向也分为三种：</p>
<ul>
<li>
<p>More Transform</p>
</li>
<li>
<p>More Regularization</p>
</li>
<li>
<p>Less Label</p>
</li>
</ul>
<p><img src="http://img.blog.csdn.net/20170606081719882?" alt="这里写图片描述" /></p>
<p>###<strong>五、总结</strong></p>
<p>本节课主要介绍了机器学习三个重要的锦囊妙计：Occam’s Razor, Sampling Bias, Data Snooping。并对《机器学习基石》课程中介绍的所有知识和方法进行“三的威力”这种形式的概括与总结，“三的威力”也就构成了坚固的机器学习基石。</p>
<p>整个机器学习基石的课程笔记总结完毕！后续将会推出机器学习技法的学习笔记，谢谢！</p>
<p><em><strong>注明：</strong></em></p>
<p>文章中所有的图片均来自台湾大学林轩田《机器学习基石》课程</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yangtf983.github.io/2020/01/02/%E5%8F%B0%E6%B9%BE%E5%A4%A7%E5%AD%A6%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E8%AF%BE%E7%A8%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B016%EF%BC%88%E5%AE%8C%E7%BB%93%EF%BC%89%20--%20Three%20Learning%20Principles/" data-id="ck4we8hp70000kwvbhhcphbem"
         class="article-share-link">Share</a>
      
    </footer>

  </div>

  
    
  <nav class="article-nav">
    
    
      <a href="/2020/01/02/Imitation_Learing&Dagger_Algorithm/" class="article-nav-link">
        <strong class="article-nav-caption">Olde posts</strong>
        <div class="article-nav-title">Imitation Learning &amp; Dagger Algorithm</div>
      </a>
    
  </nav>


  

  
    
  

</article>



</section>
  <footer class="footer">
  <div class="outer">
    <div class="float-right">
      <ul class="list-inline">
  
    <li><i class="fe fe-smile-alt"></i> <span id="busuanzi_value_site_uv"></span></li>
  
</ul>
    </div>
    <ul class="list-inline">
      <li>&copy; 2020 Young&#39;s Blog</li>
      <li>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a></li>
      <li>Theme  <a href="https://github.com/zhwangart/hexo-theme-ocean" target="_blank" rel="noopener">Ocean</a></li>
    </ul>
  </div>
</footer>

</main>

<aside class="sidebar sidebar-specter">
  
    <button class="navbar-toggle"></button>
<nav class="navbar">
  
    <div class="logo">
      <a href="/"><img src="/images/hexo.svg" alt="Young&#39;s Blog"></a>
    </div>
  
  <ul class="nav nav-main">
    
      <li class="nav-item">
        <a class="nav-item-link" href="/">Home</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/archives">Archives</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/categories">Categories</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/tags">Tag</a>
      </li>
    
    <li class="nav-item">
      <a class="nav-item-link nav-item-search" title="Search">
        <i class="fe fe-search"></i>
        Search
      </a>
    </li>
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      <div class="totop" id="totop">
  <i class="fe fe-rocket"></i>
</div>
    </li>
    <li class="nav-item">
      
        <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
          <i class="fe fe-feed"></i>
        </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
  </aside>
  
<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/jquery.justifiedGallery.min.js"></script>


<script src="/js/lazyload.min.js"></script>


<script src="/js/busuanzi-2.3.pure.min.js"></script>


  
<script src="/fancybox/jquery.fancybox.min.js"></script>




  
<script src="/js/tocbot.min.js"></script>

  <script>
    // Tocbot_v4.7.0  http://tscanlin.github.io/tocbot/
    tocbot.init({
      tocSelector: '.tocbot',
      contentSelector: '.article-entry',
      headingSelector: 'h1, h2, h3, h4, h5, h6',
      hasInnerContainers: true,
      scrollSmooth: true,
      positionFixedSelector: '.tocbot',
      positionFixedClass: 'is-position-fixed',
      fixedSidebarOffset: 'auto',
    });
  </script>



<script src="/js/ocean.js"></script>


</body>
</html>