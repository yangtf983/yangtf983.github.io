<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Young&#39;s Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yangtf983.github.io/"/>
  <updated>2020-03-18T09:05:37.853Z</updated>
  <id>http://yangtf983.github.io/</id>
  
  <author>
    <name>Alexis Young</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>机器学习基石笔记11：Linear Models for Classification</title>
    <link href="http://yangtf983.github.io/2020/03/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B011%EF%BC%9ALinear%20Models%20for%20Classification/"/>
    <id>http://yangtf983.github.io/2020/03/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B011%EF%BC%9ALinear%20Models%20for%20Classification/</id>
    <published>2020-03-18T09:05:07.000Z</published>
    <updated>2020-03-18T09:05:37.853Z</updated>
    
    <summary type="html">
    
      &lt;h1 id=&quot;0-说明&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#0-说明&quot;&gt;&lt;/a&gt; 0 说明&lt;/h1&gt;
&lt;p&gt;本堂课在整合之前讲的分类问题后将其延伸到更多更复杂的问题上。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="机器学习基石听课笔记" scheme="http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E5%90%AC%E8%AF%BE%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习基石笔记10：Logistic Regression</title>
    <link href="http://yangtf983.github.io/2020/03/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B010%EF%BC%9ALogistic%20Regression/"/>
    <id>http://yangtf983.github.io/2020/03/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B010%EF%BC%9ALogistic%20Regression/</id>
    <published>2020-03-17T17:15:07.000Z</published>
    <updated>2020-03-17T17:15:15.738Z</updated>
    
    <summary type="html">
    
      &lt;h1 id=&quot;0-说明&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#0-说明&quot;&gt;&lt;/a&gt; 0 说明&lt;/h1&gt;
&lt;p&gt;这一节探讨Logistic回归的问题，这种回归实际上是可以看做是一种soft binary classification问题的解法，这种问题是说数据的标签是硬的（确定是圈或叉），但理想的无噪声数据的标签其实是软的（只有是不同标签的概率值）。Logistic模型的形式给出的就是0到1之间的数字（对概率的估计）。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="机器学习基石听课笔记" scheme="http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E5%90%AC%E8%AF%BE%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习基石笔记9：Linear Regression</title>
    <link href="http://yangtf983.github.io/2020/03/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B09%EF%BC%9ALinear%20Regression/"/>
    <id>http://yangtf983.github.io/2020/03/17/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B09%EF%BC%9ALinear%20Regression/</id>
    <published>2020-03-17T13:48:07.000Z</published>
    <updated>2020-03-17T13:49:12.637Z</updated>
    
    <summary type="html">
    
      &lt;h1 id=&quot;0-说明&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#0-说明&quot;&gt;&lt;/a&gt; 0 说明&lt;/h1&gt;
&lt;p&gt;从这一节课开始讲几种算法。&lt;br /&gt;
这一节将线性回归，中间的计算步骤涉及到很多矩阵运算，由于这些矩阵运算一般是学过线性代数的人应该已经掌握的，所以文中对视频中对矩阵计算所做的一些解释就不再重复（实际上视频中的解释相当有限，直接看最终结果无碍）。未来几篇文章都会更加关注于对具体算法本身的理解，而对计算的解释将会尽可能忽略。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="机器学习基石听课笔记" scheme="http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E5%90%AC%E8%AF%BE%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习基石笔记8：Noise and Error</title>
    <link href="http://yangtf983.github.io/2020/03/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B08%EF%BC%9ANoise%20and%20Error/"/>
    <id>http://yangtf983.github.io/2020/03/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B08%EF%BC%9ANoise%20and%20Error/</id>
    <published>2020-03-16T05:04:07.000Z</published>
    <updated>2020-03-16T05:04:49.916Z</updated>
    
    <summary type="html">
    
      &lt;h1 id=&quot;0-说明&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#0-说明&quot;&gt;&lt;/a&gt; 0 说明&lt;/h1&gt;
&lt;p&gt;之前的VC bound和VC Dimension是建立在一定的假设上的，这一堂课将学习如何放宽这些假设，使其能够适用于更多不同的问题。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="机器学习基石听课笔记" scheme="http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E5%90%AC%E8%AF%BE%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习基石笔记7：The VC Dimension</title>
    <link href="http://yangtf983.github.io/2020/03/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B07%EF%BC%9AThe%20VC%20Dimension/"/>
    <id>http://yangtf983.github.io/2020/03/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B07%EF%BC%9AThe%20VC%20Dimension/</id>
    <published>2020-03-15T16:37:07.000Z</published>
    <updated>2020-03-15T16:37:31.484Z</updated>
    
    <summary type="html">
    
      &lt;h1 id=&quot;0-说明&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#0-说明&quot;&gt;&lt;/a&gt; 0 说明&lt;/h1&gt;
&lt;p&gt;这节课对VC Dimension做出定义和说明。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="机器学习基石听课笔记" scheme="http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E5%90%AC%E8%AF%BE%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习基石笔记6：Theory of Generalization</title>
    <link href="http://yangtf983.github.io/2020/03/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B06%EF%BC%9ATheory%20of%20Generalization/"/>
    <id>http://yangtf983.github.io/2020/03/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B06%EF%BC%9ATheory%20of%20Generalization/</id>
    <published>2020-03-15T10:25:07.000Z</published>
    <updated>2020-03-15T12:37:07.569Z</updated>
    
    <summary type="html">
    
      &lt;h1 id=&quot;0-说明&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#0-说明&quot;&gt;&lt;/a&gt; 0 说明&lt;/h1&gt;
&lt;p&gt;之前说明了假设的数量M可能达到无穷，这个时候会给机器学习的解释造成问题。之后我们定义了一个成长函数&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;H&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;m_H(N)&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:1em;vertical-align:-0.25em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;msupsub&quot;&gt;&lt;span class=&quot;vlist-t vlist-t2&quot;&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.32833099999999993em;&quot;&gt;&lt;span style=&quot;top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:2.7em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;sizing reset-size6 size3 mtight&quot;&gt;&lt;span class=&quot;mord mathdefault mtight&quot; style=&quot;margin-right:0.08125em;&quot;&gt;H&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;vlist-s&quot;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.15em;&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mopen&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.10903em;&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;mclose&quot;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;，希望能够用这个成长函数替代M，并且在上一节的最后说明了如果成长函数&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;H&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;m_H(N)&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:1em;vertical-align:-0.25em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;msupsub&quot;&gt;&lt;span class=&quot;vlist-t vlist-t2&quot;&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.32833099999999993em;&quot;&gt;&lt;span style=&quot;top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:2.7em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;sizing reset-size6 size3 mtight&quot;&gt;&lt;span class=&quot;mord mathdefault mtight&quot; style=&quot;margin-right:0.08125em;&quot;&gt;H&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;vlist-s&quot;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.15em;&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mopen&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.10903em;&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;mclose&quot;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;是多项式量级的，那么就可以达到之前用来解释有限M的条件：随着N的增大，好事情发生的概率可以无限趋近于1.&lt;br /&gt;
接下来要看一下成长函数的增长能不能不超过多项式量级。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="机器学习基石听课笔记" scheme="http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E5%90%AC%E8%AF%BE%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习基石笔记5：Training versus Testing</title>
    <link href="http://yangtf983.github.io/2020/03/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B05%EF%BC%9ATraining%20versus%20Testing/"/>
    <id>http://yangtf983.github.io/2020/03/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B05%EF%BC%9ATraining%20versus%20Testing/</id>
    <published>2020-03-14T16:07:07.000Z</published>
    <updated>2020-03-14T16:08:44.244Z</updated>
    
    <summary type="html">
    
      &lt;h1 id=&quot;0-说明&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#0-说明&quot;&gt;&lt;/a&gt; 0 说明&lt;/h1&gt;
&lt;p&gt;从这一节开始说明无限个假设情况下机器学习的可行性。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="机器学习基石听课笔记" scheme="http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E5%90%AC%E8%AF%BE%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习基石笔记4：Feasibility of Learning</title>
    <link href="http://yangtf983.github.io/2020/03/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B04%EF%BC%9AFeasibility%20of%20Learning/"/>
    <id>http://yangtf983.github.io/2020/03/13/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B04%EF%BC%9AFeasibility%20of%20Learning/</id>
    <published>2020-03-13T13:12:53.000Z</published>
    <updated>2020-03-13T13:43:07.835Z</updated>
    
    <summary type="html">
    
      &lt;h1 id=&quot;0-说明&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#0-说明&quot;&gt;&lt;/a&gt; 0 说明&lt;/h1&gt;
&lt;p&gt;本节的问题是探究learning的可行性。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="机器学习基石听课笔记" scheme="http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E5%90%AC%E8%AF%BE%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>听课笔记：半监督学习（李宏毅）</title>
    <link href="http://yangtf983.github.io/2020/03/11/%E5%90%AC%E8%AF%BE%E7%AC%94%E8%AE%B0%EF%BC%9A%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%88%E6%9D%8E%E5%AE%8F%E6%AF%85%EF%BC%89/"/>
    <id>http://yangtf983.github.io/2020/03/11/%E5%90%AC%E8%AF%BE%E7%AC%94%E8%AE%B0%EF%BC%9A%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%88%E6%9D%8E%E5%AE%8F%E6%AF%85%EF%BC%89/</id>
    <published>2020-03-11T14:15:31.416Z</published>
    <updated>2020-03-13T14:30:08.302Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;0-说明&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#0-说明&quot;&gt;&lt;/a&gt; 0 说明&lt;/h2&gt;
&lt;p&gt;本文是一篇半监督学习的听课笔记，主讲者是李宏毅，课程视频&lt;a href=&quot;https://www.bilibili.com/video/av56188223?from=search&amp;amp;seid=17235380881997042888&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;网址&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="Semi-supervised Learning" scheme="http://yangtf983.github.io/tags/Semi-supervised-Learning/"/>
    
  </entry>
  
  <entry>
    <title>机器学习基石笔记笔记3：学习类型</title>
    <link href="http://yangtf983.github.io/2020/01/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B03%EF%BC%9A%E5%AD%A6%E4%B9%A0%E7%B1%BB%E5%9E%8B/"/>
    <id>http://yangtf983.github.io/2020/01/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B03%EF%BC%9A%E5%AD%A6%E4%B9%A0%E7%B1%BB%E5%9E%8B/</id>
    <published>2020-01-24T05:01:12.808Z</published>
    <updated>2020-03-13T14:38:26.986Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;0说明&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#0说明&quot;&gt;&lt;/a&gt; 【0】说明&lt;/h2&gt;
&lt;p&gt;之前在笔记1中的机器学习三要素*（ 1. 存在一个可以被学习的潜在模式； 2. 不知道如何定义规则并写入程序； 3. 可以获得大量数据。）*中已经讲过，机器学习可以执行的前提是存在可以学习的模式以及足够的数据。在实际情况中，学习的模式和输入的数据都可能有很多不同的类型，相应的也诞生了很多不同的学习算法和思想，根据这些不同，机器学习有很多分类，这一节我们介绍一些分类方法和机器学习类型。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="机器学习基石听课笔记" scheme="http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E5%90%AC%E8%AF%BE%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习基石笔记2：从PLA到SVM</title>
    <link href="http://yangtf983.github.io/2020/01/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B02%EF%BC%9A%E4%BB%8EPLA%E5%88%B0SVM/"/>
    <id>http://yangtf983.github.io/2020/01/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B02%EF%BC%9A%E4%BB%8EPLA%E5%88%B0SVM/</id>
    <published>2020-01-23T10:54:57.435Z</published>
    <updated>2020-03-13T13:35:55.766Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;0说明&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#0说明&quot;&gt;&lt;/a&gt; 【0】说明&lt;/h2&gt;
&lt;p&gt;PLA，全称perceptron learning algorithm，是一种二分类算法，也可以认为是单层神经网络，与SVM有着密不可分的关系。SVM，即support vector machine，中文名是支持向量机，也是一种常见的二分类算法，这种算法的 一种推导方式就是从PLA出发，找到一条“最好的”PLA分类器。在机器学习基石课程中林轩田老师只介绍了PLA，而SVM是他在后续的另一门课程——机器学习技法——中介绍的。由于二者具有紧密的联系，故在此一并介绍。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="机器学习基石听课笔记" scheme="http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E5%90%AC%E8%AF%BE%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习基石笔记1：机器学习简介</title>
    <link href="http://yangtf983.github.io/2020/01/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B01%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/"/>
    <id>http://yangtf983.github.io/2020/01/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B01%EF%BC%9A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%80%E4%BB%8B/</id>
    <published>2020-01-23T02:50:18.457Z</published>
    <updated>2020-03-12T02:24:51.835Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;0-说明&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#0-说明&quot;&gt;&lt;/a&gt; 【0】 说明&lt;/h2&gt;
&lt;p&gt;机器学习基石是一门很好的机器学习入门课，能够让人在短时间内掌握大量的机器学习的基础理论和原理。从这篇文章开始，我会用大约十几篇文章的内容将机器学习基石我听机器学习基石这门课时候的所学和思考表达出来，我的笔记中会包含大部分基石课程中的内容，并且不会略去任何课程中的证明。对于课程中证明讲的不清楚的一些地方，我还会附上我的理解。关于这门课程的课件可以从&lt;a href=&quot;https://www.csie.ntu.edu.tw/~htlin/course/mlfound17fall/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;课程主页&lt;/a&gt;上下载。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="机器学习基石听课笔记" scheme="http://yangtf983.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E5%90%AC%E8%AF%BE%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>论文翻译：半监督学习介绍</title>
    <link href="http://yangtf983.github.io/2020/01/09/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%EF%BC%9A%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D/"/>
    <id>http://yangtf983.github.io/2020/01/09/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%EF%BC%9A%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D/</id>
    <published>2020-01-09T09:20:56.026Z</published>
    <updated>2020-01-24T03:34:24.928Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;0-说明&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#0-说明&quot;&gt;&lt;/a&gt; 0 说明&lt;/h2&gt;
&lt;p&gt;本文是一篇关于半监督学习介绍性论文集的第一篇的翻译，大部分是人工翻译，少部分是机器翻译加人工修改。不过限于知识有限，其中很多内容并不了解，所以一些地方翻译不到位，仅供参考。&lt;br /&gt;
论文集电子版链接&lt;a href=&quot;https://1drv.ms/b/s!AtSOzmIQPQq9jjCnQTY9h51dVHU3?e=bh5emn&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;onedrive&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="Semi-supervised Learning" scheme="http://yangtf983.github.io/tags/Semi-supervised-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Imitation Learning &amp; Dagger Algorithm</title>
    <link href="http://yangtf983.github.io/2020/01/02/Imitation_Learing&amp;Dagger_Algorithm/"/>
    <id>http://yangtf983.github.io/2020/01/02/Imitation_Learing&amp;Dagger_Algorithm/</id>
    <published>2020-01-02T00:08:47.443Z</published>
    <updated>2020-03-13T14:42:22.033Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;0-说明&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#0-说明&quot;&gt;&lt;/a&gt; [0] 说明&lt;/h2&gt;
&lt;p&gt;进入Reinforcement Learning的世界，一般而言应当是从tabular method学起，但是从imitation learning学起推广到Reinforcrmrnt Learning可以帮助我们更好的理解RL的解决问题的思路和发展目的，因此我们本次继续跟随cs294的脚步，学习Imitation learning的相关知识以及相对简单的Dagger算法。&lt;/p&gt;
&lt;p&gt;值得一提的是，IL应当归类于监督学习(Supervised Learning)，但是其可以应用于IRL(Inverse Reinforcement Learning)领域，这也是IL与IRL的重要联系之一。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="Reinforcement Learning" scheme="http://yangtf983.github.io/tags/Reinforcement-Learning/"/>
    
  </entry>
  
  <entry>
    <title>A simple introduce of reinforcement learning</title>
    <link href="http://yangtf983.github.io/2019/01/30/A_simple_introduce_of_RL/"/>
    <id>http://yangtf983.github.io/2019/01/30/A_simple_introduce_of_RL/</id>
    <published>2019-01-29T16:28:02.000Z</published>
    <updated>2020-01-02T02:33:01.870Z</updated>
    
    <summary type="html">
    
      &lt;h2 id=&quot;0-说明&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#0-说明&quot;&gt;&lt;/a&gt; [0] 说明&lt;/h2&gt;
&lt;p&gt;这是本博客RL系列的第一篇文章，旨在对RL进行一个简单的介绍，不涉及高深的理论。作为一个RL初学者，本博客的一系列RL文章都将是我在学习过相应的课程后写的，我会在学习已有资料的基础上加上自己的理解，但是一般不会注明哪些是自己的理解，哪些是材料中的观点。当然我的理解难免会有差错，因此我会在每一篇博文后注明我的学习材料，有需要的读者可以自行寻找阅读，若发现我的差错，欢迎批评指正。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="Reinforcement Learning" scheme="http://yangtf983.github.io/tags/Reinforcement-Learning/"/>
    
  </entry>
  
</feed>
